<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="武德">
<meta property="og:url" content="http://zhos.me/page/2/index.html">
<meta property="og:site_name" content="武德">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="武德">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zhos.me/page/2/">





  <title>武德</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">武德</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/21/yuque/眼底水肿病变区域自动分割/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/21/yuque/眼底水肿病变区域自动分割/" itemprop="url">眼底水肿病变区域自动分割</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-21T09:28:40+08:00">
                2018-12-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/ShawnBIT/AI-Challenger-Retinal-Edema-Segmentation" target="_blank" rel="noopener">链接</a><br><a name="4np7ea"></a></p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a><a href="#4np7ea"></a>背景</h3><ul>
<li><p>视网膜水肿是一种眼疾，可导致视力模糊，影响正常生活。</p>
</li>
<li><p>OCT（光学相干断层扫描）可用于帮助医生判断视网膜水肿。</p>
</li>
<li><p>早期发现水肿症状可以在疾病的治疗中发挥关键作用。</p>
</li>
<li><p>我们的任务是设计算法，以自动检测视网膜水肿的类型，并根据OCT图像划分视网膜水肿区域。</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545362517258-ac147a70-c630-4ba9-9596-429ce59aaf05.png#width=323" alt><br><a name="tevkbe"></a></p>
<h3 id="数据统计信息"><a href="#数据统计信息" class="headerlink" title="数据统计信息"></a><a href="#tevkbe"></a>数据统计信息</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363237135-d6a8f54b-194f-41e9-8998-42c2794fd373.png#width=745" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363261530-b1e78b3c-8896-4cf1-b15a-3f6656d5c9b7.png#width=634" alt><br><a name="2obuzk"></a></p>
<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a><a href="#2obuzk"></a>数据可视化</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363314506-c30f03d8-0647-4d55-ad77-3398e87c36a3.png#width=566" alt></p>
<ul>
<li><p>视网膜边缘弯曲</p>
</li>
<li><p>病变之间的包含关系</p>
</li>
</ul>
<p><a name="1xo7et"></a></p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a><a href="#1xo7et"></a>数据处理</h3><p>堆叠上部和下部切片以形成三通道输入<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363525794-cfdcff1f-fcfa-45bf-a7bd-86330b40bd22.png#width=826" alt><br>正则化<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363568658-bfc944e3-aee5-41cb-9e36-87604ccbc425.png#width=826" alt><br>数据增强（仅随机水平翻转）<br><a name="gpzgyi"></a></p>
<h3 id="问题和挑战"><a href="#问题和挑战" class="headerlink" title="问题和挑战"></a><a href="#gpzgyi"></a>问题和挑战</h3><p>这两项任务（分割和检测）如何相互促进？ •多任务学习框架<br><br><br><br>多尺度的视网膜水肿病变<br><br>•UNet和UNet ++<br><br>三种视网膜水肿样本不平衡<br><br>•指数对数损失<br><br>如何扩大感受野以检测边缘弯曲？<br><br>•扩张模块</p>
<p><a name="xtsmsg"></a></p>
<h3 id="基线"><a href="#基线" class="headerlink" title="基线"></a><a href="#xtsmsg"></a>基线</h3><p>分段 -  UNet<br><br>•下采样16×<br><br>•频道：[16,32,64,128,256]•输入：原始图像<br>检测 -  ResNet18<br><br>•调整大小224 * 224<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363778350-5ef98156-3f52-4bc4-81f7-ac1ff58bd5cb.png#width=826" alt><br><a name="12ngly"></a></p>
<h3 id="多任务框架"><a href="#多任务框架" class="headerlink" title="多任务框架"></a><a href="#12ngly"></a>多任务框架</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363824351-0b2aef87-4aa5-4d54-9da2-950238d08bc2.png#width=826" alt></p>
<ul>
<li><p>共同学习分割和检测</p>
</li>
<li><p>减少时间和计算成本</p>
</li>
<li><p>改善两项任务的效果</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363883446-cddecdd3-133c-4d7a-a48f-7ee3ce3c782e.png#width=826" alt><br><a name="45hwtb"></a></p>
<h3 id="UNet-和DeepSupervision"><a href="#UNet-和DeepSupervision" class="headerlink" title="UNet ++和DeepSupervision"></a><a href="#45hwtb"></a>UNet ++和DeepSupervision</h3><ul>
<li><p>密集的未来联系</p>
</li>
<li><p>深度监督</p>
</li>
<li><p>更有效地融合低级和高级功能</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363984175-b5734e15-36a3-4a26-b548-247f21e500b5.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364004491-f0fd9292-3bf9-4d45-b764-712c129650ff.png#width=826" alt><br><a name="mcxadi"></a></p>
<h3 id="Exponential-Logarithmic-Loss"><a href="#Exponential-Logarithmic-Loss" class="headerlink" title="Exponential Logarithmic Loss"></a><a href="#mcxadi"></a>Exponential Logarithmic Loss</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364043982-9f418634-6d4e-421e-8ed9-0b5bdd78898b.png#width=593" alt><br>x : pixel position d il : Kronecker delta<br>i : label e : pseudocount for<br>l : ground truth label at x<br>p i ( x ) : Softmax probability which acts as the portion of pixel<br>x owned by label i<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364142210-f09d6809-e1cf-428e-a8ec-e1e7a750d02a.png#width=462" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364180115-716d53bf-c3dd-46fb-bc90-f18e23866c24.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364198735-685fb3a5-9739-4820-9578-63357d19b78b.png#width=771" alt><br>小物件怎么样？<br><br>细分？（2,3级）<br><a name="ymmopw"></a></p>
<h3 id="扩张模块"><a href="#扩张模块" class="headerlink" title="扩张模块"></a><a href="#ymmopw"></a>扩张模块</h3><p>感受野计算公式<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364372343-71750b48-5889-4c2f-8d71-4ba7d15c8b40.png#width=504" alt><br>UNet编码器的接收域<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364421357-aae861a2-0977-4ad8-9b07-c0f7af8ab715.png#width=398" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364444046-68a08360-b0a9-4c89-b14d-85c7707c6eab.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364472055-2f2fcbe0-1c8c-4719-bcab-3d7860331235.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364500288-3ff90cf8-acdc-47e6-80d7-9df1b0f98cb0.png#width=778" alt><br>大对象怎么样？<br><br>细分？（第1类）<br><a name="hvmwoz"></a></p>
<h3 id="实验摘要"><a href="#实验摘要" class="headerlink" title="实验摘要"></a><a href="#hvmwoz"></a>实验摘要</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364586336-051b209a-18bc-4c1f-959a-656c8b1ab2e2.png#width=826" alt><br>Memory : 7.3 G(batch=8), Inference time : 9.5 s/patient<br><a name="d14kdy"></a></p>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><a href="#d14kdy"></a>可视化</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364667315-bb12e09f-92b4-4d6c-9427-10ce7851f341.png#width=826" alt><br><a name="f7g5bu"></a></p>
<h3 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a><a href="#f7g5bu"></a>未来工作</h3><ul>
<li><p>检测框架（Mask R-CNN）</p>
</li>
<li><p>3D语义分割模型</p>
</li>
<li><p>在骨干中使用Res-block或Dense-block</p>
</li>
<li><p>考虑病变的关系</p>
</li>
</ul>
<p><a name="db4gcg"></a></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><a href="#db4gcg"></a>结论</h3><p>构建端到端的多任务框架，可以同时检测和分割视网膜水肿病变。<br><br>•使用最新的UNet ++模型更好地集成高级和低级功能。<br><br>使用新的指数对数损失函数来增强两种类型的小病变的分割。<br><br>•引入扩张卷积模块，显着增加模型的感受野。<br><br>只有随机水平翻转数据增强，没有后期处理。<br><br>•测试装置上单个模型的骰子为0.736。 测试集上的融合模型的骰子为0.744，检测AUC为0.986。 另外，当我们设置批次为8时，推理阶段的记忆为7.3G，每个患者的推理时间为9.5s。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/20/yuque/语义分割 -  U-Net（第1部分）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/20/yuque/语义分割 -  U-Net（第1部分）/" itemprop="url">语义分割 -  U-Net（第1部分）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-20T15:47:26+08:00">
                2018-12-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066" target="_blank" rel="noopener">链接</a></p>
<p>这里再次写信给我6个月前的自我……<br><br>在这篇文章中，我将主要关注语义分割，像素分类任务和特定的算法。 我将提供一些关于我最近一直在努力的案例的演练。<br><br>根据定义，语义分割是将图像划分为连贯的部分。 例如，对属于我们的数据集中的人，汽车，树或任何其他实体的每个像素进行分类。</p>
<p><a name="h3mvhf"></a></p>
<h4 id="语义分割与实例分割"><a href="#语义分割与实例分割" class="headerlink" title="语义分割与实例分割"></a><a href="#h3mvhf"></a>语义分割与实例分割</h4><p>与实例分割相比，语义分割相对容易。</p>
<p>在实例分割中，我们的目标不仅是对每个人，汽车或树进行像素预测，而且还分别将每个实体识别为人1，人2，树1，树2，汽车1，汽车2，汽车3 等等。 用于实例分割的现有技术算法是Mask-RCNN：具有多个子网络一起工作的两阶段方法：RPN（区域提议网络），FPN（特征金字塔网络）和FCN（完全卷积网络）[5， 6,7,8]。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292254188-fd4348f0-a59e-441b-b0f9-9ef087b52277.png#width=640" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292265350-a6d4cf06-7a19-4037-8f98-3faa8427aa38.png#width=643" alt><br><a name="7zozib"></a></p>
<h4 id="案例研究：Data-Science-Bowl-2018"><a href="#案例研究：Data-Science-Bowl-2018" class="headerlink" title="案例研究：Data Science Bowl 2018"></a><a href="#7zozib"></a>案例研究：Data Science Bowl 2018</h4><p>数据科学比赛2018刚刚结束，我从中学到了很多东西。也许我学到的最重要的一课，即使是深入学习，与传统的ML相比，更自动化的技术，前后处理对于获得好的结果可能是至关重要的。这些是从业者获得的重要技能，它们定义了您构建和建模问题的方式。<br><br>我不会详细讨论这个特定的比赛，因为对于任务本身和整个比赛中使用的方法都有大量的讨论和解释。但我会简要提及获胜的解决方案，因为它与这篇文章的基础有关。 [13]<br><br>数据科学比赛2018就像其他数据科学比赛一样，由Booz Allen基金会组织。今年的任务是在给定的显微镜图像中识别细胞核并独立地为每个细胞核提供掩模。<br><br>现在，花一两个时间来猜测这个任务需要哪种类型的细分;语义还是实例？<br><br>这是一个掩盖图像样本，它是原始的显微镜图像。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292383227-8c6f4c64-4e97-4006-90c9-558b1d59c4e0.png#width=729" alt><br>虽然起初听起来像是语义分段任务，但这里的任务是实例分割。我们需要独立地处理图像中的每个核，并将它们识别为核1，核2，核3，……类似于我们对汽车1，汽车2，人1等的示例。也许这项任务的动机是跟踪细胞样本中细胞核的大小，数量和特征。自动化该跟踪过程并进一步加速用于治疗各种疾病的不同治疗方法的实验是非常重要的。<br><br>现在，您可能会认为如果本文是关于语义分段的，并且如果Data Science Bowl 2018是实例分段任务的一个示例，那么为什么我一直在谈论这个特定的竞争。如果你正在考虑这个问题，那么你肯定是对的，而且这次比赛的最终目标确实不是语义分割的一个例子。但是，随着我们将继续前进，您将看到如何将此实例分段问题实际转变为多类语义分段任务。这是我尝试但在实践中失败的方法，但也成为获胜解决方案的高级动机。<br><br>在这3个月的比赛期间，只有两个模型（或其变体）在论坛中共享或至少明确讨论过; Mask-RCNN和U-Net。正如我之前提到的，Mask-RCNN是最先进的物体检测算法，它可以检测单个物体并预测其掩模，如实例分割。 Mask-RCNN的实施和培训更加困难，因为它采用了两阶段学习方法，您首先优化RPN（区域提案网络），然后同时预测边界框，类和掩模。<br><br>另一方面，U-Net是一种非常流行的端到端编码器 - 解码器网络，用于语义分割[9]。它最初是发明并首次用于生物医学图像分割，这是我们为Data Science Bowl所做的一项非常类似的任务。竞争中没有银弹，没有任何一个没有邮政或预处理的建筑或建筑设计中的任何小调整都没有得到最高分。我没有机会为这次比赛尝试Mask-RCNN，所以我在U-Net周围进行了实验并学到了很多东西。<br><br>此外，由于我们的主题是语义分割，我将把Mask-RCNN留给其他博客文章进行解释。但是如果你仍然坚持在自己的CV应用程序中尝试它们，这里有两个流行的github存储库，在Tensorflow和PyTorch中实现。 [10,11]<br><br>现在，我们可以继续使用U-Net并深入了解它的细节……<br><br>这是开始的架构：<br><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292433921-5a4506b4-63e9-48d0-adce-6d7e750d5209.png#width=826" alt><br>对于熟悉传统卷积神经网络的人来说，架构的第一部分（表示为DOWN）将是熟悉的。第一部分被调用，或者您可能认为它是编码器部分，您应用卷积块，然后进行maxpool下采样，将输入图像编码为多个不同级别的要素表示。<br><br>网络的第二部分包括上采样和连接，然后是常规卷积操作。 CNN中的上采样可能是一些读者的新概念，但这个想法相当简单：我们正在扩展要素尺寸以使用左边相应的连接块来满足相同的尺寸。您可能会看到灰色和绿色箭头，我们将两个要素图连接在一起。与其他完全卷积分割网络相比，U-Net在这种意义上的主要贡献在于，在网络中进行上采样和深入研究的同时，我们将更高分辨率的特征从下部与上采样特征连接起来，以便更好地定位和学习表示。以下卷积。由于上采样是稀疏操作，因此我们需要从早期阶段获得良好的优先级以更好地表示本地化。在FPN（特征金字塔网络）中也可以看到组合匹配级别的类似想法。 [7]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292475699-4aa5a478-7954-41a5-ab80-9e8a6125d4e4.png#width=765" alt><br>我们可以在下部定义一个操作块作为卷积→下采样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a sample down block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_conv_bn_relu</span><span class="params">(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,  stride=stride, padding=padding, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    ]</span><br><span class="line">self.down1 = nn.Sequential(</span><br><span class="line">    *make_conv_bn_relu(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> ),</span><br><span class="line">    *make_conv_bn_relu(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> ),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convolutions followed by a maxpool</span></span><br><span class="line">down1 = self.down1(x)</span><br><span class="line">out1   = F.max_pool2d(down1, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>类似地，我们可以将一个操作块定义为上采样→连接→卷积。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a sample up block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_conv_bn_relu</span><span class="params">(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,  stride=stride, padding=padding, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    ]</span><br><span class="line">self.up4 = nn.Sequential(</span><br><span class="line">    *make_conv_bn_relu(<span class="number">128</span>,<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> ),</span><br><span class="line">    *make_conv_bn_relu(<span class="number">64</span>,<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> )</span><br><span class="line">)</span><br><span class="line">self.final_conv = nn.Conv2d(<span class="number">32</span>, num_classes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># upsample out_last, concatenate with down1 and apply conv operations</span></span><br><span class="line">out   = F.upsample(out_last, scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>)  </span><br><span class="line">out   = torch.cat([down1, out], <span class="number">1</span>)</span><br><span class="line">out   = self.up4(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># final 1x1 conv for predictions</span></span><br><span class="line">final_out = self.final_conv(out)</span><br></pre></td></tr></table></figure></p>
<p>通过仔细检查图形，您可能会注意到输出尺寸（388 x 388）与原始输入（572 x 572）不同。如果您希望获得一致的大小，您可以应用填充卷积来保持跨越级联级别的维度，就像我们在上面的示例代码中所做的那样。<br><br>当提到这样的上采样时，您可能会遇到以下任一项：转置卷积，上卷积，反卷积或上移。包括我自己和PyTorch文档在内的许多人都不喜欢反卷积这个术语，因为在上采样阶段我们实际上正在进行常规的卷积运算，并且没有任何关于它的信息。如果您不熟悉基本的卷积运算及其算术，我会强烈建议您访问此处。 [12]<br><br>我将解释从最简单到更复杂的上采样方法。以下是在PyTorch中对2D张量进行上采样的三种方法：<br><a name="qab0nn"></a></p>
<h4 id="最近邻"><a href="#最近邻" class="headerlink" title="最近邻"></a><a href="#qab0nn"></a>最近邻</h4><p>这是在将张量调整（转换）为更大张量时找到丢失像素值的最简单方法，例如， 2x2到4x4,5x5或6x6。<br><br>让我们使用Numpy逐步实现这个基本的计算机视觉算法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_interpolate</span><span class="params">(A, new_size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Nearest Neighbor Interpolation, Step by Step</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># get sizes</span></span><br><span class="line">    old_size = A.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate row and column ratios</span></span><br><span class="line">    row_ratio, col_ratio = new_size[<span class="number">0</span>]/old_size[<span class="number">0</span>], new_size[<span class="number">1</span>]/old_size[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define new pixel row position i</span></span><br><span class="line">    new_row_positions = np.array(range(new_size[<span class="number">0</span>]))+<span class="number">1</span></span><br><span class="line">    new_col_positions = np.array(range(new_size[<span class="number">1</span>]))+<span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># normalize new row and col positions by ratios</span></span><br><span class="line">    new_row_positions = new_row_positions / row_ratio</span><br><span class="line">    new_col_positions = new_col_positions / col_ratio</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply ceil to normalized new row and col positions</span></span><br><span class="line">    new_row_positions = np.ceil(new_row_positions)</span><br><span class="line">    new_col_positions = np.ceil(new_col_positions)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># find how many times to repeat each element</span></span><br><span class="line">    row_repeats = np.array(list(Counter(new_row_positions).values()))</span><br><span class="line">    col_repeats = np.array(list(Counter(new_col_positions).values()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># perform column-wise interpolation on the columns of the matrix</span></span><br><span class="line">    row_matrix = np.dstack([np.repeat(A[:, i], row_repeats) </span><br><span class="line">                            <span class="keyword">for</span> i <span class="keyword">in</span> range(old_size[<span class="number">1</span>])])[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># perform column-wise interpolation on the columns of the matrix</span></span><br><span class="line">    nrow, ncol = row_matrix.shape</span><br><span class="line">    final_matrix = np.stack([np.repeat(row_matrix[i, :], col_repeats)</span><br><span class="line">                             <span class="keyword">for</span> i <span class="keyword">in</span> range(nrow)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_matrix</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_interpolate</span><span class="params">(A, new_size)</span>:</span></span><br><span class="line">    <span class="string">"""Vectorized Nearest Neighbor Interpolation"""</span></span><br><span class="line"></span><br><span class="line">    old_size = A.shape</span><br><span class="line">    row_ratio, col_ratio = np.array(new_size)/np.array(old_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># row wise interpolation </span></span><br><span class="line">    row_idx = (np.ceil(range(<span class="number">1</span>, <span class="number">1</span> + int(old_size[<span class="number">0</span>]*row_ratio))/row_ratio) - <span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># column wise interpolation</span></span><br><span class="line">    col_idx = (np.ceil(range(<span class="number">1</span>, <span class="number">1</span> + int(old_size[<span class="number">1</span>]*col_ratio))/col_ratio) - <span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    final_matrix = A[:, row_idx][col_idx, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_matrix</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292610278-9d12de49-89e6-454e-a3ed-35ea21ccbf82.png#width=826" alt><br><strong>[PyTorch]</strong> F.upsample(…, mode = “nearest”)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)</span><br><span class="line">&gt;&gt;&gt; input</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1  2</span><br><span class="line">  3  4</span><br><span class="line">[torch.FloatTensor of size (1,1,2,2)]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;nearest&apos;)</span><br><span class="line">&gt;&gt;&gt; m(input)</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1  1  2  2</span><br><span class="line">  1  1  2  2</span><br><span class="line">  3  3  4  4</span><br><span class="line">  3  3  4  4</span><br><span class="line">[torch.FloatTensor of size (1,1,4,4)]</span><br></pre></td></tr></table></figure></p>
<p>双线性插值<br><br>双线性插值算法的计算效率低于最近邻居，但它是一种更精确的近似。 根据距离计算单个像素值作为所有其他值的加权平均值。<br><br>[PyTorch] F.upsample（…，mode =“bilinear”）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)</span><br><span class="line">&gt;&gt;&gt; input</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1  2</span><br><span class="line">  3  4</span><br><span class="line">[torch.FloatTensor of size (1,1,2,2)]</span><br><span class="line">&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;)</span><br><span class="line">&gt;&gt;&gt; m(input)</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1.0000  1.2500  1.7500  2.0000</span><br><span class="line">  1.5000  1.7500  2.2500  2.5000</span><br><span class="line">  2.5000  2.7500  3.2500  3.5000</span><br><span class="line">  3.0000  3.2500  3.7500  4.0000</span><br><span class="line">[torch.FloatTensor of size (1,1,4,4)]</span><br></pre></td></tr></table></figure></p>
<p>转置卷积<br><br>在转置卷积中，我们通过反向传播学习权重。 在论文中，我遇到了针对各种情况的所有这些上采样方法，并且在实践中，您可能会更改您的体系结构并尝试所有这些以查看哪种方法最适合您自己的问题。 我个人更喜欢转置卷积，因为我们对它有更多的控制权，但你也可以选择双线性插值或最近邻居。<br><strong>[PyTorch]</strong> nn.ConvTranspose2D(…, stride=…, padding=…)<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292702247-9ed06c87-3b1d-49f0-9794-88a906ebb639.png#width=826" alt><br>如果我们回到最初的案例，数据科学碗，在竞争中使用香草U-Net方法的主要缺点是重叠核。 如上图所示，如果您创建一个二元蒙版并将其用作目标，U-Net肯定会预测类似于此的东西，并且您将拥有多个核的组合掩模，这些核重叠或彼此非常接近。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292738956-2056a9a8-93c3-46b1-aca5-6dfc1da18863.png#width=730" alt><br>关于重叠实例问题，U-Net论文的作者使用加权交叉熵来强调学习细胞的边界。 此方法帮助他们分离重叠的实例。 基本的想法是更多地加权边界，并推动网络在近距离实例之间找到学习差距。[9]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292766571-1bab28fe-4b7d-4e4d-b158-f641816a25cd.png#width=780" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292777101-f5f1af70-43dc-40e4-a20c-6bb04d662c03.png#width=826" alt><br>这种问题的另一种解决方案是许多竞争者使用的方法，包括获胜的解决方案，是将二进制掩码转换为多类目标。 U-Net的优点在于，您可以构建网络以根据需要输出任意数量的通道，并通过在最后一层使用1x1卷积来表示任何通道中的任何类。<br>引自Data Science Bowl获奖解决方案：</p>
<blockquote>
<p>用于具有S形激活的网络的2通道掩模，即（掩模 - 边界，边界）或用于具有softmax激活的网络的3通道掩模，即（掩模 - 边界，边界，1  - 掩模 - 边界）<br>2通道全面罩即（面罩，边框）</p>
</blockquote>
<p>在进行这些预测之后，诸如分水岭的经典图像处理算法可以用于后处理以进一步分割单个核。[14]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292929262-dda5f714-55e9-4a52-b5ef-7402a732ea29.png#width=320" alt><br>这是第一次正式的计算机视觉竞赛，我有勇气参加Kaggle，它是一个数据科学碗。虽然我只在前20％（这被认为是平均分数）完成了比赛，但我感到很高兴参加数据科学碗并学习如果我实际上没有参与的话我可能永远不会学到的东西并试着靠自己。积极学习远比观看或阅读来自在线资源的类似方法更富有成效。<br><br>作为一名刚刚开始用Fast.ai开始练习数月的深度学习练习者，这对我来说是一个重要的一步，也是我永无止境的旅程，在获得经验方面非常有价值。所以，对于那些在你之前从未见过或已经解决的挑战感到暗示的人，我强烈建议你专门去处理这些类型的挑战，以便感受学习以前你不知道的东西的乐趣。<br><br>我在本次比赛中学到的另一个有价值的教训是，在计算机视觉中（这也适用于NLP）竞赛，通过眼睛检查每一个预测非常重要，看看哪些有效，哪些无效。如果您的数据足够小，您应该去检查每个输出。如果出现问题，这将允许您进一步提出更好的想法，甚至调试代码。<br><a name="79zwnl"></a></p>
<h4 id="转移学习及其他"><a href="#转移学习及其他" class="headerlink" title="转移学习及其他"></a><a href="#79zwnl"></a>转移学习及其他</h4><p>到目前为止，我们已经定义了vanilla U-Net的构建块，并提到了我们如何操纵目标来解决实例分割问题。现在我们可以进一步讨论这些类型的编码器 - 解码器网络的灵活性。通过灵活性，我的意思是你拥有它的自由以及你可以对它进行设计的创造力。<br><br>任何在某些时候进行深度学习的人都会转移学习，因为这是一个非常强大的想法。简而言之，转移学习是使用预训练网络的概念，该网络在许多样本上进行训练，以完成我们所面临的类似任务，但缺少相同数量的数据。即使有足够的数据传输，学习也可以在一定程度上提升性能，不仅适用于计算机视觉任务，也适用于NLP。<br><br>迁移学习也被证明是U-Net类似架构的强大技术。我们之前已经定义了U-Net的两个主要组件;向上和向上。我们这次将这些部分重新编号为编码器和解码器。编码器部分基本上接受输入并将其编码在低维特征空间中，该特征空间表示我们在较低维度中的输入。现在想象用你最喜欢的ImageNet获胜者替换这个编码器; VGG，ResNet，Inception，NasNet，……你想要的。这些网络经过精心设计，可以做一个常见的事情：以最佳方式编码自然图像进行分类，ImageNet上的预训练重量等待您在线抓取它们。<br><br>那么为什么不使用这些架构之一作为我们的编码器并以与原始U-Net相同的方式构建解码器，但更好的是，使用类固醇。<br><br>TernausNet是Kaggle Carvana挑战赛的赢家架构，它使用与VGG11相同的传输学习理念作为编码器。 [15,16]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545293023504-202250cf-02aa-47c8-bc29-5661cddacdad.png#width=832" alt><br><a name="4rcezx"></a></p>
<h4 id="Fast-ai：动态U-Net"><a href="#Fast-ai：动态U-Net" class="headerlink" title="Fast.ai：动态U-Net"></a><a href="#4rcezx"></a>Fast.ai：动态U-Net</h4><p>受到TernausNet论文和许多其他优秀资源的启发，我想概括为U-Net架构使用预训练或自定义编码器的想法。 所以，我想出了一个通用的架构：动态Unet。<br><br>Dynamic Unet是这个想法的一个实现，它通过为您完成所有计算和匹配，自动为任何给定的编码器创建解码器部分。 编码器既可以是现成的预训练网络，也可以是您自己定义的任何自定义架构。<br><br>它是用PyTorch编写的，目前在Fast.ai库中。 您可以参考此笔记本以查看其中的操作或查看来源。 Dynamic Unet的主要目标是节省实践者的时间，并允许使用尽可能少的代码更容易地使用不同的编码器进行实验。<br><br>在第2部分中，我将解释体积数据的3D编码器解码器模型，例如MRI扫描，并给出我一直在研究的真实世界的例子。</p>
<p><strong>References</strong></p>
<p>[5] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01497</a><br>[6] Mask R-CNN: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">https://arxiv.org/abs/1703.06870</a><br>[7] Feature Pyramid Networks for Object Detection: <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a><br>[8] Fully Convolutional Networks for Semantic Segmentation: <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf</a><br>[9] U-net: Convolutional Networks for Biomedical Image Segmentation: <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">https://arxiv.org/abs/1505.04597</a><br>[10] Tensorflow Mask-RCNN: <a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="noopener">https://github.com/matterport/Mask_RCNN</a><br>[11] Pytorch Mask-RCNN:_ _<a href="https://github.com/multimodallearning/pytorch-mask-rcnn" target="_blank" rel="noopener">https://github.com/multimodallearning/pytorch-mask-rcnn</a><br>[12] Convolution Arithmetic: <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic</a><br>[13] Data Science Bowl 2018 Winning Solution, ods-ai: <a href="https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741" target="_blank" rel="noopener">https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741</a><br>[14] Watershed Algorithm <a href="https://docs.opencv.org/3.3.1/d3/db4/tutorial_py_watershed.html" target="_blank" rel="noopener">https://docs.opencv.org/3.3.1/d3/db4/tutorial_py_watershed.html</a><br>[15] Carvana Image Masking Challenge: <a href="https://www.kaggle.com/c/carvana-image-masking-challenge" target="_blank" rel="noopener">https://www.kaggle.com/c/carvana-image-masking-challenge</a><br>[16] TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation: <a href="https://arxiv.org/abs/1801.05746" target="_blank" rel="noopener">https://arxiv.org/abs/1801.05746</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/" itemprop="url">Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-20T15:28:04+08:00">
                2018-12-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>分享，重用和分解模型复杂性的有效方法</p>
<p>更新到 Pytorch 4.1版本</p>
<p>你可以找到代码 <a href="https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict" target="_blank" rel="noopener">here</a></p>
<p>Pytorch是一个开源的深度学习框架，提供了创建ML模型的智能方法。 即使文档制作完好，我仍然发现大多数人仍然能够编写错误的而不是有组织的PyTorch代码。</p>
<p>今天，我们将看到如何使用PyTorch的三个主要构建块：Module，Sequential和ModuleList。 我们将从一个例子开始，迭代地我们将使它变得更好。<br><br><br><br>所有这四个类都包含在torch.nn中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Module</span></span><br><span class="line"><span class="comment"># nn.Sequential</span></span><br><span class="line"><span class="comment"># nn.Module</span></span><br></pre></td></tr></table></figure>
<p><a name="puggyn"></a></p>
<h2 id="模块：主要构建块"><a href="#模块：主要构建块" class="headerlink" title="模块：主要构建块"></a><a href="#puggyn"></a>模块：主要构建块</h2><p>Module是主要的构建块，它定义了所有神经网络的基类，你必须将它子类化。<br><br><br><br>让我们创建一个经典的CNN分类器作为示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (fc1): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这是一个非常简单的分类器，其编码部分使用两层3x3 convs + batchnorm + relu和一个带有两个线性层的解码部分。 如果您不是PyTorch的新手，您可能以前看过这种类型的编码，但有两个问题。<br><br><br><br>如果我们想要添加一个图层，我们必须在<strong>init</strong>和forward函数中再次编写大量代码。 此外，如果我们有一些我们想要在另一个模型中使用的公共块，例如 3x3 conv + batchnorm + relu，我们必须再写一次。</p>
<p><a name="kpmyrn"></a></p>
<h2 id="Sequential：堆叠和合并图层"><a href="#Sequential：堆叠和合并图层" class="headerlink" title="Sequential：堆叠和合并图层"></a><a href="#kpmyrn"></a>Sequential：堆叠和合并图层</h2><p><a name="di8wcs"></a></p>
<h2 id><a href="#" class="headerlink" title></a><a href="#di8wcs"></a></h2><p>Sequential是一个模块的容器，可以堆叠在一起并同时运行。<br><br><br><br>你可以注意到我们必须将自己的一切存储起来。 我们可以使用Sequential来改进我们的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_block1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv_block2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv_block1(x)</span><br><span class="line">        x = self.conv_block2(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (conv_block1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): BatchNorm2d(<span class="number">32</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (conv_block2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): Sigmoid()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>你觉得好多了？<br><br><br><br>您是否注意到conv_block1和conv_block2看起来几乎相同？ 我们可以创建一个重新生成nn.Sequential的函数来简化代码！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_f, out_f, *args, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_f, out_f, *args, **kwargs),</span><br><span class="line">        nn.BatchNorm2d(out_f),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>然后我们可以在我们的模块中调用此函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_block1 = conv_block(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv_block2 = conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv_block1(x)</span><br><span class="line">        x = self.conv_block2(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (conv_block1): Sequential(</span><br><span class="line">    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    (2): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (conv_block2): Sequential(</span><br><span class="line">    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    (2): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Even cleaner! Still <code>conv_block1</code> and <code>conv_block2</code> are almost the same! We can merge them using <code>nn.Sequential</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            conv_block(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>self.encoder</code> now holds booth <code>conv_block</code>. We have decoupled logic for our model and make it easier to read and reuse. Our <code>conv_block</code> function can be imported and used in another model.</p>
<p><a name="h5bzub"></a></p>
<h2 id="Dynamic-Sequential-create-multiple-layers-at-once"><a href="#Dynamic-Sequential-create-multiple-layers-at-once" class="headerlink" title="Dynamic Sequential: create multiple layers at once"></a><a href="#h5bzub"></a>Dynamic Sequential: create multiple layers at once</h2><p>What if we can to add a new layers in <code>self.encoder</code>, hardcoded them is not convinient:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.encoder = nn.Sequential(</span><br><span class="line">            conv_block(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line"></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>Would it be nice if we can define the sizes as an array and automatically create all the layers without writing each one of them? Fortunately we can create an array and pass it to <code>Sequential</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">        </span><br><span class="line">        conv_blocks = [conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.enc_sizes, self.enc_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(*conv_blocks)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Let’s break it down. We created an array <code>self.enc_sizes</code> that holds the sizes of our encoder. Then we create an array <code>conv_blocks</code> by iterating the sizes. Since we have to give booth a in size and an outsize for each layer we <code>zip</code>ed the size’array with itself by shifting it by one.</p>
<p>Just to be clear, take a look at the following example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sizes = [<span class="number">1</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> in_f,out_f <span class="keyword">in</span> zip(sizes, sizes[<span class="number">1</span>:]):</span><br><span class="line">    print(in_f,out_f)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 32</span><br><span class="line">32 64</span><br></pre></td></tr></table></figure>
<p>Then, since <code>Sequential</code> does not accept a list, we decompose it by using the <code>*</code> operator.</p>
<p>Tada! Now if we just want to add a size, we can easily add a new number to the list. It is a common practice to make the size a parameter.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        </span><br><span class="line">        conv_blokcs = [conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.enc_sizes, self.enc_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(*conv_blokcs)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>, <span class="number">128</span>], <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (2): Sequential(</span><br><span class="line">      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>We can do the same for the decoder part</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dec_block</span><span class="params">(in_f, out_f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Linear(in_f, out_f),</span><br><span class="line">        nn.Sigmoid()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, dec_sizes,  n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        self.dec_sizes = [<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, *dec_sizes]</span><br><span class="line"></span><br><span class="line">        conv_blokcs = [conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.enc_sizes, self.enc_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(*conv_blokcs)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        dec_blocks = [dec_block(in_f, out_f) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.dec_sizes, self.dec_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(*dec_blocks)</span><br><span class="line">        </span><br><span class="line">        self.last = nn.Linear(self.dec_sizes[<span class="number">-1</span>], n_classes)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>], [<span class="number">1024</span>, <span class="number">512</span>], <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">      (1): Sigmoid()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">      (1): Sigmoid()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (last): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>We followed the same pattern, we create a new block for the decoding part, linear + sigmoid, and we pass an array with the sizes. We had to add a <code>self.last</code> since we do not want to activate the output</p>
<p>Now, we can even break down our model in two! Encoder + Decoder</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_sizes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(enc_sizes, enc_sizes[<span class="number">1</span>:])])</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self.conv_blokcs(x)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dec_sizes, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(dec_sizes, dec_sizes[<span class="number">1</span>:])])</span><br><span class="line">        self.last = nn.Linear(dec_sizes[<span class="number">-1</span>], n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.dec_blocks()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, dec_sizes,  n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        self.dec_sizes = [<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, *dec_sizes]</span><br><span class="line"></span><br><span class="line">        self.encoder = MyEncoder(self.enc_sizes)</span><br><span class="line">        </span><br><span class="line">        self.decoder = MyDecoder(dec_sizes, n_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.flatten(<span class="number">1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>], [<span class="number">1024</span>, <span class="number">512</span>], <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): MyEncoder(</span><br><span class="line">    (conv_blokcs): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): ReLU()</span><br><span class="line">      )</span><br><span class="line">      (1): Sequential(</span><br><span class="line">        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): ReLU()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): MyDecoder(</span><br><span class="line">    (dec_blocks): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">        (1): Sigmoid()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (last): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Be aware that <code>MyEncoder</code> and <code>MyDecoder</code> could also be functions that returns a <code>nn.Sequential</code>. I prefer to use the first pattern for models and the second for building blocks.</p>
<p>By diving our module into submodules it is easier to <strong>share</strong> the code, <strong>debug</strong> it and <strong>test</strong> it.</p>
<p><a name="8xg9py"></a></p>
<h2 id="ModuleList-when-we-need-to-iterate"><a href="#ModuleList-when-we-need-to-iterate" class="headerlink" title="ModuleList : when we need to iterate"></a><a href="#8xg9py"></a>ModuleList : when we need to iterate</h2><p><code>ModuleList</code> allows you to store <code>Module</code> as a list. It can be useful when you need to iterate through layer and store/use some information, like in U-net.</p>
<p>The main difference between <code>Sequential</code> is that <code>ModuleList</code> have not a <code>forward</code> method so the inner layers are not connected. Assuming we need each output of each layer in the decoder, we can store it by:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([nn.Linear(in_f, out_f) <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(sizes, sizes[<span class="number">1</span>:])])</span><br><span class="line">        self.trace = []</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">            self.trace.append(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = MyModule([<span class="number">1</span>, <span class="number">16</span>, <span class="number">32</span>])</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model(torch.rand((<span class="number">4</span>,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">[print(trace.shape) <span class="keyword">for</span> trace <span class="keyword">in</span> model.trace]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 16])</span><br><span class="line">torch.Size([4, 32])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[None, None]</span><br></pre></td></tr></table></figure>
<p><a name="f7epth"></a></p>
<h2 id="ModuleDict-when-we-need-to-choose"><a href="#ModuleDict-when-we-need-to-choose" class="headerlink" title="ModuleDict: when we need to choose"></a><a href="#f7epth"></a>ModuleDict: when we need to choose</h2><p>What if we want to switch to <code>LearkyRelu</code> in our <code>conv_block</code>? We can use <code>ModuleDict</code> to create a dictionary of <code>Module</code> and dynamically switch <code>Module</code> when we want</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_f, out_f, activation=<span class="string">'relu'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">    </span><br><span class="line">    activations = nn.ModuleDict([</span><br><span class="line">                [<span class="string">'lrelu'</span>, nn.LeakyReLU()],</span><br><span class="line">                [<span class="string">'relu'</span>, nn.ReLU()]</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_f, out_f, *args, **kwargs),</span><br><span class="line">        nn.BatchNorm2d(out_f),</span><br><span class="line">        activations[activation]</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(conv_block(<span class="number">1</span>, <span class="number">32</span>,<span class="string">'lrelu'</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">print(conv_block(<span class="number">1</span>, <span class="number">32</span>,<span class="string">'relu'</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (2): LeakyReLU(negative_slope=0.01)</span><br><span class="line">)</span><br><span class="line">Sequential(</span><br><span class="line">  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (2): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a name="c51cxn"></a></p>
<h2 id="Final-implementation"><a href="#Final-implementation" class="headerlink" title="Final implementation"></a><a href="#c51cxn"></a>Final implementation</h2><p>Let’s wrap it up everything!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_f, out_f, activation=<span class="string">'relu'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">    activations = nn.ModuleDict([</span><br><span class="line">                [<span class="string">'lrelu'</span>, nn.LeakyReLU()],</span><br><span class="line">                [<span class="string">'relu'</span>, nn.ReLU()]</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_f, out_f, *args, **kwargs),</span><br><span class="line">        nn.BatchNorm2d(out_f),</span><br><span class="line">        activations[activation]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dec_block</span><span class="params">(in_f, out_f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Linear(in_f, out_f),</span><br><span class="line">        nn.Sigmoid()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_sizes, *args, **kwargs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, *args, **kwargs) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(enc_sizes, enc_sizes[<span class="number">1</span>:])])</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self.conv_blokcs(x)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dec_sizes, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(dec_sizes, dec_sizes[<span class="number">1</span>:])])</span><br><span class="line">        self.last = nn.Linear(dec_sizes[<span class="number">-1</span>], n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.dec_blocks()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, dec_sizes,  n_classes, activation=<span class="string">'relu'</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        self.dec_sizes = [<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, *dec_sizes]</span><br><span class="line"></span><br><span class="line">        self.encoder = MyEncoder(self.enc_sizes, activation=activation)</span><br><span class="line">        </span><br><span class="line">        self.decoder = MyDecoder(dec_sizes, n_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.flatten(<span class="number">1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>], [<span class="number">1024</span>, <span class="number">512</span>], <span class="number">10</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): MyEncoder(</span><br><span class="line">    (conv_blokcs): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): LeakyReLU(negative_slope=0.01)</span><br><span class="line">      )</span><br><span class="line">      (1): Sequential(</span><br><span class="line">        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): LeakyReLU(negative_slope=0.01)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): MyDecoder(</span><br><span class="line">    (dec_blocks): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">        (1): Sigmoid()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (last): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a name="8gyuvn"></a></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><a href="#8gyuvn"></a>Conclusion</h2><p>So, in summary.</p>
<ul>
<li><p>Use <code>Module</code> when you have a big block compose of multiple smaller blocks</p>
</li>
<li><p>Use <code>Sequential</code> when you want to create a small block from layers</p>
</li>
<li><p>Use <code>ModuleList</code> when you need to iterate through some layers or building blocks and do something</p>
</li>
<li><p>Use <code>ModuleDict</code> when you need to parametise some blocks of your model, for example an activation function</p>
</li>
</ul>
<p>That’s all folks!</p>
<p>Thank you for reading</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/20/yuque/使用新的fastai库实现世界一流的结果/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/20/yuque/使用新的fastai库实现世界一流的结果/" itemprop="url">使用新的fastai库实现世界一流的结果</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-20T14:20:52+08:00">
                2018-12-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://towardsdatascience.com/transfer-learning-using-the-fastai-library-d686b238213e" target="_blank" rel="noopener">链接</a></p>
<p><a name="hmoufh"></a></p>
<h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a><a href="#hmoufh"></a>动机</h3><p>我目前正在做一个fast.ai Live MOOC，名为《Practical Deep learning for Coders》，将于2019年1月在fast.ai网站上公开发布。 以下代码基于该课程的第1课。 我将使用位于Pytorch 1.0顶部的fastai V1库。 fastai库提供了许多有用的功能，使我们能够快速，轻松地构建神经网络并训练我们的模型。 我正在撰写此博客，作为在数据集上试验课程示例的一部分，该数据集在结构和复杂性上有所不同，并表明使用fastai库是多么容易。<br>在下面的例子中，您将看到在<a href="https://plantvillage.psu.edu/" target="_blank" rel="noopener">PlantVintage</a>数据集上进行转移学习并获得世界级结果是多么荒谬。 PlantVintage数据包含植物叶片的图像，其中包括通常在作物上发现的38种疾病类别，是来自斯坦福大学背景图像开放数据集的一个背景类别–DAGS。 我从这个<a href="https://github.com/MarkoArsenovic/DeepLearning_PlantDiseases" target="_blank" rel="noopener">Github Repo</a>上给出的链接下载了数据。 我对这个例子特别感兴趣，因为在我写这篇博客的时候，我为一个帮助农民发展业务的组织工作，提供产品和技术解决方案，以便更好地管理农场。 让我们开始吧！<br>PS：这个博客也在我的<a href="https://github.com/aayushmnit" target="_blank" rel="noopener">GitHub</a>个人资料中作为jupyter笔记本发布。<br><a name="5dzgih"></a></p>
<h3 id="导入快速AI库"><a href="#导入快速AI库" class="headerlink" title="导入快速AI库"></a><a href="#5dzgih"></a>导入快速AI库</h3><p>让我们导入fastai库并将我们的batch_size参数定义为64.通常，图像数据库很大，所以我们需要使用批量将这些图像提供给GPU，批量大小64意味着我们将一次提供64个图像来更新深度参数 学习模式。 如果由于GPU RAM较小而导致内存不足，则可以将批量大小减小到32或16。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br><span class="line">bs =<span class="number">64</span></span><br></pre></td></tr></table></figure></p>
<p><a name="vr3dmp"></a></p>
<h3 id="看数据"><a href="#看数据" class="headerlink" title="看数据"></a><a href="#vr3dmp"></a>看数据</h3><p>我们处理问题时首先要做的是查看数据。 在我们弄清楚如何解决问题之前，我们总是需要很好地理解问题是什么以及数据是什么样的。 查看数据意味着了解数据目录的结构，标签是什么以及一些示例图像是什么样的。 我们的数据已经在train和validation文件夹中分割，在每个子目录中，我们的文件夹名称代表该子文件夹中存在的所有图像的类名。 幸运的是，fastai库有一个方便的功能，<a href="https://docs.fast.ai/vision.data#ImageDataBunch.from_folder" target="_blank" rel="noopener">ImageDataBunch.from_folder</a>自动从文件夹名称中获取标签名称。 fastai库提供了很棒的文档来浏览它们的库函数，并提供有关如何使用它们的实例。 加载数据后，我们还可以使用.normalize到ImageNet参数来规范化数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Declaring path of dataset</span></span><br><span class="line">path_img = Path(<span class="string">'/home/jupyter/fastai_v3_experimentation/data/PlantVillage/'</span>)</span><br><span class="line"><span class="comment">## Loading data </span></span><br><span class="line">data = ImageDataBunch.from_folder(path=path_img, train=<span class="string">'train'</span>, valid=<span class="string">'valid'</span>, ds_tfms=get_transforms(),size=<span class="number">224</span>, bs=bs, check_ext=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">## Normalizing data based on Image net parameters</span></span><br><span class="line">data.normalize(imagenet_stats)</span><br></pre></td></tr></table></figure></p>
<p>要查看随机的图像样本，我们可以使用.show_batch（）函数ImageDataBunch类。 正如我们在下面所看到的，我们有一些不同作物上的疾病病例加上来自DAGS数据集的一些背景噪声图像，这些图像将作为噪声。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.show_batch(rows=<span class="number">3</span>, figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287666783-17e163b9-fba6-4ab9-9952-1168913169b0.png#width=826" alt><br>让我们打印数据库中存在的所有数据类。 总的来说，我们在动机部分中提到了39个课程中的图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(data.classes)</span><br><span class="line">len(data.classes),data.c</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287718206-c67e4447-9a83-4032-86c8-5d298c775b72.png#width=826" alt><br><a name="hzustc"></a></p>
<h3 id="使用预先训练的模型转移学习：ResNet-50"><a href="#使用预先训练的模型转移学习：ResNet-50" class="headerlink" title="使用预先训练的模型转移学习：ResNet 50"></a><a href="#hzustc"></a>使用预先训练的模型转移学习：ResNet 50</h3><p>现在我们将开始训练我们的模型。 我们将使用卷积神经网络骨干ResNet 50和具有单个隐藏层的完全连接头作为分类器。 如果您想了解所有架构细节，也可以阅读ResNet论文。 要创建转移学习模型，我们需要使用Learner类中的函数create_cnn，并从模型类中提供预先训练的模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## To create a ResNET 50 with pretrained weights</span></span><br><span class="line">learn = create_cnn(data, models.resnet50, metrics=error_rate)</span><br></pre></td></tr></table></figure></p>
<p>由create_cnn函数创建的ResNet50模型具有冻结的初始层，我们将学习最后完全连接的层的权重。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit_one_cycle(<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287818426-ef1da8b8-26e2-4bee-835a-abfa2386fa31.png#width=584" alt><br>正如我们在上面看到的那样，只使用默认设置运行五个周期，我们对这个细粒度分类任务的准确度在验证数据集上约为99.64％。 让我们保存模型，因为我们稍后会对其进行微调。 如果你想知道这个结果有多好，它已经超过了这个<a href="https://github.com/MarkoArsenovic/DeepLearning_PlantDiseases" target="_blank" rel="noopener">Github Page</a>的96.53％的浅层学习（仅培训最后一层）基准。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.save(<span class="string">'plant_vintage_stage1'</span>)</span><br></pre></td></tr></table></figure></p>
<p>FastAI库还提供了更快地探索结果的功能，并查找我们的模型是否正在学习应该学习的内容。 我们将首先看到模型最混淆的类别。 我们将尝试使用ClassificationInterpretation类来查看模型预测的是否合理。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassificationInterpretation.from_learner(learn)</span><br><span class="line">interp.plot_top_losses(<span class="number">4</span>, figsize=(<span class="number">20</span>,<span class="number">25</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287958131-9082d640-9b93-482c-8ed4-7becd11c97b9.png#width=826" alt><br>在这种情况下，该模型在从玉米植株上的灰色叶斑病和番茄叶片中的早/晚疫病检测北叶枯病方面变得混乱，其在视觉上看起来非常相似。 这是我们的分类器正常工作的指示器。 此外，当我们绘制混淆矩阵时，我们可以看到大多数事物都被正确分类，并且它几乎是一个接近完美的模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.plot_confusion_matrix(figsize=(<span class="number">20</span>,<span class="number">20</span>), dpi=<span class="number">60</span>)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545288034116-dbb5f685-7a96-4031-8b2b-e54abae02d81.png#width=826" alt><br>所以到目前为止，我们只训练了最后的分类层，但是如果我们想要优化早期的层也会如此。 在迁移学习中，应谨慎调整初始图层，学习率应保持在较低水平。 FastAI库提供了一个功能，可以查看要训练的理想学习速率，让我们绘制它。 lr_find函数以多学习速率运行数据子集的模型，以确定哪种学习速率最佳。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.lr_find()</span><br><span class="line">learn.recorder.plot()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545288100487-9ddefc18-b8fb-4fe1-8232-8cc43139a730.png#width=447" alt><br>看起来我们应该保持低于10e-4的学习率。 对于网络中的不同层，我们可以使用切片函数来对数分布10e-6到10e-4之间的学习速率。 保持初始层的最低学习速率，并为后续层增加它。 让我们解冻所有层，以便我们可以使用unfreeze()函数训练整个模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.unfreeze()</span><br><span class="line">learn.fit_one_cycle(<span class="number">2</span>, max_lr=slice(<span class="number">1e-7</span>,<span class="number">1e-5</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545288161256-736a1108-3167-4d5f-be85-2ab6317a60c3.png#width=528" alt><br>正如我们通过训练所有层次所看到的，我们将准确度提高到99.7％，这与使用Inception-v3模型的Github基准测试中的99.76％相当。<br><a name="zv6xnx"></a></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><a href="#zv6xnx"></a>结论</h3><p>Fast.ai是Jeremy Howard和他的团队的一项出色的倡议，我相信fastai库可以通过使构建深度学习模型变得非常简单，真正实现将深度学习民主化的动机。<br><br>我希望你喜欢阅读，并随意使用我的代码为你的目的尝试。 此外，如果对代码或博客文章有任何反馈，请随时联系LinkedIn或发送电子邮件至<a href="mailto:aayushmnit@gmail.com" target="_blank" rel="noopener">aayushmnit@gmail.com</a>。<br><br>PS：如果你喜欢这些内容，请留下评论或鼓掌，并希望我更频繁地写这样的博客。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/20/yuque/XGBoost不是黑魔法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/20/yuque/XGBoost不是黑魔法/" itemprop="url">XGBoost不是黑魔法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-20T13:48:34+08:00">
                2018-12-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://towardsdatascience.com/xgboost-is-not-black-magic-56ca013144b4" target="_blank" rel="noopener">链接</a></p>
<blockquote>
<p>而不是对缺失值进行估算并不总是正确的选择</p>
</blockquote>
<p><br>现在很容易在数据科学任务中获得不错的结果：只需要对流程有一个大致的了解，Python的基本知识和10分钟的时间来实例化XGBoost并适应模型。好的，如果这是你的第一次，那么你可能会花几分钟通过pip收集所需的包，但就是这样。这种方法的唯一问题是它运作得很好🤷🏻♂️：几年前我在大学竞赛中排名前5位，只是通过一些基本的特征工程将数据集提供给XGBoost，表现优于团队非常复杂的架构和数据管道。 XGBoost最酷的特征之一就是它如何处理缺失值：决定每个样本，这是最好的方法来判断它们。对于我在过去几个月中遇到的许多项目和数据集，此功能非常有用;为了更加值得以我的名义撰写的数据科学家的头衔，我决定深入挖掘，花几个小时阅读原始论文，试图了解XGBoost究竟是什么以及它如何处理它以某种神奇的方式缺少价值。<br><a name="7vdtxk"></a></p>
<h4 id="从决策树到XGBoost"><a href="#从决策树到XGBoost" class="headerlink" title="从决策树到XGBoost"></a><a href="#7vdtxk"></a>从决策树到XGBoost</h4><p>决策树可能是机器学习中最简单的算法：树的每个节点都是对特征的测试，每个分支代表测试的结果; leaves包含模型的输出，无论是离散标签还是实数。 决策树可能被描述为一个功能：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285357509-9eb19212-1f84-4151-925f-695f322ba1a9.png#width=826" alt><br>函数f根据从根到叶子的路径，根据树结构T分配m大小的样本x所遵循的权重w。<br><br>现在想象一下，不只有一棵决策树而且还有K; 最终产生的输出不再是与叶子相关的权重，而是与每棵树产生的叶子相关的权重之和。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285385079-d2797243-1542-4e33-8215-bde9adc10d39.png#width=826" alt></p>
<p>这些结构不是固定的，并且与网络结构不变的经典梯度下降框架中发生的不同，并且在每个步骤更新权重时，在每次迭代时添加新函数（树）以改善模型的性能。 为了避免过度拟合和/或非常复杂的结构，误差由两部分组成：第一部分对在第k次迭代中获得的模型的优度进行评分，第二部分在相关权重的大小中惩罚复杂性。 叶子和发达的树木的深度和结构。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285422201-bef8962d-373e-4670-8789-f8fffb46cbc2.png#width=826" alt><br>然后使用二阶梯度统计简化该目标函数，并且 - 不输入太多细节 - 可以直接用于以封闭形式计算与固定树结构相关联的最佳叶子权重。 权重可以直接与错误相关联，因此与所使用的固定结构的优点相关联（3）。<br><br>训练XGBoost是一个迭代过程，它在每个步骤计算第k个树的最佳可能分割，该第k个树枚举路径中该点仍然可用的所有可能结构。 所有可能的分裂的详尽列举非常适合本文的范围，但在实践中是不可行的，并且它被一个近似版本所取代，该版本不会尝试所有可能的分裂，而是根据百分位数列举相关的分裂。 每个功能分布。<br><a name="e5q4ag"></a></p>
<h3 id="XGBoost和缺失值：魔术发生的地方"><a href="#XGBoost和缺失值：魔术发生的地方" class="headerlink" title="XGBoost和缺失值：魔术发生的地方"></a><a href="#e5q4ag"></a>XGBoost和缺失值：魔术发生的地方</h3><p>一旦树结构被训练，就不难考虑测试集中是否存在缺失值：它足以将默认方向附加到每个决策节点。如果缺少样本的特征并且决策节点在该特征上分裂，则路径采用分支的默认方向并且路径继续。但是为每个分支分配默认方向更复杂，这可能是本文中最有趣的部分。<br><br>已经解释的拆分查找算法可以稍微调整一下，不仅返回每一步的最佳拆分，而且还返回分配给新插入的决策节点的默认方向。给定一个特征集I，枚举所有可能的分割，但是现在相应的丢失不会被计算一次而是两次，每个默认方向一次丢失该特征的缺失值。两者中最好的是根据特征m的值j进行分割时分配的最佳默认方向。最佳分割仍然是最大化计算分数的分割，但现在我们已经为其附加了默认方向。<br><br>这种算法被称为稀疏感知的分裂发现，它是XGBoost背后的许多魔力所在。最后不要太复杂。稀疏性感知方法仅保证在已经遍历的分裂的情况下平均采用默认方向导致最佳可能结果，并不保证已经遍历的分裂（可能通过采用默认方向来解决）是最好的考虑整个样本。如果样本中缺失值的百分比增加，则内置策略的性能可能会恶化很多。</p>
<blockquote>
<p><em>好的，默认方向是最佳选择，只要它到达当前位置，但考虑到当前样本的所有特征，无法保证当前位置是最佳情况。</em></p>
</blockquote>
<p>克服此限制意味着处理同时考虑其所有特征的样本，并直接处理同一实现中可能同时存在多个缺失值。<br><a name="z5afml"></a></p>
<h3 id="改变缺失值并改善表现"><a href="#改变缺失值并改善表现" class="headerlink" title="改变缺失值并改善表现"></a><a href="#z5afml"></a>改变缺失值并改善表现</h3><p>为了击败XGBoost内置策略，我们必须同时考虑样本的所有功能，并以某种方式处理可能存在的缺失值。 这种方法的一个很好的例子是K-Nearest Neighbors（KNN），它具有ad-hoc距离度量以正确处理缺失值。 一般而言，KNN是众所周知的算法，其将K（例如，3,10,50，……）最接近的样本检索到所考虑的样本。 它可以用于对看不见的输入进行分类或者用于估算缺失值，在分配给目标值的情况下，考虑K个最近邻居的均值或中值。 这种方法需要距离度量（或相应地，相似性度量）来实际对训练集中的所有样本进行排序并检索最相似的K.<br>要超越XGBoost内置默认策略，我们需要两件事：</p>
<ul>
<li>考虑缺失值的距离指标（感谢AirBnb的这篇<a href="https://medium.com/airbnb-engineering/overcoming-missing-values-in-a-random-forest-classifier-7b1fc1fc03ba" target="_blank" rel="noopener">文章</a>的灵感）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_with_miss</span><span class="params">(a,b,l=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>(len(a) != len(b)):</span><br><span class="line">        <span class="keyword">return</span> np.inf</span><br><span class="line">    ls = l * np.ones(len(a))</span><br><span class="line">    msk = ~ (np.isnan(a) | np.isnan(b))</span><br><span class="line">    res = np.sum((np.abs(a-b)[msk]))+np.sum((ls[~msk]))</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<ul>
<li>规范化数据集以获得有意义的距离，获得了不同域之间特征之间的差异（XGBoost并不严格要求，但KNN估算需要它！）。</li>
</ul>
<p>使用K个最接近样本的所述特征的中值来估算特征的缺失值，并且在非特定情况下，在K个检索的邻居中不发现至少一个非缺失值，整个列的中值 用来。<br><a name="kp0eey"></a></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><a href="#kp0eey"></a>实验结果</h3><p>我使用scikit-learn中免费提供的三个众所周知的数据集（两个分类和一个回归）进行了一些测试。 通过k-fold交叉验证比较三种不同的插补策略，测量了性能：</p>
<ul>
<li><p>XGBoost算法中内置的默认值</p>
</li>
<li><p>一个简单的列中位插值</p>
</li>
<li><p>一个KNN，如前一段所述</p>
</li>
</ul>
<p>对于KNN案例，我已经绘制了针对考虑的缺失值百分比获得的最佳性能，其中k与（要考虑的邻居的数量）和λ（当至少一个特征缺失时要添加到距离的常数） 两个样本）。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285831963-0ba52730-2b35-4d6a-9581-f84258cd72e8.png#width=826" alt><br>使用稀疏性感知KNN来估算缺失值与其他两种方法的表现一致。 差异的程度当然是数据集依赖的。 作为第一个天真的结论：数据集的质量越低，更好的插补策略的影响越大。 如图2所示，内置策略最终具有接近于平凡的列中值插值的性能。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285859844-3faa8e2e-ebc5-4521-adb5-28b05b251281.png#width=826" alt><br>看看k和λ如何影响最终结果以及如何引入惩罚因素不仅仅是纸上谈兵，这是非常有趣的。 距离度量不仅丢弃缺失值而且还为每一个增加权重对于用该方法获得的性能是至关重要的，即使其值与缺失值的增加百分比不直接相关。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285901042-1590c9c8-9d75-42b3-b76d-3810e74a8f62.png#width=826" alt><br>测试表明，根据经验，缺失值的数量越多，为更好的插补而考虑的邻居数量越多。 再次，一个非常直观的结论。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/认识和实现：批量标准化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/19/yuque/认识和实现：批量标准化/" itemprop="url">认识和实现：批量标准化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T22:43:17+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b" target="_blank" rel="noopener">链接</a><br>在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。<br><a name="worndf"></a></p>
<h3 id="批量归一化的直观解释"><a href="#批量归一化的直观解释" class="headerlink" title="批量归一化的直观解释"></a><a href="#worndf"></a>批量归一化的直观解释</h3><p><a name="iegmed"></a></p>
<h3 id="训练中的问题"><a href="#训练中的问题" class="headerlink" title="训练中的问题"></a><a href="#iegmed"></a>训练中的问题</h3><p>问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。<br>不同批次分布的另一个影响是消失的梯度。 消失的梯度问题是一个大问题，特别是对于S形激活函数。 如果g（x）表示sigmoid激活函数，则为| x | 增加，g’（x）趋于零。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230849299-89520b64-5633-46b5-a7ab-8343e37bce6d.png#width=731" alt><br>问题2.当输入分布变化时，神经元输出也会变化。 这导致神经元输出偶尔波动到S形函数的可饱和区域。 在那里，神经元既不能更新自己的权重，也不能将梯度传递回先前的层。 我们如何保持神经元输出不变为可饱和区域？</p>
<p>如果我们可以将神经元输出限制在零附近的区域，我们可以确保每个层在反向传播期间都会传回一个实质的梯度。 这将导致更快的训练时间和更准确的结果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230974089-30772f12-78be-446d-ac94-1f658676b64c.png#width=731" alt><br><a name="laeagu"></a></p>
<h4 id="批量标准作为解决方案。"><a href="#批量标准作为解决方案。" class="headerlink" title="批量标准作为解决方案。"></a><a href="#laeagu"></a>批量标准作为解决方案。</h4><p>批量标准化减轻了不同层输入的影响。 通过归一化神经元的输出，激活函数将仅接收接近零的输入。 这确保了非消失的梯度，解决了第二个问题。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231047448-2503dee7-466f-4472-a4d3-d4051a58af2f.png#width=747" alt><br>批量归一化将层输出转换为单位高斯分布。 当这些输出通过激活功能馈送时，层激活也将变得更加正常分布。<br>由于一层的输出是下一层的输入，因此层输入现在具有明显较小的批次间差异。 通过减少层输入的变化分布，我们解决了第一个问题。<br><a name="6w4qpz"></a></p>
<h3 id="数学解释"><a href="#数学解释" class="headerlink" title="数学解释"></a><a href="#6w4qpz"></a>数学解释</h3><p>通过批量归一化，我们为每个激活函数寻找以零为中心的单位方差分布。 在训练期间，我们采用激活输入x并将其减去批次均值μ以实现零中心分布。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231155587-c517033a-5650-4858-8dc8-fe71e96bb8f0.png#width=204" alt><br>接下来，我们取x并将其除以批量方差和一个小数，以防止除以零σ+ε。 这可确保所有激活输入分布都具有单位差异。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231189951-b72be3c8-1a47-42a5-bd6d-04e2ba9a6006.png#width=215" alt><br>最后，我们将x hat进行线性转换以缩放并移动批量标准化的输出。 尽管在反向传播期间网络发生了变化，但仍能确保保持这种正常化效果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231250609-5da7503e-4334-4d00-9e24-61689bab04a2.png#width=231" alt><br>在测试模型时，我们不使用批处理均值或方差，因为这会破坏模型。 （提示：单个观察的平均值和方差是多少？）相反，我们计算训练群体的移动平均值和方差估计值。 这些估计值是培训期间计算的所有批次平均值和方差的平均值。<br><a name="kg00mo"></a></p>
<h3 id="批量标准化的好处"><a href="#批量标准化的好处" class="headerlink" title="批量标准化的好处"></a><a href="#kg00mo"></a>批量标准化的好处</h3><p>批量标准化的好处如下。<br>1.有助于防止具有可饱和非线性（sigmoid，tanh等）的网络中的消失梯度<br>通过批量标准化，我们确保任何激活函数的输入不会变为可饱和区域。 批量归一化将这些输入的分布转换为单位高斯（零中心和单位方差）。<br>2.规范模型<br>也许。 Ioffe和Svegeddy提出了这一主张，但没有就此问题进行广泛撰写。 也许这是归一化层输入的结果？<br>3.允许更高的学习率<br>通过在训练期间防止梯度消失的问题，我们可以设置更高的学习率。 批量标准化还降低了对参数标度的依赖性。 较大的学习速率可以增加层参数的规模，这导致梯度在反向传播期间回传时放大。 我需要阅读更多关于此的内容。<br><a name="5qelzv"></a></p>
<h3 id="在Keras实施"><a href="#在Keras实施" class="headerlink" title="在Keras实施"></a><a href="#5qelzv"></a>在Keras实施</h3><p>引入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import keras</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.image as mpimg</span><br><span class="line"></span><br><span class="line">from keras.models import Model, Sequential</span><br><span class="line">from keras.layers import Input</span><br><span class="line"></span><br><span class="line">from keras.callbacks import ModelCheckpoint, EarlyStopping</span><br><span class="line">from keras.layers import BatchNormalization</span><br><span class="line">from keras.layers import GlobalAveragePooling2D</span><br><span class="line">from keras.layers import Activation</span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Dense</span><br><span class="line">from keras.layers import MaxPooling2D, Dropout, Flatten</span><br><span class="line"></span><br><span class="line">import time</span><br></pre></td></tr></table></figure></p>
<p>数据加载和预处理<br>在这笔记本中，我们使用Cifar 100数据集，因为它具有相当的挑战性，并且不会永远用于训练。 唯一的预处理是零中心和图像变化发生器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from keras.datasets import cifar100</span><br><span class="line">from keras.utils import np_utils</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=&apos;fine&apos;)</span><br><span class="line"></span><br><span class="line">#scale and regularize the dataset</span><br><span class="line">x_train = (x_train-np.mean(x_train))</span><br><span class="line">x_test = (x_test - x_test.mean())</span><br><span class="line"></span><br><span class="line">x_train = x_train.astype(&apos;float32&apos;)</span><br><span class="line">x_test = x_test.astype(&apos;float32&apos;)</span><br><span class="line"></span><br><span class="line">#onehot encode the target classes</span><br><span class="line">y_train = np_utils.to_categorical(y_train)</span><br><span class="line">y_test = np_utils.to_categorical(y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">        shear_range=0.2,</span><br><span class="line">        zoom_range=0.2,</span><br><span class="line">        horizontal_flip=True)</span><br><span class="line"></span><br><span class="line">train_datagen.fit(x_train)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow(x_train,</span><br><span class="line">                                     y = y_train,</span><br><span class="line">                                    batch_size=80,)</span><br></pre></td></tr></table></figure></p>
<p><a name="kl8xlg"></a></p>
<h3 id="在Keras中构建模型"><a href="#在Keras中构建模型" class="headerlink" title="在Keras中构建模型"></a><a href="#kl8xlg"></a>在Keras中构建模型</h3><p>我们的架构将包括堆叠的3x3卷积，然后是最大池化和dropout。 每个网络中有5个卷积块。 最后一层是一个完全连接的层，有100个节点和softmax激活。<br>我们将构建4个不同的卷积网络，每个网络都具有sigmoid或ReLU激活以及批量标准化或不标准化。 我们将比较每个网络的验证损失。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">def conv_block_first(model, bn=True, activation=&quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    The first convolutional block in each architecture. Only    separate so we can specify the input shape.</span><br><span class="line">    &quot;&quot;&quot;    </span><br><span class="line">   #First Stacked Convolution</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;, input_shape =   x_train.shape[1:]))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line">    #Second Stacked Convolution</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(MaxPooling2D())</span><br><span class="line">    model.add(Dropout(0.15))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def conv_block(model, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Generic convolutional block with 2 stacked 3x3 convolutions, max pooling, dropout, </span><br><span class="line">    and an optional Batch Normalization.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(MaxPooling2D())</span><br><span class="line">    model.add(Dropout(0.15))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def conv_block_final(model, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    I bumped up the number of filters in the final block. I made this separate so that I might be able to integrate Global Average Pooling later on. </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Conv2D(100,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(100,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def fn_block(model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    I&apos;m not going for a very deep fully connected block, mainly so I can save on memory.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Dense(100, activation = &quot;softmax&quot;))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def build_model(blocks=3, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Builds a sequential network based on the specified parameters.</span><br><span class="line">    </span><br><span class="line">    blocks: number of convolutional blocks in the network, must be greater than 2.</span><br><span class="line">    bn: whether to include batch normalization or not.</span><br><span class="line">    activation: activation function to use throughout the network.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model = Sequential()</span><br><span class="line"></span><br><span class="line">    model = conv_block_first(model, bn=bn, activation=activation)</span><br><span class="line"></span><br><span class="line">    for block in range(1,blocks-1):</span><br><span class="line">        model = conv_block(model, bn=bn, activation = activation)</span><br><span class="line"></span><br><span class="line">    model = conv_block_final(model, bn=bn, activation=activation)</span><br><span class="line">    model = fn_block(model)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def compile_model(model, optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;]):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compiles a neural network.</span><br><span class="line">    </span><br><span class="line">    model: the network to be compiled.</span><br><span class="line">    optimizer: the optimizer to use.</span><br><span class="line">    loss: the loss to use.</span><br><span class="line">    metrics: a list of keras metrics.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.compile(optimizer = optimizer,</span><br><span class="line">                 loss = loss,</span><br><span class="line">                 metrics = metrics)</span><br><span class="line">    return model</span><br><span class="line">#COMPILING THE 4 MODELS</span><br><span class="line">sigmoid_without_bn = build_model(blocks = 5, bn=False, activation = &quot;sigmoid&quot;)</span><br><span class="line">sigmoid_without_bn = compile_model(sigmoid_without_bn)</span><br><span class="line"></span><br><span class="line">sigmoid_with_bn = build_model(blocks = 5, bn=True, activation = &quot;sigmoid&quot;)</span><br><span class="line">sigmoid_with_bn = compile_model(sigmoid_with_bn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">relu_without_bn = build_model(blocks = 5, bn=False, activation = &quot;relu&quot;)</span><br><span class="line">relu_without_bn = compile_model(relu_without_bn)</span><br><span class="line"></span><br><span class="line">relu_with_bn = build_model(blocks = 5, bn=True, activation = &quot;relu&quot;)</span><br><span class="line">relu_with_bn = compile_model(relu_with_bn)</span><br></pre></td></tr></table></figure></p>
<p><a name="c4xpfd"></a></p>
<h3 id="模特训练"><a href="#模特训练" class="headerlink" title="模特训练"></a><a href="#c4xpfd"></a>模特训练</h3><p>没有批量标准化的Sigmoid<br>训练陷入困境。 有100个课程，这个模型从未达到比随机猜测更好的性能（10％准确度）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history1 = sigmoid_without_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        epochs=20,</span><br><span class="line">        verbose=0,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231717659-060d4678-06bf-4045-abc1-cd7ce7cb6c77.png#width=743" alt><br><a name="m0uloq"></a></p>
<h3 id="具有批量标准化的Sigmoid"><a href="#具有批量标准化的Sigmoid" class="headerlink" title="具有批量标准化的Sigmoid"></a><a href="#m0uloq"></a>具有批量标准化的Sigmoid</h3><p>与没有批量标准化不同，该模型在训练期间开始实施。 这可能是批量标准化减轻消失梯度的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history2 = sigmoid_with_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        verbose=0,</span><br><span class="line">        epochs=20,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231810973-aebf7896-6c50-4d6c-8372-702891142c6f.png#width=724" alt><br><a name="w5h1ml"></a></p>
<h3 id="没有批量标准化的ReLU"><a href="#没有批量标准化的ReLU" class="headerlink" title="没有批量标准化的ReLU"></a><a href="#w5h1ml"></a>没有批量标准化的ReLU</h3><p>在没有批量规范的情况下实施ReLU导致一些初始收益，然后收敛到非最优的局部最小值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history3 = relu_without_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        epochs=20,</span><br><span class="line">        verbose=0,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231864866-451c1cd9-d6ae-4f7e-9aa2-1da0b34f889b.png#width=737" alt><br>具有批量标准化的ReLU<br>与sigmoid模型一样，批量标准化提高了该网络的训练能力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history4 = relu_with_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        verbose=0,</span><br><span class="line">        epochs=20,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231931611-3c026d90-61b0-41df-b4dc-7a538bf01ce6.png#width=730" alt><br><a name="e5gkmq"></a></p>
<h3 id="比较架构"><a href="#比较架构" class="headerlink" title="比较架构"></a><a href="#e5gkmq"></a>比较架构</h3><p>我们在这里清楚地看到批量标准化的好处。 没有批量标准化的ReLU和S形模型都无法保持训练性能提升。 这可能是渐变消失的结果。 具有批量标准化的体系结构训练得更快，并且比没有批量标准化的体系结构表现更好。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231970491-afc0113d-a1fb-4218-90d5-7104d35cb98b.png#width=724" alt><br><a name="0457"></a></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><a href="#0457"></a>Conclusion</h3><p>结论<br>批量标准化减少了训练时间并提高了神经网络的稳定性。 此效果适用于sigmoid和ReLU激活功能。 原帖可以在我的<a href="https://www.harrisonjansma.com/" target="_blank" rel="noopener">网站</a>上找到，代码可以在我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/tree/master/07-28-18-Implementing-Batch-Norm" target="_blank" rel="noopener">GitHub</a>上找到。<br><a name="f6b6"></a></p>
<h3 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a><a href="#f6b6"></a>Resources</h3><ul>
<li><p>Original paper by Ioffe and Szegedy. <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">here.</a></p>
</li>
<li><p>Insert a batch normalization before or after nonlinearities? <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="noopener">Usage explanation</a></p>
</li>
<li><p>For an explanation of the math and implementation in TensorFlow. <a href="https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8" target="_blank" rel="noopener">Pitfalls of Batch Norm</a></p>
</li>
<li><p>Also this post <a href="https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73" target="_blank" rel="noopener">How to use Batch Normalization with TensorFlow and tf.keras</a></p>
</li>
</ul>
<p><a name="3277"></a></p>
<h3 id="Further-reading"><a href="#Further-reading" class="headerlink" title="Further reading"></a><a href="#3277"></a>Further reading</h3><p>Below are some more recent research papers that extend Ioffe and Svegedy’s work.<br><a href="https://arxiv.org/abs/1702.03275v2" target="_blank" rel="noopener">[1]</a> How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)<br><a href="https://arxiv.org/abs/1702.03275v2" target="_blank" rel="noopener">[2]</a> Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models<br><a href="https://arxiv.org/abs/1607.06450v1" target="_blank" rel="noopener">[3]</a> Layer Normalization<br><a href="https://arxiv.org/abs/1602.07868v3" target="_blank" rel="noopener">[4]</a> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks<br><a href="https://arxiv.org/abs/1803.08494v3" target="_blank" rel="noopener">[5]</a> Group Normalization</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/不要在卷积网络中使用Dropout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/19/yuque/不要在卷积网络中使用Dropout/" itemprop="url">不要在卷积网络中使用Dropout</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T22:04:13+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16" target="_blank" rel="noopener">链接</a></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545228300354-eac11f1e-9363-4b7f-a68d-3b8e81fd80e0.png#width=747" alt><br>如果您想知道如何使用dropout，这里有您要的答案。</p>
<p>我注意到有很多资源可以用来学习深度学习的内容和原因。 不幸的是，当需要制作模型时，他们很少有资源来解释何时以及如何。<br>我正在为试图实施深度学习的其他数据科学家撰写本文。 因此，您不必像我一样通过研究文章和Reddit讨论。<br>在本文中，您将了解为什么dropout在卷积体系结构中不再受欢迎。</p>
<p><a name="drs4ub"></a></p>
<h4 id="DROPOUT"><a href="#DROPOUT" class="headerlink" title="DROPOUT"></a><a href="#drs4ub"></a>DROPOUT</h4><p>如果你正在读这篇文章，我认为你已经了解了什么是dropout，以及它在正则化神经网络方面的作用。 如果您想要复习，请阅读Amar Budhiraja的这篇文章。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545228517714-8daebec1-a6f7-4fcc-b4a8-8c826e025c76.png#width=747" alt><br>通常，当我们的网络存在过度拟合的风险时，我们只需要实现正规化。 如果网络太大，如果您训练时间过长，或者您没有足够的数据，则会发生这种情况。<br>如果在卷积网络末端有完全连接的层，则实现dropout很容易。<br><a name="qseuim"></a></p>
<h4 id="使用Keras"><a href="#使用Keras" class="headerlink" title="使用Keras"></a><a href="#qseuim"></a>使用Keras</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=None, seed=None)</span><br></pre></td></tr></table></figure>
<p>以0.5的dropout率开始并将其调低，直到性能最大化。 （<a href="https://www.reddit.com/r/MachineLearning/comments/3oztvk/why_50_when_using_dropout/" target="_blank" rel="noopener">资源</a>）<br><a name="dx1bhs"></a></p>
<h4 id="例如"><a href="#例如" class="headerlink" title="例如"></a><a href="#dx1bhs"></a>例如</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model=keras.models.Sequential()</span><br><span class="line">model.add(keras.layers.Dense(150, activation=&quot;relu&quot;))</span><br><span class="line">model.add(keras.layers.Dropout(0.5))</span><br></pre></td></tr></table></figure>
<p>请注意，这仅适用于您的convnet的完全连接区域。 对于所有其他地区，您不应使用dropout。<br>相反，您应该在卷积之间插入批量标准化。 这将使您的模型正常化，并使您的模型在训练期间更加稳定。<br><a name="042axx"></a></p>
<h4 id="批正则化"><a href="#批正则化" class="headerlink" title="批正则化"></a><a href="#042axx"></a>批正则化</h4><p>批标准化是规范卷积网络的另一种方法。<br>除了正则化效应之外，批量归一化还可以使您的卷积网络在训练期间抵抗消失的梯度。 这可以减少训练时间并获得更好的性能。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545229002104-b14e8bf0-9ba5-4470-85c3-ac86fed74c75.png#width=747" alt><br>批量标准化可以消除消失的梯度<br><br><br></p>
<p><a name="uat1zt"></a></p>
<h4 id="Keras实施"><a href="#Keras实施" class="headerlink" title="Keras实施"></a><a href="#uat1zt"></a>Keras实施</h4><p>要在Keras中实现批量标准化，请使用以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.BatchNormalization()</span><br></pre></td></tr></table></figure></p>
<p>构建具有批量规范化的卷积体系结构时：</p>
<ul>
<li><p>在卷积和激活层之间插入批量标准化层。 （<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="noopener">资源</a>）</p>
</li>
<li><p>您可以在此功能中调整一些超参数，并使用它们。</p>
</li>
</ul>
<p>您也可以在激活功能之后插入批量标准化，但根据我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/blob/master/08-12-18%20Batch%20Norm%20vs%20Dropout/08-12-18%20Batch%20Norm%20vs%20Dropout.ipynb" target="_blank" rel="noopener">经验</a>，这两种方法都具有相似的性能。<br><a name="i9denc"></a></p>
<h4 id="例如-1"><a href="#例如-1" class="headerlink" title="例如"></a><a href="#i9denc"></a>例如</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(&quot;relu&quot;))</span><br></pre></td></tr></table></figure>
<p>批量标准化取代了dropout。<br>即使您不需要担心过度拟合，实现批量标准化也有很多好处。 正因为如此，它的正规化效应，批量归一化已经在很大程度上取代了现代卷积体系结构中的dropout。<br>“我们提出了一种使用批量规范化网络构建，训练和执行推理的算法。 由此产生的网络可以通过饱和非线性进行训练，更能容忍增加的训练率，并且通常不需要Dropout进行正规化。“ -  Ioffe and Svegedy 2015<br>至于为什么dropout在最近的应用中失宠，主要有两个原因。<br><strong>首先</strong>，在对卷积层进行正则化时，dropout通常不太有效。<br>原因？ 由于卷积层具有很少的参数，因此它们开始时需要较少的正则化。 此外，由于在特征图中编码的空间关系，激活可以变得高度相关。 这使得dropout无效。（<a href="https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/" target="_blank" rel="noopener">资源</a>）<br><strong>其次</strong>，擅长正规化的dropout现在已经过时了。<br>像VGG16这样在网络末端包含完全连接的层的大型模型。 对于这样的模型，过度拟合是通过在完全连接的层之间包括dropout来解决的。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545229692686-cc1c012c-7f9a-4865-b6eb-fe547e0f1213.png#width=470" alt><br>不幸的是，<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">最近的架构</a>远离了这个完全连接块。<br>通过用全局平均池替换密集层，现代的网络可以减少模型大小，同时提高性能。<br>我将在未来再写一篇文章，详细说明如何在卷积网络中实现全球平均汇集。 在此之前，我建议阅读<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">ResNet论文</a>，以了解GAP的好处。<br><a name="btyoyw"></a></p>
<h4 id="一个实验"><a href="#一个实验" class="headerlink" title="一个实验"></a><a href="#btyoyw"></a>一个实验</h4><p>我创建了一个实验来测试批量标准化是否会减少在卷积之间插入时的泛化错误。 （<a href="https://github.com/harrisonjansma/Research-Computer-Vision/blob/master/08-12-18%20Batch%20Norm%20vs%20Dropout/08-12-18%20Batch%20Norm%20vs%20Dropout.ipynb" target="_blank" rel="noopener">链接</a>）<br>我构建了5个相同的卷积体系结构，并在卷积之间插入了dropout，批量规范或任何（控制）。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545229984300-3501e371-42ce-41de-8aec-2d6962959cd3.png#width=500" alt><br>通过在Cifar100数据集上训练每个模型，我获得了以下结果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230018898-2895598f-bf57-42f7-ad4f-b7440add1220.png#width=724" alt><br>批量标准化模型的良好表现说明应在卷积之间使用批量标准化。<br>此外，不应在卷基层之间放置dropout，因为dropout的模型往往比控制模型表现更差。<br>有关更多信息，请查看我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/blob/master/08-12-18%20Batch%20Norm%20vs%20Dropout/08-12-18%20Batch%20Norm%20vs%20Dropout.ipynb" target="_blank" rel="noopener">GitHub</a>上的完整文章。<br>小贴士<br>如果你想知道是否应该在卷积网络中实现dropout，现在你知道了。 仅在完全连接的层上使用dropout，并在卷积之间实现批量标准化。<br>如果您想了解有关批量标准化的更多信息，请阅读：<br><a href="https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b" target="_blank" rel="noopener">https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/" itemprop="url">理解二进制交叉熵、对数损失函数:一种可视化解释</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T16:27:20+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" target="_blank" rel="noopener">链接</a></p>
<p><a name="61a3ec66"></a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><a href="#ba72yr"></a>介绍</h2><p>如果您正在训练二进制分类器，则可能使用二进制交叉熵/对数损失作为损失函数。<br>你有没有想过使用这种损失函数究竟是什么意思？ 问题是，考虑到今天的库和框架的易用性，很容易忽略所使用的损失函数的真正含义。<br>动机<br>我正在寻找一篇博文，以一种视觉上清晰简洁的方式解释二进制交叉熵/对数损失背后的概念，所以我可以在Data Science Retreat向我的学生展示它。 由于我找不到任何符合我目的的东西，我自己负责编写任务:-)。<br>一个简单的分类问题<br>让我们从10个随机点开始：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</span><br></pre></td></tr></table></figure></p>
<p>这是我们唯一的特征：x。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545208575711-9a5c4e19-f272-468f-b36e-0d7074d62586.png#align=left&amp;display=inline&amp;height=111&amp;originHeight=111&amp;originWidth=625&amp;status=done&amp;width=625" alt><br>现在，让我们为我们的点分配一些颜色：红色和绿色。 这些是我们的标签。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545208606509-b2a7b1cd-b1cf-4ca4-baca-88196d101a0b.png#align=left&amp;display=inline&amp;height=110&amp;originHeight=110&amp;originWidth=643&amp;status=done&amp;width=643" alt><br>因此，我们的分类问题非常简单：鉴于我们的特征x，我们需要预测其标签：红色或绿色。</p>
<p>由于这是一个二元分类，我们也可以将这个问题描述为：“是点绿色”，或者更好的是，“点是绿色的概率是多少”？ 理想情况下，绿点的概率为1.0（绿色），而红点的概率为0.0（绿色）。</p>
<p>在此设置中，绿点属于正类（YES，它们是绿色），而红点属于负类（NO，它们不是绿色）。<br>如果我们拟合模型来执行此分类，它将预测每个点的绿色概率。 根据我们对点的颜色的了解，我们如何评估预测概率的优劣（或差）？ 这是损失功能的全部目的！ 它应该为错误预测返回高值，为良好预测返回低值。</p>
<p>对于像我们的例子那样的二进制分类，典型的损失函数是二进制交叉熵/对数损失函数。<br><a name="6dab43bd"></a></p>
<h2 id="损失函数：二进制交叉熵-对数损失函数"><a href="#损失函数：二进制交叉熵-对数损失函数" class="headerlink" title="损失函数：二进制交叉熵/对数损失函数"></a><a href="#gh52id"></a>损失函数：二进制交叉熵/对数损失函数</h2><p>如果你观察这个损失函数，这就是你会发现的：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545208847996-2aba3677-2186-4f8c-bdab-ae8590ad7c6b.png#align=left&amp;display=inline&amp;height=79&amp;originHeight=79&amp;originWidth=548&amp;status=done&amp;width=548" alt><br>Binary Cross-Entropy / Log Loss<br>其中y是标签（绿点为1，红点为0），p(y)是所有N点的点为绿色的预测概率。</p>
<p>阅读这个公式，它告诉你，对于每个绿点（y = 1），它将log（p（y））添加到损失中，即它是绿色的对数概率。 相反，它为每个红点（y = 0）添加log（1-p（y）），即，它为红色的对数概率。 不一定很难，当然也不是那么直观……</p>
<p>此外，熵与这一切有什么关系？ 为什么我们首先记录概率？ 这些是有效的问题，我希望在下面的“给我看数学”部分回答它们。</p>
<p><a name="9acf4584"></a></p>
<h3 id="计算损失-可视化方法"><a href="#计算损失-可视化方法" class="headerlink" title="计算损失-可视化方法"></a><a href="#sbsnmi"></a>计算损失-可视化方法</h3><p>首先，让我们根据他们的类别（正面或负面）分割点数，如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214792607-9e822a37-5f9e-46cb-bc0b-0a68f744faff.png#align=left&amp;display=inline&amp;height=224&amp;originHeight=224&amp;originWidth=620&amp;status=done&amp;width=620" alt><br>现在，让我们训练一个Logistic回归来对我们的点进行分类。 拟合回归是一个S形曲线，表示任何给定x点的绿色概率。 它看起来像这样：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214823897-b1ef51c7-4c5f-4ca4-8543-81231a9f1918.png#align=left&amp;display=inline&amp;height=215&amp;originHeight=215&amp;originWidth=686&amp;status=done&amp;width=686" alt><br>那么，对于属于正类（绿色）的所有点，我们的分类器给出的预测概率是多少？ 这些是S形曲线下的绿色条形，在与这些点对应的x坐标处。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214857312-a0a2e0e3-5317-4fe4-b9a8-64d6f07dbdf1.png#align=left&amp;display=inline&amp;height=209&amp;originHeight=209&amp;originWidth=683&amp;status=done&amp;width=683" alt><br>好的，到目前为止，真好！ 负面阶级的观点怎么样？ 请记住，S形曲线下的绿色条表示给定点为绿色的概率。 那么，给定点是红色的概率是多少？ 红色条在S形曲线上方，当然:-)<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214885683-601d1b29-50b1-4f89-98d8-242b19e28808.png#align=left&amp;display=inline&amp;height=209&amp;originHeight=209&amp;originWidth=678&amp;status=done&amp;width=678" alt><br>总而言之，我们最终会得到这样的结论：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214919348-98029aeb-bbdd-4969-b2bd-c5d5a5b9b83d.png#align=left&amp;display=inline&amp;height=210&amp;originHeight=210&amp;originWidth=683&amp;status=done&amp;width=683" alt><br>条形表示与每个点的相应真实类别相关联的预测概率！</p>
<p>好的，我们有预测的概率……通过计算二进制交叉熵/对数损失来评估它们的时间！</p>
<p>这些概率就是我们所需要的，所以，让我们摆脱x轴并将条带彼此相邻：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214956710-95a7d392-f358-48ef-bf1e-6fc94c658e98.png#align=left&amp;display=inline&amp;height=262&amp;originHeight=262&amp;originWidth=425&amp;status=done&amp;width=425" alt><br>好吧，吊杆不再有意义了，所以让我们重新定位它们：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215026705-0bd2aed9-8956-416f-9449-899f50e0ef36.png#align=left&amp;display=inline&amp;height=254&amp;originHeight=254&amp;originWidth=434&amp;status=done&amp;width=434" alt><br>既然我们正试图计算损失，我们需要惩罚不好的预测，对吧？ 如果与真实类相关的概率为1.0，我们需要将其损失为零。 相反，如果这个概率很低，比如0.01，我们需要它的损失是巨大的！<br>事实证明，对于这个目的，取概率的（负）对数就足够了（因为0.0和1.0之间的值的对数是负的，我们采用负对数来获得损失的正值）。<br>实际上，我们使用日志的原因来自交叉熵的定义，请查看下面的“给我看数学”部分了解更多详情。<br>下图给出了一个清晰的图片 - 如果真实类的预测概率接近零，则损失呈指数增长：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215109720-0cebdd78-1004-4e66-8dba-ed416c518790.png#align=left&amp;display=inline&amp;height=307&amp;originHeight=307&amp;originWidth=418&amp;status=done&amp;width=418" alt><br>很公平！ 让我们采用概率的（负）对数 - 这些是每个点的相应损失。<br>最后，我们计算所有这些损失的平均值。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215139483-4a54d212-a279-400b-afe6-df918499a792.png#align=left&amp;display=inline&amp;height=257&amp;originHeight=257&amp;originWidth=441&amp;status=done&amp;width=441" alt><br>瞧！ 我们已经成功计算了这个玩具示例的二进制交叉熵/对数损失。 它是0.3329！<br>告诉我代码<br>如果你想仔细检查我们找到的值，只需运行下面的代码并亲自看看:-)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.metrics import log_loss</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])</span><br><span class="line">y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])</span><br><span class="line"></span><br><span class="line">logr = LogisticRegression(solver=&apos;lbfgs&apos;)</span><br><span class="line">logr.fit(x.reshape(-1, 1), y)</span><br><span class="line"></span><br><span class="line">y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()</span><br><span class="line">loss = log_loss(y, y_pred)</span><br><span class="line"></span><br><span class="line">print(&apos;x = &#123;&#125;&apos;.format(x))</span><br><span class="line">print(&apos;y = &#123;&#125;&apos;.format(y))</span><br><span class="line">print(&apos;p(y) = &#123;&#125;&apos;.format(np.round(y_pred, 2)))</span><br><span class="line">print(&apos;Log Loss / Cross Entropy = &#123;:.4f&#125;&apos;.format(loss))</span><br></pre></td></tr></table></figure></p>
<p>告诉我数学（真的吗？！）<br>除了笑话之外，这篇文章并不打算在数学上倾向于……但是对于你们这些人，我的读者，想要了解熵的作用，所有这些中的对数，我们在这里:-)<br>如果你想深入了解信息理论，包括所有这些概念 - 熵，交叉熵等等 - 检查Chris Olah的帖子，它非常详细！<br><a name="b2c9e438"></a></p>
<h4 id="分布"><a href="#分布" class="headerlink" title="分布"></a><a href="#gs7ekn"></a>分布</h4><p>让我们从分配点开始吧。 由于y表示我们的点的类（我们有3个红点和7个绿点），这就是它的分布，我们称之为q（y），如下所示：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215306977-97044aee-62cc-46c7-b7b2-cc4e60a53c39.png#align=left&amp;display=inline&amp;height=284&amp;originHeight=284&amp;originWidth=432&amp;status=done&amp;width=432" alt></p>
<p>熵<br>熵是与给定分布q（y）相关的不确定性的度量。<br>如果我们所有的积分都是绿色怎么办 那个分布的不确定性是什么？ ZERO，对吗？ 毕竟，对于一个点的颜色毫无疑问：它总是绿色的！ 所以，熵是零！<br>另一方面，如果我们确切地知道一半的点是绿色而另一半点是红色的呢？ 那是最糟糕的情况，对吗？ 猜测点的颜色绝对没有优势：它完全是随机的！ 对于这种情况，熵由下面的公式给出（我们有两个类别（颜色） - 红色或绿色 - 因此，2）：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215372228-11ebb814-6d1e-4797-b42e-94b77db29f6d.png#align=left&amp;display=inline&amp;height=43&amp;originHeight=43&amp;originWidth=166&amp;status=done&amp;width=166" alt><br>对于中间的其他情况，我们可以使用下面的公式计算分布的熵，如q（y），其中C是类的数量：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215399649-c50c5363-f426-4527-bf68-134aee0b35a4.png#align=left&amp;display=inline&amp;height=80&amp;originHeight=80&amp;originWidth=306&amp;status=done&amp;width=306" alt><br>因此，如果我们知道随机变量的真实分布，我们就可以计算其熵。 但是，如果是这样的话，为什么首先要费心去训练分类器呢？ 毕竟，我们知道真正的分布……<br>但是，如果我们不这样做呢？ 我们可以尝试用其他一些分布近似真实分布，比如p（y）吗？ 我们当然可以！:-)<br><a name="bf08985d"></a></p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a><a href="#ekaaou"></a>交叉熵</h4><p>让我们假设我们的观点遵循其他分布p（y）。 但我们知道它们实际上来自真实（未知）分布q（y），对吧？<br>如果我们像这样计算熵，我们实际上是在计算两个分布之间的交叉熵：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215448740-5ec55a28-5c13-46a0-b6d1-197063d62b05.png#align=left&amp;display=inline&amp;height=85&amp;originHeight=85&amp;originWidth=320&amp;status=done&amp;width=320" alt><br>如果我们奇迹般地将p（y）与q（y）完美匹配，则交叉熵和熵的计算值也将匹配。<br>由于这可能永远不会发生，因此交叉熵将具有比在真实分布上计算的熵更大的BIGGER值。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215480123-9afd0d8f-9398-450a-b194-4ab6348a4685.png#align=left&amp;display=inline&amp;height=47&amp;originHeight=47&amp;originWidth=208&amp;status=done&amp;width=208" alt><br>事实证明，交叉熵和熵之间的这个区别有一个名字……<br><a name="bbec16ca"></a></p>
<h4 id="Kullback-Leibler发散"><a href="#Kullback-Leibler发散" class="headerlink" title="Kullback-Leibler发散"></a><a href="#fk7aht"></a>Kullback-Leibler发散</h4><p>Kullback-Leibler Divergence，简称“KL Divergence”，衡量两种发行版之间的差异：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215542635-c4a2d275-64a9-45c9-b68e-3f99fce5fcf5.png#align=left&amp;display=inline&amp;height=89&amp;originHeight=89&amp;originWidth=619&amp;status=done&amp;width=619" alt><br>这意味着，p（y）越接近q（y），发散越低，因此交叉熵越低。<br>所以，我们需要找到一个好的p（y）来使用……但是，这是我们的分类器应该做的，不是吗？！ 确实如此！ 它寻找最好的p（y），这是最小化交叉熵的那个。<br><a name="7162a4e0"></a></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><a href="#xokywp"></a>损失函数</h4><p>在训练期间，分类器使用其训练集中的N个点中的每一个来计算交叉熵损失，有效地拟合分布p（y）！ 由于每个点的概率是1 / N，因此交叉熵由下式给出：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215580283-c3d40241-87ec-4541-aaf9-971fc6cc90e7.png#align=left&amp;display=inline&amp;height=85&amp;originHeight=85&amp;originWidth=411&amp;status=done&amp;width=411" alt><br>还记得上面的图6到10吗？ 我们需要在与每个点的真实类相关联的概率之上计算交叉熵。 这意味着使用绿色条形作为正类（y = 1）中的点，使用红色条形作为负类中的点（y = 0），或者数学上说：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215611182-db1eab68-632e-45e8-ac03-396234f18194.png#align=left&amp;display=inline&amp;height=75&amp;originHeight=75&amp;originWidth=230&amp;status=done&amp;width=230" alt><br>最后一步是计算两个类中所有点的平均值，正面和负面：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215637206-7fda25fc-c763-46c0-9eca-6e4fe9cec776.png#align=left&amp;display=inline&amp;height=92&amp;originHeight=92&amp;originWidth=592&amp;status=done&amp;width=592" alt><br>最后，通过一些操作，我们可以采用相同的公式，从正面或负面的类中获取任何一点：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215662641-4915d2cb-3c29-4fc8-a535-c0db8b055951.png#align=left&amp;display=inline&amp;height=79&amp;originHeight=79&amp;originWidth=548&amp;status=done&amp;width=548" alt><br>瞧！ 我们回到二进制交叉熵/日志丢失的原始公式:-)<br><a name="fef1217c"></a></p>
<h3 id="最后的想法"><a href="#最后的想法" class="headerlink" title="最后的想法"></a><a href="#3038bg"></a>最后的想法</h3><p>我真的希望这篇文章能够对一个经常被认为理所当然的概念，即二元交叉熵作为损失函数的概念有所启发。 此外，我也希望它能够向您展示机器学习和信息理论如何联系在一起。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/" itemprop="url">关于目标检测，所有你应该知道的深度学习模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T15:22:45+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://medium.com/@syshen/%E7%89%A9%E9%AB%94%E5%81%B5%E6%B8%AC-object-detection-740096ec4540" target="_blank" rel="noopener">链接</a></p>
<p><a name="e60a"></a></p>
<h3 id="Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO"><a href="#Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO" class="headerlink" title="Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO"></a><a href="#e60a"></a>Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO</h3><p>这篇是简介一些用来辨识影像中物体的AI 模型。</p>
<p>在前面有提到，透过CNN模型，你可以输入一张图片，<a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5" target="_blank" rel="noopener">得到该图片属于哪种类别的结果</a>，这过程我们把他称作分类(Classification)。</p>
<p>但在真实世界的应用情境通常要从一张图片中辨识所有出现的物体， 并且标示出位置来(标出位置称之为Object Localization)。你一定在网路上看过类似底下的影片，这段影片可以看出中国闭路摄影机(CCTV)发展的概况，不只是可以框出影像中每个物件，辨别物件种类，侦测出移动物体的动量，甚至是人脸辨识，实现楚门世界的恶梦。要做到这就需要靠深度学习中的Object Detection 演算法，这也是最近几年来深度学习最蓬勃发展的一块领域。<br><a href="https://youtu.be/aE1kA0Jy0Xg" target="_blank" rel="noopener">https://youtu.be/aE1kA0Jy0Xg</a><br>基本的想法是，既然CNN 对于物体的分类又快又好，那我们可不可以拿CNN 来扫描并辨识图片中的任何物体？答案当然是 — 可以。</p>
<p>最简单的作法就是用Sliding Windows 的概念，也就是用一个固定大小的框框，逐一的扫过整张图片，每次框出来的图像丢到CNN 中去判断类别。由于物体的大小是不可预知的，所以还要用不同大小的框框去侦测。但是Sliding Window 是非常暴力的作法，对单一影像我们需要扫描非常多次，每扫一次都需要算一次CNN，这将会耗费大量的运算资源，而且速度慢，根本无法拿来应用！<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204251780-a74eec91-8d1c-42c3-b27a-da11667ac2c3.png#width=826" alt></p>
<p>所以后来就有人提出了R-CNN (Regions with CNN)<br><a name="68d0"></a></p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="#68d0"></a>R-CNN</h3><p>与其用Sliding Window的方式扫过一轮，<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a>的作法是预先筛选出约2000个可能的区域，再将这2000区域个别去作分类，所以他的演算法流程如下：</p>
<ol>
<li><p>产生一群约2000 个可能的区域(Region Proposals)</p>
</li>
<li><p>经由一个预先训练好的CNN 模型如AlexNet 撷取特征，将结果储存起来。</p>
</li>
<li><p>然后再以SVM (Support Vector Machine) 分类器来区分是否为物体或者背景。</p>
</li>
<li><p>最后经由一个线性回归模型来校正bounding box 位置。</p>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204292906-82d75e3e-ada2-4402-8a3e-e1f746718a78.png#width=826" alt><br><a name="9185"></a></p>
<h4 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a><a href="#9185"></a>Selective Search</h4><p>R-CNN用来筛选Region Proposals的方法称之为<a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">Selective Search</a>，而Selective Search又是基于Felzenszwal于2004年发表的论文<a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf" target="_blank" rel="noopener">Graph Base Image Segmentation</a>。</p>
<p>图像经由Graph Base Image Segmentation 可以切出数个Segment 来，如下图：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204317496-a09e245d-e3f5-4fa9-9e62-3fbcd048e2c9.png#width=554" alt><br>而Selective Search 的作法是将Segment 的结果先各自画出bounding box，然后以一个回圈，每次合并相似度最高的两个box，直到整张图合并成单一个box 为止，在这过程中的所有box 便是selective search 出来的region proposals。Selective Search 的演算法如下：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204363747-83bbc0c6-5f23-4b0c-bae2-3d3d18561a34.png#width=826" alt><br>取自Selective Search 论文。先以Graph base image segmentation 取得一些区域，计算每个区域间的相似度，每次合并相似度最高的两个区域，直到整张图片成为单一区域为止。<br>但是R-CNN 存在一些问题，速度仍然不够快：</p>
<ol>
<li><p>R-CNN 一开始必须先产生约2000 个区域，每个区域都要丢进CNN 中去撷取特征，所以需要跑过至少2000 次的CNN</p>
</li>
<li><p>R-CNN 的model 是分开成三部份，分别是用来取出特征的CNN，分类的SVM，以及优化bounding box 的线性回归。所以R-CNN 不容易作训练。</p>
</li>
</ol>
<p>所以R-CNN的其中一个作者<a href="http://www.rossgirshick.info/" target="_blank" rel="noopener">Ross Girshick (RBG大神)</a>在2015年又提出了一个改良版本，并称之为Fast R-CNN。<br><a name="1250"></a></p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="#1250"></a>Fast R-CNN</h3><p><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a>的想法很简单，在R-CNN中，2000多个区域都要个别去运算CNN，这些区域很多都是重叠的，也就是说这些重叠区域的CNN很多都是重复算的。所以Fast R-CNN的原则就是全部只算一次CNN就好，CNN撷取出来的特征可以让这2000多个区域共用！</p>
<p>Fast R-CNN 采用的作法就是RoIPooling (Region of Interest Pooling)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204447478-48e75d73-6502-4714-a7e2-b1064c212381.png#width=826" alt><br>Fast RCNN 一样要预选Region proposals，但是只做一次CNN。在跑完Convolution layers 的最后一层时，会得到一个HxW 的feature map，同时也要将region proposals 对应到HxW 上，然后在feature map 上取各自region 的MaxPooling，每个region 会得到一个相同大小的矩阵(例如2x2)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204932699-604244a0-ec32-4161-b825-10e50a4d088b.png#width=800" alt><br>from <a href="https://blog.deepsense.ai/region-of-interest-pooling-explained/" target="_blank" rel="noopener">https://blog.deepsense.ai/region-of-interest-pooling-explained/</a><br>然后各自连接上FC 网路，以及softmax 去作分类。在分类的同时也作bounding box 的线性回归运算。</p>
<p>Fast RCNN 的优点是：</p>
<ol>
<li><p>只需要作一次CNN，有效解省运算时间</p>
</li>
<li><p>使用单一网络，简化训练过程</p>
</li>
</ol>
<p><a name="fcaf"></a></p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="#fcaf"></a>Faster R-CNN</h3><p>不管是R-CNN还是Fast R-CNN都还是要先透过selective search预选region proposals，这是一个缓慢的步骤。在2015年时，Microsoft的<a href="http://shaoqingren.com/" target="_blank" rel="noopener">Shaoqing Ren</a> , <a href="http://kaiminghe.com/" target="_blank" rel="noopener">Kaiming He</a> , <a href="http://www.rossgirshick.info/" target="_blank" rel="noopener">Ross Girshick</a> ,以及<a href="http://www.jiansun.org/" target="_blank" rel="noopener">Jian Sun</a>提出了<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>，一个更快的R-CNN。<br>Faster R-CNN 的想法也很直觉，与其预先筛选region proposals，到不如从CNN 的feature map 上选出region proposals。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204990054-48efb89e-132f-444c-938d-7e4441882e73.png#width=650" alt><br><a name="fef5"></a></p>
<h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a><a href="#fef5"></a>Region Proposal Network</h4><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205011580-f995c601-05b6-4d1c-9958-ca8d13714c32.png#width=826" alt><br>RPN (Region Proposal Network) 也是一个Convolution Network，Input 是之前CNN 输出的feature map，输出是一个bounding box 以及该bounding box 包含一个物体的机率。</p>
<p>RPN 在feature map 上取sliding window，每个sliding window 的中心点称之为anchor point，然后将事先准备好的k 个不同尺寸比例的box 以同一个anchor point 去计算可能包含物体的机率(score) ，取机率最高的box。这k 个box 称之为anchor box。所以每个anchor point 会得到2k 个score，以及4k 个座标位置(box 的左上座标，以及长宽，所以是4 个数值)。在Faster R-CNN 论文里，预设是取3 种不同大小搭配3 种不同长宽比的anchor box，所以k 为3x3 = 9 。</p>
<p>经由RPN 之后，我们便可以得到一些最有可能的bounding box，虽然这些bounding box 不见得精确，但是透过类似于Fast RCNN 的RoIPooling， 一样可以很快的对每个region 分类，并找到最精确的bounding box 座标。<br><a name="83ca"></a></p>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a><a href="#83ca"></a>Mask R-CNN</h3><p>前述几个方法都是在找到物体外围的bounding box，bounding box基本上都是方形，另外一篇有趣的论文是Facebook AI researcher <a href="http://kaiminghe.com/" target="_blank" rel="noopener">Kaiming He</a>所提出的<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>，透过Mask R-CNN不只是找到bounding box，可以做到接近pixel level的遮罩(图像分割Image segmentation)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205050916-f7cefd4c-be49-44ba-8786-bb25170f7e4d.png#width=826" alt><br>要了解Mask R-CNN 如何取遮罩，要先看一下FCN (Fully Convolutional Network)</p>
<p><a name="28c4"></a></p>
<h4 id="FCN-Fully-Convolutional-Network-for-Image-Segmentation"><a href="#FCN-Fully-Convolutional-Network-for-Image-Segmentation" class="headerlink" title="FCN (Fully Convolutional Network) for Image Segmentation"></a><a href="#28c4"></a>FCN (Fully Convolutional Network) for Image Segmentation</h4><p>有别于CNN 网络最后是连上一个全连接(Fully Connected)的网络，FCN (Fully Convolutional Network)最后接上的是一个卷积层。一般的CNN 只能接受固定大小的Input，但是FCN 则能接受任何大小的Input，例如W x H 。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205073642-70d8970e-7b91-4772-93fe-4c805439a274.png#width=826" alt><br><em>图上方一般的CNN 网络，只能接受大小固定的输入，得到单一维度的输出，分别代表每个类别的机率。图下则是FCN 网路，最后两层由卷积取代，输出为hxwx 1000，代表每个pixel 种类的机率，可以视为一个heapmap。</em></p>
<p>在CNN 的过程中会一直作downsampling，所以FCN 最后的输出可能为H/32 x W/32，实际上得到的会是一个像heapmap 的结果。但是由于这过程是downsampling，所以Segment 的结果是比较粗糙，为了让Segment 的效果更好，要再做upsampling，来补足像素。upsamping 的作法是取前面几层的结果来作差补运算。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205110140-81193d10-5537-493b-9d73-b9b4353ac2d1.png#width=826" alt><br>FCN 的结果会跟前面几层的输出作差补运算<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205127937-663202b8-0a40-43e5-a833-7854a202f6f9.png#width=826" alt><br>Mask R-CNN是建构于Faster R-CNN之上，如果是透过RoIPooling取得Region proposals之后，针对每个region会再跑FCN取得遮罩分割，但是由于RoIPooling在做Max pooling时，会使用最近插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Nearest Neighbor Interpolation</a> )取得数值，所以出来的遮罩会有偏移现象，再加上pooling下来的结果，会让region的尺寸出现非整数的情况，然后取整数的结果就是没办法做到Pixel层级的遮罩。所以Mask R-CNN改采用双线性插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Bilinear Interpolation</a> )来改善RoIPooling，称之为RoIAlign，RoIAlign会让遮罩位置更准确。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205148755-2522b72b-9b05-4e9b-af6f-ef024168767e.png#width=826" alt><br>Mask RCNN 架构，将原有的RoIPooling 改成 RoIAlign。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205172687-cd927423-a678-4557-8914-e933152be762.png#width=826" alt><br>Fast R-CNN 的RoIPool。将一个7x5 的Anchor box 取2x2 的MaxPool，由于使用最近插值法，会有偏差。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205202715-768d38b1-ac5b-48f7-91fd-35ef7abb8a64.png#width=826" alt></p>
<p>RoIAlign的作法是使用双线性插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Bilinear Interpolation</a> )，减少mis-alignment的问题。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205230066-e554096e-51e2-492f-8e1d-fbd2fd312f9d.png#width=826" alt><br><a name="c62f"></a></p>
<h3 id="YOLO-You-Only-Look-Once"><a href="#YOLO-You-Only-Look-Once" class="headerlink" title="YOLO: You Only Look Once"></a><a href="#c62f"></a>YOLO: You Only Look Once</h3><p><a href="https://pjreddie.com/media/files/papers/yolo.pdf" target="_blank" rel="noopener">YOLO</a>有个很讨喜的名字，取自<a href="https://zh.wikipedia.org/wiki/YOLO" target="_blank" rel="noopener">You Only Live Once</a>，但用在Object detection上则为You only look once，意思是说YOLO模型的特性只需要对图片作一次CNN便能够判断里面的物体类别跟位置，大大提升辨识速度。</p>
<p>R-CNN 的概念是先提出几个可能包含物体的Region proposal，再针对每个region 使用CNN 作分类，最后再以regression 修正bounding box 位置，速度慢且不好训练。YOLO 的好处是单一网路设计，判断的结果会包含bounding box 位置，以及每个bounding box 所属类别及概率。整个网路设计是end-to-end 的，容易训练，而且速度快。</p>
<ol>
<li><p>YOLO 速度快，在Titan X GPU 上可以达到每秒45 祯的速度，简化版的YOLO 甚至可以达到150 fps 的速度。这意味着YOLO 已经可以对影像作即时运算了。准确度(mAP) 也狠甩其他深度学习模型好几条街。看看底下YOLO2 的demo 视频，这侦测速度会吓到吃手手了<a href="https://youtu.be/VOC3huqHrss" target="_blank" rel="noopener">https://youtu.be/VOC3huqHrss</a></p>
</li>
<li><p>有别于R-CNN 都是先提region 再做判断，看的范围比较小，容易将背景的background patch 看成物体。YOLO 在训练跟侦测时都是一次看整张图片，背景错误侦测率(background error, 抑或false positive) 都只有Fast R-CNN 的一半。</p>
</li>
<li><p>YOLO 的泛用性也比R-CNN 或者DPM 方式来得好很多，在新的domain 使用YOLO 依旧可以很稳定。<br>YOLO 的概念是将一张图片切割成S x S 个方格，每个方格以自己为中心点各自去判断B 个bounding boxes 中包含物体的confidence score 跟种类。<br>confidence score = Pr(Object) * IOU (ground truth)<br>如果该bounding box 不包含任何物体(Pr(Object) = 0)，confidence score 便为零，而IOU 则为bounding box 与ground truth 的交集面积，交集面积越大，分数越高。<br>每个方格预测的结果包含5 个数值，x 、y 、w 、 h 跟confidence，x 与y 是bounding box 的中间点，w 与h 是bounding box 的宽跟高。</p>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205319660-33410751-7339-4534-a37e-f2d61752ae1b.png#width=826" alt><br>S = 7，B = 2，PASCAL VOC label 20 种种类，所以tensor 为S x S x (5 * B + C) = 7 x 7 x 30<br>YOLO 的网路设计包含了24 个卷积层，跟2 层的FC 网络。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205351976-8c342184-e75a-4ac0-add0-a2e06853c86d.png#width=826" alt><br>另外一个版本的YOLO Fast 则只有9 个卷积层，不过最后的输出都是7x7x30 的tensor。</p>
<p><a name="e2e8"></a></p>
<h4 id="YOLO-的缺点"><a href="#YOLO-的缺点" class="headerlink" title="YOLO 的缺点"></a><a href="#e2e8"></a>YOLO 的缺点</h4><ol>
<li><p>由于YOLO 对于每个方格提两个bounding box 去作侦测，所以不容易去区分两个相邻且中心点又非常接近的物体</p>
</li>
<li><p>只有两种bounding box，所以遇到长宽比不常见的物体的检测率较差</p>
</li>
</ol>
<p><a name="5aaa"></a></p>
<h4 id="YOLO-与其他模型的比较"><a href="#YOLO-与其他模型的比较" class="headerlink" title="YOLO 与其他模型的比较"></a><a href="#5aaa"></a>YOLO 与其他模型的比较</h4><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205373222-774ef6c0-1da2-49fa-a9c6-1aa802765f10.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205389614-e1f9cac9-d060-463b-b15f-cae65400191c.png#width=826" alt><br><a name="da63"></a></p>
<h3 id="YOLO2"><a href="#YOLO2" class="headerlink" title="YOLO2"></a><a href="#da63"></a>YOLO2</h3><p><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO2</a>建构于YOLO之上，但是有更好的准确度，更快速的判断速度，能够判断更多的物件种类(多达9000种)，所以是<strong>更好(Better)、更快(Faster)、更强大(Stronger)</strong>！<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205410945-959e3ef1-9aac-408a-bde4-e16a2f47fb24.png#width=826" alt><br>YOLO2 在准确度上比YOLO 好，且追上什至超越其他模型像是Faster R-CNN 或者SSD 等，速度还是别人的2–10 倍以上。</p>
<p>YOLO2 采用了许多改善方式，例如batch normalization、anchor box 等，使用了这些改良方式让YOLO2 不管在辨识速度还是准确率上都有了提升，此外对于不同图档大小也有很好的相容性，提供了在速度与准确性上很好的平衡，所以也很适合运用在一些便宜的GPU 或者CPU 上，依旧提供水准以上的速度与准确率。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205442472-9616bcc1-7bd3-4ecf-a655-b4ec3dde4948.png#width=826" alt><br><a name="ccdb"></a></p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a><a href="#ccdb"></a>结语</h3><p>物体辨识(Object detection)的进展飞快，为了整理这篇大概也看了七八篇论文，还有很多都还没涵盖到的，例如SSD ( <a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">Single Shot Mulitbox Detector</a> )。如果想更了解AI在Computer Vision最近几年的发展，也可以参考这篇<a href="http://www.themtank.org/a-year-in-computer-vision" target="_blank" rel="noopener">搜文</a> <a href="http://www.themtank.org/a-year-in-computer-vision" target="_blank" rel="noopener">A Year in Computer vision</a>，内容涵盖了Classification、Object detection、Object tracking、Segmentation、Style transfer、Action recognition、3D object、Human post recognition等等，看完会大致知道在Computer Vision中有哪些AI所做的努力，以及各自的进展。</p>
<p>Google的Tensorflow也有提供<a href="https://github.com/tensorflow/models/tree/master/research/object_detection" target="_blank" rel="noopener">Object detection API</a>，透过使用API ，不用理解这些模型的实作也能快速实作出速度不错涵盖率又广的object detection。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205466794-6c46e6e8-6b5c-4375-ab82-46fb5c460706.png#width=826" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/目标检测和定位算法的演变/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/19/yuque/目标检测和定位算法的演变/" itemprop="url">目标检测和定位算法的演变</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T14:21:01+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://towardsdatascience.com/evolution-of-object-detection-and-localization-algorithms-e241021d8bad" target="_blank" rel="noopener">链接</a><br>通过对基本概念的直观解释，了解对象检测和本地化的最新进展。<br>目标检测是计算机视觉领域中非常迅速成熟的领域之一。 感谢深度学习！ 每年，新的算法/模型都会比以前更好。 事实上，Facebook AI团队上周刚刚发布了最先进的物体检测软件系统之一。 该软件称为Detectron，它包含许多用于物体检测的研究项目，并由Caffe2深度学习框架提供支持。<br>今天，有大量用于目标检测的预训练模型（YOLO，RCNN，Faster RCNN，Mask RCNN，Multibox等）。 因此，只需花费少量精力即可检测视频或图像中的大多数对象。 但我的博客的目标不是谈论这些模型的实现。 相反，我试图以清晰简洁的方式解释基本概念。<br>我最近完成了3周的Andrew Ng的卷积神经网络课程，其中他谈到了目标检测算法。 此博客的大部分内容都受到该课程的启发。<br><br>编辑：我目前正在进行Fast.ai的尖端深度学习编码课程，由Jeremy Howard教授。 现在，我使用PyTorch和fast.ai库实现了下面讨论的算法。 这是代码的<a href="https://github.com/groverpr/deep-learning/tree/master/computer_vision" target="_blank" rel="noopener">链接</a>。 如果您想了解下面讨论的算法的实现部分，请查看此信息。 该实现已经从fast.ai<a href="https://github.com/fastai/fastai/tree/master/courses" target="_blank" rel="noopener">课程笔记</a>本借用，附有评论和注释。<br><a name="zs11cg"></a></p>
<h2 id="关于CNN的简介"><a href="#关于CNN的简介" class="headerlink" title="关于CNN的简介"></a><a href="#zs11cg"></a>关于CNN的简介</h2><p>在我解释目标检测算法的工作之前，我想在卷积神经网络（也称为CNN或ConvNets）上写上几笔。 CNN是深度学习时代大多数计算机视觉任务的基本构建模块。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545200900345-553c0b70-f95a-4497-aed7-dc9b3febdcde.png#width=826" alt><br><strong>我们想要什么？</strong> 我们需要一些查看图像的算法，查看图像中的图案并告知图像中的对象类型。 例如，是猫或狗的形象。<br><strong>什么是电脑的图像？</strong> 只是数字矩阵。 对于例如 见上图1。 左边的图像只是手写数字2的28 * 28像素图像（取自MNIST数据），在Excel电子表格中表示为数字矩阵。<br><strong>我们怎样才能教电脑学会识别图像中的物体？</strong> 通过让计算机学习垂直边缘，水平边缘，圆形以及许多其他人类未知的模式。<br><strong>计算机如何学习模式？</strong>卷积！<br><br>（阅读本文时请看上图）卷积是两个矩阵之间的数学运算，给出第三个矩阵。 我们称之为滤波器或内核（图1中的3x3）的较小矩阵在图像像素矩阵上操作。 根据滤波器矩阵中的数字，输出矩阵可以识别输入图像中存在的特定模式。 在上面的示例中，滤波器是垂直边缘检测器，其学习输入图像中的垂直边缘。 在深度学习的背景下，输入图像及其后续输出从许多这样的滤波器传递。 过滤器中的数字是通过神经网络学习的，模式是自己导出的。<br><strong>为什么卷积有效？</strong> 因为在大多数图像中，对象具有可以通过卷积来利用的相对像素密度（数字的大小）的一致性。<br>我知道CNN上只有几行对于不了解CNN的读者来说是不够的。 但CNN不是本博客的主题，我已经提供了基本介绍，因此读者可能不需要再打开10个链接来先了解CNN，然后再继续。<br>阅读本博客后，如果您仍想了解更多关于CNN的信息，我强烈建议您阅读Adam Geitgey撰写的这篇<a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" target="_blank" rel="noopener">博客</a>。<br><a name="hcfpka"></a></p>
<h2 id="计算机视觉任务的分类"><a href="#计算机视觉任务的分类" class="headerlink" title="计算机视觉任务的分类"></a><a href="#hcfpka"></a>计算机视觉任务的分类</h2><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545201176058-a349b8fe-bc13-4e55-8106-c6bc54387534.png#width=826" alt><br>以图2中的猫狗图像为例，以下是计算机视觉建模算法最常见的任务：</p>
<ol>
<li><p><strong>图像分类</strong>：这是最常见的计算机视觉问题，其中算法查看图像并对其中的对象进行分类。 图像分类具有广泛的应用，从社交网络上的面部检测到医学中的癌症检测。 通常使用卷积神经网络（CNN）对这些问题进行建模。</p>
</li>
<li><p><strong>目标分类和定位</strong>：假设我们不仅想知道图像中是否有猫，而且猫的确切位置。 对象定位算法不仅标记对象的类，还在图像中的对象位置周围绘制边界框。</p>
</li>
<li><p><strong>多个目标检测和定位</strong>：如果图像中有多个物体（如上图中的3只狗和2只猫），我们想要检测它们，该怎么办？ 这将是一个对象检测和定位问题。 众所周知的应用是在自动驾驶汽车中，该算法不仅需要检测汽车，还需要检测车架中的行人，摩托车，树木和其他物体。 这些问题需要利用从图像分类和对象定位中学到的思想或概念。</p>
</li>
</ol>
<p>现在回到计算机视觉任务。 在深度学习的背景下，上述3种任务之间的基本算法差异就是选择相关的输入和输出。 让我用信息图解释这一行。<br><a name="rdrdnd"></a></p>
<h2 id="1-图像分类"><a href="#1-图像分类" class="headerlink" title="1 图像分类"></a><a href="#rdrdnd"></a>1 图像分类</h2><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545201367951-62c92f43-597b-4174-82e8-9f7640900af4.png#width=826" alt></p>
<ol>
<li><p>图2中的信息图显示了用于图像分类的典型CNN的外观。通过n滤波器（图3中n = 4）卷积一些高度，宽度和通道深度（上面的情况下为940,550,3）的输入图像[如果你仍然感到困惑究竟卷积是什么意思，请检查 这个链接来理解深度神经网络中的卷积]。</p>
</li>
<li><p>卷积的输出用非线性变换处理，通常是Max Pool和RELU。</p>
</li>
<li><p>Convolution，Max Pool和RELU的上述3个操作被执行多次。</p>
</li>
<li><p>最终层的输出被发送到Softmax层，该层转换0和1之间的数字，从而给出图像特定类的概率。 我们将损失降至最低，以便使最后一层的预测接近实际值。</p>
</li>
</ol>
<p><a name="0vv7zh"></a></p>
<h2 id="2-物体分类和定位"><a href="#2-物体分类和定位" class="headerlink" title="2.物体分类和定位"></a><a href="#0vv7zh"></a>2.物体分类和定位</h2><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545201635254-f917f28e-ef71-4efd-a175-9c89c060370f.png#width=826" alt></p>
<p>现在，为了使我们的模型绘制对象的边界框，我们只需更改前一算法的输出标签，以使我们的模型学习对象类以及对象在图像中的位置。 我们在输出层添加4个数字，包括对象的质心位置和图像中边界框的宽度和高度的比例。<br>简单吧？ 只需添加一堆输出单元即可吐出您想要识别的不同位置的x,y坐标。 对于我们拥有的所有图像中的特定对象，这些不同的位置或界标将是一致的。 对于例如对于汽车而言,高度将小于宽度，并且与图像中的其他点相比，质心将具有一些特定的像素密度。<br><br>隐含相同的逻辑，如果图像中有多个对象并且我们想要对所有这些对象进行分类和定位，您认为会发生什么变化？ 我建议你暂时停下来思考，你可能会自己得到答案。<br><a name="irp4tp"></a></p>
<h2 id="3-目标检测和定位"><a href="#3-目标检测和定位" class="headerlink" title="3.目标检测和定位"></a><a href="#irp4tp"></a>3.目标检测和定位</h2><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545201786310-58306acb-5629-4864-8fb1-fa3c62ebdca7.png#width=826" alt></p>
<p>为了检测图像中的各种对象，我们可以直接使用我们从目前为止学到的东西。 不同之处在于我们希望我们的算法能够对图像中的所有对象进行分类和定位，而不仅仅是一个。 因此，我们的想法是，只需将图像裁剪成多个图像，然后为所有裁剪的图像运行CNN以检测对象。<br>算法的工作方式如下：</p>
<ol>
<li><p>制作一个比实际图像尺寸小得多的窗口。 裁剪它并将其传递给ConvNet（CNN）并让ConvNet进行预测。</p>
</li>
<li><p>继续滑动窗口并将裁剪后的图像传递到ConvNet。</p>
</li>
<li><p>在使用此窗口大小裁剪图像的所有部分后，再次重复所有步骤以获得更大的窗口大小。 再次将裁剪后的图像传递到ConvNet并让它进行预测。</p>
</li>
<li><p>最后，您将拥有一组裁剪区域，这些区域将包含一些对象，以及对象的类和边界框</p>
</li>
</ol>
<p>该解决方案被称为具有滑动窗口的物体检测。 这是非常基本的解决方案，有以下几点需要注意：<br><strong>A.计算成本高：</strong>裁剪多个图像并通过ConvNet传递它将在计算上非常昂贵。<br><strong>解决方案</strong>：有一个简单的hack来提高滑动窗口方法的计算能力。 它是用1x1卷积层替换ConvNet中的完全连接层，对于给定的窗口大小，只传递一次输入图像。 因此，在实际实现中，我们不会一次传递一个裁剪后的图像，但我们会立即传递完整的图像。</p>
<p><strong>B.不准确的边界框</strong>：我们在整个图像上滑动方形窗口，也许对象是矩形的，或者没有一个正方形与对象的实际大小完全匹配。 虽然该算法具有查找和定位图像中多个对象的能力，但是边界框的准确性仍然很差。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545202101543-96dbf0cd-cf32-463f-bc52-ce24e9fa7c51.png#width=478" alt><br>我已经谈到了对象检测问题的最基本的解决方案。 但它有许多警告，并不是最准确的，并且实施起来计算成本很高。 那么，我们如何才能使我们的算法更好更快？<br><strong>好的解决方案？YOLO
</strong><br>事实证明，我们有YOLO（你只看一次），它比滑动窗口算法更准确，更快。 它仅基于我们已经知道的算法顶部的微小调整。 我们的想法是将图像分成多个网格。 然后我们改变数据的标签，以便我们为每个网格单元实现定位和分类算法。 让我再向您解释一下这个信息图。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545202200059-aac2dc95-09af-4e82-9b90-0ff6da1c6b8e.png#width=826" alt></p>
<p><a name="1k73ha"></a></p>
<h4 id="YOLO简单的步骤："><a href="#YOLO简单的步骤：" class="headerlink" title="YOLO简单的步骤："></a><a href="#1k73ha"></a>YOLO简单的步骤：</h4><blockquote>
<ol>
<li>将图像分成多个网格。 为了说明，我在上图中绘制了4x4网格，但YOLO的实际实现具有不同的网格数量。 （7x7用于在PASCAL VOC数据集上培训YOLO）</li>
</ol>
</blockquote>
<blockquote>
<ol>
<li>标记训练数据，如上图所示。 如果C是我们数据中唯一对象的数量，S <em> S是我们分割图像的网格数，那么我们的输出向量将是长度S </em> S <em>（C + 5）。 对于例如 在上面的例子中，我们的目标向量是4 </em> 4 <em>（3 + 5），因为我们将图像划分为4 </em> 4网格，并训练3个独特的对象：汽车，光和行人。</li>
</ol>
</blockquote>
<blockquote>
<ol>
<li>制作一个具有损失函数的深度卷积神经网络作为输出激活和标签矢量之间的误差。 基本上，该模型通过ConvNet仅在输入图像的一个前向通道中预测所有网格的输出。</li>
</ol>
</blockquote>
<blockquote>
<ol>
<li>请记住，对象存在于网格单元格（P.Object）中的标签由该网格中对象的质心的存在决定。 重要的是不允许在不同的网格中多次对一个对象进行计数。</li>
</ol>
</blockquote>
<p><strong>YOLO的注意事项及其解决方案：</strong></p>
<ul>
<li>A.无法检测同一网格中的多个对象。</li>
</ul>
<p> 通过选择较小的网格大小可以解决此问题。 但即使选择较小的网格大小，在对象彼此非常接近的情况下，算法仍然会失败，如鸟群的图像。<br>解决方案：<strong>Anchor boxes</strong>。 除了每个网格单元具有5 + C标签（其中C是不同对象的数量）之外，<strong>Anchor boxes</strong>的想法是每个网格单元具有（5 + C）* A标签，其中A是必需的<strong>Anchor boxes</strong>。 如果将一个对象分配给一个网格中的一个<strong>Anchor boxes</strong>，则可以将另一个对象分配给同一网格的另一个<strong>Anchor boxes</strong>。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545202412058-e7d724af-065d-48e1-b4cb-f0b6183a5eac.png#width=826" alt></p>
<ul>
<li>B.可多次检测一个物体的可能性。</li>
</ul>
<p>解决方案：非最大抑制。 非最大抑制消除了非常接近高概率边界框的低概率边界框。<br><a name="csrntv"></a></p>
<h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a><a href="#csrntv"></a>结论：</h2><p>截至今天，有多种版本的预训练YOLO模型可用于不同的深度学习框架，包括Tensorflow。 最新的YOLO论文是：“YOLO9000：更好，更快，更强”。 该模型接受了9000个课程的培训。 还有一些基于选择性区域提案的区域CNN（R-CNN）算法，我没有讨论过。 由Facebook AI开发的Detectron软件系统也实现了R-CNN，Masked R-CNN的变体。<br><a name="4c1f"></a></p>
<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a><a href="#4c1f"></a>参考文献:</h4><ol>
<li><p><strong>You Only Look Once: Unified, Real-Time Object Detection</strong><br><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.02640.pdf</a></p>
</li>
<li><p><strong>YOLO9000: Better, Faster, Stronger</strong><br><a href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.08242.pdf</a></p>
</li>
<li><p><strong>Convolutional Neural Networks by Andrew Ng (deeplearning.ai)</strong><br><a href="https://www.coursera.org/learn/convolutional-neural-networks" target="_blank" rel="noopener">https://www.coursera.org/learn/convolutional-neural-networks</a></p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhos</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
