<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="dns-prefetch" href="http://zhos.me">
  <title>A+</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="A+">
<meta property="og:url" content="http://zhos.me/page/3/index.html">
<meta property="og:site_name" content="A+">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A+">
  
    <link rel="alternative" href="/atom.xml" title="A+" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

</head>
</html>
<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">Zhos</a></h1>
		</hgroup>
		
		<p class="header-subtitle">武德</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:ujujzhao@gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">Zhos</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>武德<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:ujujzhao@gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 100%"><a href="/">主页</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-yuque/F-35是1.4万亿美元的国家灾难" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/21/yuque/F-35是1.4万亿美元的国家灾难/">F-35是1.4万亿美元的国家灾难</a>
    </h1>
  

        
        <a href="/2018/12/21/yuque/F-35是1.4万亿美元的国家灾难/" class="archive-article-date">
  	<time datetime="2018-12-21T05:34:55.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-21</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://medium.com/war-is-boring/the-f-35-is-a-terrible-fighter-bomber-and-attacker-and-unfit-for-aircraft-carriers-c6e36763574b" target="_blank" rel="noopener">链接</a></p>
<p>JSF是一个可怕的战斗机，轰炸机和攻击者 - 并且不适合航空母舰。<br>F-35在战斗准备就绪之前还有很长的路要走。 这是现任退休运营测试与评估总监迈克尔吉尔摩在上一份年度报告中的离职信息。<br>联合攻击战斗机计划已经消耗了超过1000亿美元和近25年。 要完成基本开发阶段，至少需要10亿美元和两年多。 吉尔莫尔告诉国会，五角大楼和公众，即使有大量的时间和金钱投入，“所有变种的运作适用性仍然低于服务部门的预期。”<br>Gilmore详细介绍了该计划存在的一系列遗留问题，有时甚至是恶化问题，包括数百个关键性能缺陷和维护问题。 他还提出了一个严肃的问题，即空军的F-35A能否在空对空或空对地任务中取得成功，海军陆战队的F-35B是否可以进行基本的近距离空中支援，以及海军是否能够 F-35C适用于航空母舰。<br><br>事实上，他发现“如果在战斗中使用，F-35飞机将需要支持来定位和避开现代威胁地面雷达，获取目标，并且由于未解决的性能缺陷和有限的武器运输可用性而使敌方战斗机编队“。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545370645879-05f3b68c-3916-4631-a30e-605fbcaa9402.png#width=60" alt><br>在一份公开声明中，F-35联合计划办公室试图驳回吉尔摩的报告，声称“所有这些问题都是日本知识产权组织，美国服务机构，我们的国际合作伙伴和我们的行业所熟知的。”<br><br>JPO对众多问题的承认是可以接受的，但没有迹象表明该办公室有任何计划 - 包括成本和进度重新估算 - 以解决目前已知问题而不偷工减料。<br><br>显然，他们还没有计划应对并为将来四年即将进行的更加严格，发展和运营测试中发现的无数未知问题提供补救。这样的计划是必不可少的，应该由实际解决问题的速度而不是不切实际的现有时间表来推动。<br><br>如何解决Gilmore发现的众多问题，以及我们如何才能最好地推进历史上最昂贵的武器计划，这个计划一直无法达到自己非常适度的承诺？<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545370694304-542d261f-be35-40fe-8a8e-ba5df980e909.png#width=826" alt><br><a name="w1qwta"></a></p>
<h4 id="电子化用于证明成本合理-而不是提供能力"><a href="#电子化用于证明成本合理-而不是提供能力" class="headerlink" title="电子化用于证明成本合理 - 而不是提供能力"></a><a href="#w1qwta"></a>电子化用于证明成本合理 - 而不是提供能力</h4><p>F-35正在向美国人民出售，其中很大一部分就是其任务系统，喷气式飞机上的大量精密电子设备。仔细阅读有关F-35的任何关于W-35的正式文章将会发现它们几乎总是指出它能够收集大量信息。<br><br>这些信息应该通过其板载传感器和数据链接到外部网络源，然后由F-35的计算机系统合并，以便为飞行员识别和显示特定威胁，目标和伴随力图片 - 即“情境”意识。”<br><br>这个过程旨在让飞行员主宰战场。然而，基于这些系统在开发测试期间的实际测试性能，电子设备实际上会干扰飞行员的生存和普及能力。<br><br>总的来说，F-35的传感器，计算机和软件的问题，包括制造虚假目标和报告不准确的位置，已经非常严重，以至于爱德华兹空军基地的测试团队将他们评为“红色”，这意味着他们无法进行战斗。他们期待的任务。<br><br>一个系统，即光电目标系统（EOTS），被飞行员挑选出来，其分辨率和范围都低于目前在传统飞机上使用的系统。 EOTS是旨在帮助F-35从足够远的地方探测和摧毁敌方战斗机以使斗狗成为过去的系统之一。它安装在靠近飞机机头的地方，包括一台电视摄像机，一个红外搜索和跟踪系统，以及一个激光测距仪和指示器。<br><br>这些传感器在计算机控制下旋转，以在广泛的视野范围内跟踪目标，并在飞行员的头盔遮阳板显示器上显示图像。<br><br>但是EOTS的局限性，包括图像随着湿度的降低，迫使飞行员飞行的距离比使用早期系统时更接近目标只是为了获得足够清晰的图像来发射导弹或射击。<br><br>该报告说，问题非常严重，以至于F-35飞行员可能需要飞得如此接近才能获得他们必须机动的目标才能获得导弹射击所需的距离。因此，该系统的局限性可以迫使攻击性的F-35妥协意外，让敌人机动到第一次机会。<br><em>投降惊喜的元素并让对手先射击是我们想要迫使敌人做的事情，而不是我们自己。</em><br><br><br>另一个经常被吹捧的功能是分布式孔径系统（DAS），该功能应该赋予F-35卓越的态势感知能力。 DAS是将显示器供给臭名昭着的600,000美元头盔系统的主要传感器之一，它也未能实现炒作。<br><br>DAS传感器是分布在F-35机身周围的六个摄像机或“眼睛”，它们向头盔遮阳板投射到飞行员想要观察的任何方向的外部视图，包括向下或向后。同时，头盔遮阳板显示飞行仪表以及从传感器和任务系统得出的目标和威胁符号。<br><br>但由于过多的错误目标，不稳定的“抖动”图像​​和信息过载等问题，飞行员正在关闭一些传感器和计算机输入，而是依靠简化的显示器或更传统的仪表板。<br><br>在这里，系统再次比它应该取代的系统好一点。<br><br>在一些重要的武器交付准确性测试中，试飞员也对头盔有困难。一些飞行员将头盔中的显示描述为“操作上无法使用且可能不安全”，因为“符号杂乱”使地面目标模糊不清。<br>在试图对目标射击短程AIM-9X空对空导弹时，飞行员报告说，他们对目标的看法被头盔护目镜上显示的符号所阻挡。飞行员还报告说，这些符号在试图追踪目标时不稳定。<br><br>然后是由于“虚假轨道”导致飞行员实际上看到双重的问题。将各种车载仪器产生的所有信息并将其合并为飞行员的连贯图像存在问题，该过程称为传感器融合。<br><br>飞行员报告说，不同的仪器，如飞机的雷达和EOTS，正在检测相同的目标，但编制信息的计算机正在将单个目标显示为两个。<br><br>飞行员试图通过关闭一些传感器来解决这个问题，使多余的目标消失。 DOT＆E表示，这是“不可接受的战斗，违反了将多个传感器的贡献融合到一个准确的轨道和清晰的显示中以获得态势感知以及识别和接触敌方目标的基本原则。”<br><br>虽然问题出在一个平面上，但是当几架飞机试图通过网络共享数据时，情况会更糟。 F-35具有多功能高级数据链路（MADL），旨在使飞机能够与其他F-35共享信息，以便为所有飞行员提供战斗空间的共同图像。它通过获取每个平面生成的所有数据并将其组合到一个共享的世界视图中来实现这一点。</p>
<p><em>但是，这个系统也会产生错误或分裂的目标图像。 使问题更加复杂的是，系统有时也会完全丢弃目标图像，导致驾驶舱内部存在混淆。</em><br><br><br>所有这些意味着系统意味着让飞行员更好地了解周围的世界可以完全相反。 根据该报告，这些系统“继续降低战斗空间意识并增加飞行员的工作量。 这些缺陷的解决方法对于飞行员来说是耗时的，并且会减少高效和有效的任务执行。“<br><br>F-35助推器表示这是重要的网络 - 实际上重要的是网络无法正常工作。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545370927968-ab261e3d-8341-4999-a9c1-d24ef08dfa74.png#width=826" alt><br><a name="xp62pu"></a></p>
<h4 id="作为战士无效"><a href="#作为战士无效" class="headerlink" title="作为战士无效"></a><a href="#xp62pu"></a>作为战士无效</h4><p>F-35从一开始就打算成为一架多用途飞机。这份最新的报告清楚地描述了迄今为止它在各种角色中如何叠加，包括与它应该取代的每架飞机相比。这个消息并不令人鼓舞。<br><br>F-35作为空对空战斗机的缺点已经有了很好的记录。<br><br>它在视觉范围（WVR）中的模拟空战中失去了名称，它的雷达隐身没有任何优势，在2015年初的F-16中，其中一架F-35应该取代作为空中战斗机。 F-35在空对空机动中反复丢失，尽管该测试被操纵，因为所使用的F-16是较重的双座版本并且进一步装载了重型拖曳外部燃料坦克阻碍其机动性。<br><br>F-35助推器认为飞机的低雷达标志将使其远离WVR情况，但空战的历史是无法完全避免WVR的攻击。导弹故障，雷达干扰的影响以及其他难以预测的因素往往会一次又一次地迫使WVR参与。<br><br>这份最新报告证实F-35并不像传统战斗机那样机动。<br><em>所有三种变型“在跨音速下都表现出令人反感或不可接受的飞行质量，其中飞机上的空气动力正在迅速变化。”</em><br><br><br>一个这样的问题被称为机翼下降，其中喷气机的翼尖在急转弯时突然下降，这可能导致飞机旋转并可能发生碰撞。<br><br>在声屏障正下方的跨音速是战斗机飞行包线的最关键点。这些是历史上大多数空战发生的速度。正是在这些速度下，F-35需要最灵活才能成为有效的战斗机。</p>
<p>该计划试图通过改变F-35的飞行软件而不是通过重新设计导致问题的实际飞行表面来解决机动性能问题。<br><br>该软件称为控制法则，将飞行员的操纵杆命令转换为飞机的行为。人们可以预期飞行员对飞机的某些力量会导致飞机的等效响应。由于软件的变化，有时并非如此。</p>
<p>例如，如果飞行员使用尖锐的杆移动以转动飞机，控制法软件现在可以更温和地转向以防止诸如 - 包括 - 挖掘等问题。 F-35辩护人试图通过声称F-35从未打算用于近距离空中斗狗来解雇这些问题，空军强烈要求飞机配备短距离空对空炮。</p>
<p>作为空对空战斗机，F-35的作​​战能力非常有限，因为目前软件版本只能使用两枚导弹，而且它们必须是雷达引导的先进中程空中飞行器。空中导弹（AMRAAMs）;如果它想要保持其隐形特征，它将来不会超过四个。</p>
<p>F-35作为空对空战斗机的能力目前进一步受到限制，因为AMRAAM并未针对近距​​离视距作战进行优化。最终，升级的软件版本将允许飞机携带除AMRAAM之外的导弹，但不会很快。这意味着F-35进入的任何战斗最好都是短暂的，因为它很快就会耗尽弹药。<br><br>它的枪也可用于近距离战斗，但它目前还没有工作，因为在战斗中有效使用它的软件还没有完成。</p>
<p>F-35A中的大炮位于飞机侧面的一扇小门后面，在大炮发射前瞬间快速打开 - 这一特性旨在使飞机保持隐身状态。测试飞行表明，这扇门能够捕捉到飞过飞机表面的空气，将F-35的机头从瞄准点拉开，导致“超出精度规格”的误差。</p>
<p>工程师们正在对F-35的控制法进行更多修改，以纠正门引起的误差。进行这些更改并执行随后的“回归”重新测试以确认更改的有效性会延迟实际的枪支准确度测试。在这些测试发生之前，没有人能够知道F-35A的大炮是否能够真正击中目标。</p>
<p>F-35B和F-35C都将使用外置式枪架，而不是像空军型号那样的内部版本。由于两种型号机身形状的差异，海军陆战队和海军将使用不同型号的枪荚。两者都在地面上进行了试验，但飞行测试看看吊舱对喷气式飞机空气动力学的影响才刚刚开始。</p>
<p>DOT＆E警告说，就像F-35A上的枪门一样，可能会发现意外的飞行控制问题。必须设计对这些的修复，然后进行测试。只有这样，程序才能开始更全面的飞行中精度测试，这对于确定枪荚是否准确是必要的。</p>
<p>开发测试延迟以及解决测试可能发现的问题的过程非常严重，以至于该计划可能没有有效的初始操作测试和评估枪支。这不仅可以进一步延迟预定的测试，而且更重要的是，可以防止飞机很快到达战士。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371080991-45289738-757c-42c0-b9aa-d61c7a02c077.png#width=826" alt></p>
<p><a name="55vqcv"></a></p>
<h4 id="作为拦截轰炸机无效"><a href="#作为拦截轰炸机无效" class="headerlink" title="作为拦截轰炸机无效"></a><a href="#55vqcv"></a>作为拦截轰炸机无效</h4><p>F-35将具有极其有限的拦截有用性的几个主要原因 - 空军和海军陆战队的“初始作战能力”声明尽管如此。<br><br>例如，欧洲，俄罗斯，中国甚至伊朗的国防公司多年来一直在努力开发和生产打败隐形飞机的系统。他们取得了一些成功。<br><br>我们在1999年清楚地看到了这一点，当时一支塞尔维亚导弹部队击落了一架F-117隐形战斗机，该战斗机配备了过时的苏联时代SA-3地对空导弹，这是1961年首次部署的系统。塞尔维亚防空人员发现他们可以通过使用导弹电池的长波搜索雷达探测隐形飞机。<br><br>然后，利用观察员和导弹自己的制导雷达，塞尔维亚部队能够追踪，瞄准并杀死一架隐身的F-117。<br><br>为了表明这不是侥幸，塞尔维亚地空导弹击中并损坏了另一架F-117，以至于它再也没有在科索沃空战中飞行过。<br><br>这些搜索雷达不受现代隐形飞机的特殊形状和涂层的影响，可以轻松检测到今天的隐形飞机，包括F-35。自第二次世界大战以来，俄罗斯人从未停止过制造这种雷达，并且现在在公开市场上以低至1000万美元的价格销售现代化，高度移动的卡车式数字长波雷达。中国和伊朗人也开始采用类似的雷达系统。<br><br>比长波探测雷达更难对付的更简单的系统是无源探测系统（PDS），用于探测和跟踪飞机发射的射频（RF）信号 - 雷达信号，UHF和VHF无线电信号，识别-friend-or-foe（IFF）信号，Link-16等数据链路信号和TACAN等导航转发器信号。<br><br>现代PDS的一个很好的例子是VERA-NG，这是一种捷克系统，在国际上销售，使用三个或更多的间隔良好的接收天线来检测和跟踪和识别战斗机和轰炸机发出的射频信号。系统的中央分析模块计算到达接收器的信号的时间差，以识别，定位和跟踪多达200架飞机发射雷达信号。<br><br>VERA-NG只是世界上使用的众多PDS中的一种 - 俄罗斯人，中国人和其他人也生产PDS，这些PDS已经广泛使用了好几年。<br><br>从对手采用PDS的角度来看，PDS的优点在于雷达隐身与其探测和跟踪飞机的能力无关。如果飞机必须使用其雷达，无线电，数据链路或导航系统来完成其任务，PDS很有可能通过这些发射来检测，跟踪和识别它。<br><br>世界上每架飞机都容易受到PDS，隐身和非隐身的影响，而F-35也不例外。<br><br>F-35的主要空对空武器AIM-120是超视距雷达导弹 - 因此，F-35必须使用大型雷达发射高功率信号才能探测到空中目标然后引导导弹到他们身边。同样，飞机必须采用高功率地面测绘雷达信号来远距离寻找地面目标。<br><br>此外，如果飞机的系统必须与地层中的其他飞机通信或与AWACS等非机载支持飞机通信，则必须使用其无线电和数据链路。因此，F-35可能易受被动跟踪系统的检测。这些无源探测系统中的一些比搜索雷达便宜得多 - 而且它们在电子方面几乎检测不到。<br><br>DOT＆E报告还列出了限制拦截有用性的几个主要原因。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371153712-4fb2550d-f06c-468e-ac83-a79ccc95ec77.png#width=60" alt><br>其中一个原因是F-35的Block 2B（USMC）和Block 3i（USAF）软件阻止它检测到许多威胁和目标，同时严重限制它可携带的武器种类。<br><br>例如，F-35目前只能携带几种型号的大型制导直接攻击炸弹。这些都不能像电力导弹那样从远处发射。相反，它们落在从飞机到目标的弹道轨道上，这意味着它们只能在目标视野中以相对较短的距离释放。<br><br>目前，F-35飞行员“将被迫飞得更近，以接近地面目标，并且根据敌方防空系统的威胁程度和可接受的任务风险，它可能仅限于接触仅由短程防御的地面目标防空，或根本没有防空。“<br><br>F-35可携带的少量武器类型也限制了其在战斗中的灵活性。目前的软件一次只能支持一种炸弹，DOT＆E表示只有在攻击一个或两个类似目标时才有用。因此，例如，当一架F-35飞机装载的炸弹装载用于摧毁地面目标的炸弹时，它们将无法摧毁任何硬化或沙坑目标，因为它们不会需要更重的炸弹。<br><br>预计F-35将携带更多种类的武器，因为更多的软件，炸弹架和测试验证这些武器已经开发出来 - 但我们直到2021年才知道哪些武器实际上是适合作战的。此外，为了携带除两个大型制导炸弹之外的东西，它将不得不使用外部武器和机架，大大降低了飞机本已令人失望的射程和机动性 - 当然，或多或少地消除了隐身。<br><br>能够穿透严密防御的空域以摧毁敌方领土深处的固定目标，这是F-35经常被引用的理由。当然，F-35的有限射程 - 低于传统的F-16战斗机 - 意味着它不可能在俄罗斯和中国等大国的家乡内完成空军所谓的“深度打击”。 <br>2016年的DOT＆E报告描述了一些官方的拖延行动，推迟了F-35的穿透力测试。例如，该计划现在才开始接收模拟敌方雷达系统的关键地面雷达模拟器设备，这些设备是在高度竞争的近邻情景中对F-35的有效性进行有力测试所需的。<br><br>它只接收该设备，因为它是由DOT＆E寻求和采购的，当时很明显服务和JSF计划办公室不会寻求足以复制F-35预计能够复制近端威胁的测试基础设施反击。这种设备的交付工作已经开始，但要到2018年初才能完成。初级专业人员没有计划或预算进行发展性飞行试验。<br><br>军方在内华达州内利斯空军基地的西部测试区进行隐形飞机的开发和操作测试。测试是针对地面雷达模拟器设备和地对空导弹发射器进行的。正在测试的飞机飞过这些阵列以查看飞机的机载传感器 - 特别是其电子战系统和地面测绘雷达 - 与通过数据链路提供的机外情报相结合，可以检测到威胁并做出适当的响应，例如通过警告飞行员，干扰信号或发射防御抑制导弹。<br><br>问题是一个复杂的问题，因为雷达信号显示SAM的存在，例如，从而允许飞机瞄准SAM或避免它，不一定是独特的，并且通常非常类似雷达的信号，不立即对飞机的威胁。<br><br>F-35无法携带足够的武器轰炸一切。它的传感器和传感器融合系统必须能够区分构成真正威胁的敌方SAM雷达与可能在探测范围内的许多无害雷达之间的区别 - 通用空中监视雷达，短程，低空防空针对武器而非飞机的雷达，甚至是附近的民用空中交通管制和气象雷达系统。<br><br>同样瘫痪，直到地面雷达模拟器设备到位，F-35程序将无法正确开发，验证和更新F-35的任务关键型机载软件文件，称为任务数据负载（MDL）。 MDL是指定所有目标和威胁位置的巨大文件，以及它们各自的电子和/或红外签名以及所有相关的映射数据。</p>
<p><em>如果没有准确，最新的MDL，F-35就无法找到目标或逃避和抵御威胁 - 它也无法实现据称是其主要优势的网络和传感器融合功能。</em><br><br><br>如果没有MDL，F-35就无法开战。 MDL还需要不断更新有关每个F-35任务收集的威胁，目标和信号等信息。 F-35飞行员只有在配备必要的地面雷达模拟器设备的测试范围内进行测试后，才能确保他们所需的MDL能够正常工作。<br><br>必须通过中央重编程实验室使用相关作战命令的海量数据输入为每个战区或冲突区创建新的和完整的MDL。例如，在英格兰境外运营的F-35将拥有与日本F-35不同的档案。今天只存在一个这样的重编程实验室，并且由于JPO管理不善，它最近才被安排接受必要的升级以产生经过验证的MDL。<br><br>实验室需要15个月才能生成完整的MDL。如果在一个新的，未曾预料到的战区中突然需要F-35战斗机，那些F-35将无法至少执行15个月的战斗任务。<br><br>由于重编程实验室尚未建立全套必要的地面雷达模拟器设备，DOT＆E表示，最早的重编程实验室将能够为IOT和E生产经过验证的MDL，将于2018年6月完成。<br><br>这是在2017年8月计划的IOT＆E开始后近一年 - 也就是海军陆战队宣布F-35B最初具备运营能力两年后。 DOT＆E进一步表示，F-35 MDL适合战斗“将不会进行测试和优化，以确保F-35能够在2020年前检测，定位和识别现代部署的威胁。”<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371288913-2328d482-bb99-4e2b-97f9-48d44b5c6a9f.png#width=826" alt><br><a name="ekgawy"></a></p>
<h4 id="作为近距离空中支援平台无效"><a href="#作为近距离空中支援平台无效" class="headerlink" title="作为近距离空中支援平台无效"></a><a href="#ekgawy"></a>作为近距离空中支援平台无效</h4><p>F-35有很多不足之处，在远离战场的情况下执行空对地拦截任务，但在其他预定的空对地作用中更为严重，直接支持部队，近距离空中支援（CAS）。</p>
<p>DOT＆E得出结论认为，F-35目前的配置“还没有证明CAS能力与第四代飞机的能力相当。”鉴于空军部长最近的声明，该服务打算重新努力，这一说法特别令人不安。在2021年取消CAS战斗证明的A-10。</p>
<p>CAS是另一项主要任务，缺乏有效的大炮将极大地限制F-35的战斗实用性。<br><br><br>对于许多CAS任务来说，有效的大炮是必不可少的，在这些任务中，任何大小的炸弹，无论是导弹还是非制导炸弹都会对地面上的友军造成危险，或者担心附带损害，例如在城市环境中。</p>
<p>当我们的部队被距离只有几米远的敌人伏击或超支时，大炮更加重要，在“危险关闭”的情况下，只有最准确的火力才能帮助我们的方面杀死或驱散敌人。</p>
<p>在最近的兰德研究中接受采访的地面指挥官表示，他们更倾向于使用A-10的炮火甚至是导弹弹药，因为80％的炮弹在瞄准点的20英尺半径范围内发射，提供了精确的精确度。危险关闭情况绝对需要。大炮对于击中移动目标也是最有用的，因为大炮爆发可以在预期移动时引导目标。</p>
<p><em>目前舰队中的三架F-35型号都不能在战斗中使用大炮。 事实上，他们都没有接近完成他们的发展飞行测试 - 更不用说他们的操作适用性测试 - 对于机身安全性，准确性和目标杀伤力。</em><br><br><br>更糟糕的是，根据初步的测试经验，似乎所有三种F-35版本的头盔式瞄准具的严重不准确性使得大炮在空对空作战中无效 - 这也会使CAS无效 - 而且头盔的准确性问题可能在技术上是固有的，也是无法治愈的。</p>
<p>请注意，CAS的炮弹精度要求比空战要严格得多：在友军部队附近射击时，即使是轻微的精确度问题也会产生悲剧性后果。如前所述，海军陆战队的F-35B和海军的F-35C的炮舱可能会增加另一个不准确的来源 - 也可能是无法治愈的 - 并且仍未经CAS测试。</p>
<p>F-35大炮对CAS的战斗适用性直到3F区块IOT＆E结束时才会知道，这在2021年之前是不可能的。未能完全实现这些CAS测试 - 由于JPO管理不善和测试资源延迟，这种可能性很大 - 肯定会危害美军的生命。</p>
<p>除了关键的大炮不准确性问题之外，飞行员头盔显示器中符号杂乱的引起误差的混乱在CAS角色中尤其危险。 DOT＆E表示，由于符号杂乱模糊目标，难以读取关键信息和pipper [aimpoint]稳定性，目前的系统“在操作上无法使用，并且可能无法完成计划的测试。”</p>
<p>即使头盔显示的符号没有遮挡飞行员看到目标的能力，F-35的顶篷也可能。喷气式飞机的顶篷是一种厚的丙烯酸材料，具有低可观察的涂层，以保护隐形。这使得顶篷不太透明，并且根据DOT＆E似乎扭曲了飞行员的视野。</p>
<p>在F-35的每个版本中进一步限制加农炮的有效性是它携带的25毫米炮弹的数量–F-35A为182，B和C为220.这对于CAS来说是非常不足的，特别是与由A-10运载的超过1,100个30毫米炮弹。虽然A-10有足够的炮弹用于10到20次攻击，但F-35的任何变种只有两次，也许四次传球。</p>
<p>更有效地使用任何CAS武器，大炮或其他装置，更有限制的是，F-35无法飞得低而且速度慢，无法找到典型的难以看见的CAS目标并安全地将其识别为敌人或友军，即使是在提示 由地面或空中观察员。</p>
<p>由于其小而重载的机翼，F-35无法在寻找隐藏和伪装目标所需的低速下充分操纵 - 并且完全没有装甲和高度易燃，它将遭受来自小型步枪和轻型机枪的灾难性损失在低海拔和所需的低速度下不可避免地命中。与此形成鲜明对比的是，A-10专门设计用于出色的低速和低速机动性，并且在设计上具有前所未有的生存能力，可以抵御那些枪支，甚至可以抵抗肩射式导弹。</p>
<p>空军官员经常认为，缺乏有效的枪支或无法操纵低速和慢速在未来的战争中无关紧要，因为空军打算以不同的方式进行CAS，即在高海拔地区使用较小的精确弹药。但F-35将不会被清除至少携带这些武器五年。</p>
<p>与此同时，F-35现在只能携带两枚制导炸弹，而这些炸弹则为500磅或更大。这些模型都不适用于友军部队附近。根据军方的风险估算表，在250米（820英尺）处，500磅重的炸弹有10％的几率使友军失去能力。这意味着在那个泡沫中，敌人可以在没有近距离空中支援的情况下进行机动。<br><br>250磅重的小直径炸弹II现在处于低速生产状态，并在F-15E上使用;然而，即便如此，在“危险关闭”的交火中，在友军部队附近使用它太大了，在F-35上使用它所需的软件和炸弹机架将无法在2021年之前完成战斗。<br><br>近距离空中支援不仅仅是对目标投掷炸弹的飞机。为了真正有效，CAS任务需要飞行员和在地面作战的部队之间进行详细的战术协调。几十年来，这已经通过无线电通信有效地完成，并且近年来，运营中的飞机已经通过称为可变消息格式和链路-16的网络系统上的语音和数据的数字通信链路进行了升级。<br><br>在飞行测试中，F-35的数字数据链路遇到了很大的困难，包括丢失的信息或以错误格式传输的信息。这迫使飞行员和地面控制器通过无线电通过语音重复信息来在系统周围工作。在近距离的交火中，当秒数计算时，这是部队无法承受的危险延误。<br><br>F-35防守队员总是迅速指出近乎对等的对手防空系统所谓的致命能力，作为在CAS中使用F-35以及禁止轰炸的必要性的理由。空军上校Mike Pietrucha介绍了一个更健全的战术和历史观点，指出在一个重防空威胁领域飞行CAS任务的情景充其量不太可能。<br><br>繁琐，缓慢，物流密集的“高威胁”导弹系统不太可能被近邻敌人进行现代机动战争拖累。正如他们在第二次世界大战期间面对的那样，韩国，越南，沙漠风暴，我们的近距离支持飞行员更有可能面对较轻的轻型和移动防空（机枪，轻型高射炮和人类携带的寻热导弹）以及过去15多年的战争。<br><br>在宣布F-35 IOC时，海军陆战队员 - 曾经将CAS作为独特海军遗产的一部分而获奖 - 而空军显然认为这些F-35 CAS限制是可以接受的。<br><br>但是，看到近距离空中支援作为F-35计划的事后补救被视为可耻是可耻的。为了提供足够的CAS，纳税人的资金将更好地用于维持经过战斗验证的A-10，直到测试和部署更加有效且更实惠的后续工作。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371489499-d230ab6a-88c6-49b8-9ed0-fa5750183844.png#width=826" alt><br><a name="hnuaxl"></a></p>
<h4 id="海军的F-35不适合航母作战"><a href="#海军的F-35不适合航母作战" class="headerlink" title="海军的F-35不适合航母作战"></a><a href="#hnuaxl"></a>海军的F-35不适合航母作战</h4><p>海军F-35变型必须具备的最重要特征之一是它必须能够从航空母舰上运行。否则，设计飞机的专业海军版本有什么意义？但海军自己的飞行员说F-35C并不适用于这些船只。<br><br>发展测试显示，弹射器发射期间发生了大量的抽搐 - 称为“过度垂直振荡” - “使F-35C在操作上不适合航母作战，”在最新一套船舶试验期间在美国乔治华盛顿号航空母舰上进行训练的舰队飞行员表示“。<br><br>从承载的甲板上起飞的飞机需要大力提升以达到提升和起飞所需的速度，这是通过安装在驾驶舱内的弹射器实现的。<br><br>在喷气式飞机发射之前，飞行员增加发动机推力。为了防止喷气式飞机在发射前从船的前部滚落，它们会被挡住杆挡住。当推力被压下时，推力会压缩齿轮的支柱。根据2017年1月泄露给内部防御的海军报告，当释放后退杆并且发射喷射时，F-35C的支柱被卸下，导致机头上下弹跳，震动飞行员。<br><br>这里的严重程度可以在这里清楚地看到：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371535128-4beffe90-8534-40a6-aaad-ca44e7de7567.png#width=660" alt><br>这个问题对飞行员来说很危险。头盔式显示器非常重，目前重量为5.1磅，当它与弹射器发射时产生的力相结合时，额外的重量会使飞行员的头部前后晃动。在70％的F-35弹射器发射中，飞行员报告头部和颈部出现中度至重度疼痛。<br><br><br>发射也会影响头盔的对齐。飞行员报告说难以读取头盔内的重要信息，他们必须在进入空中后重新调整它。飞行员说，这是不安全的，因为它发生在任何飞行的最关键阶段之一。飞行员试图通过收紧身体安全带来抵抗振荡，但这会在紧急情况下难以触及紧急开关和弹射手柄，从而产生新的问题。</p>
<p>F-35的项目经理克里斯托弗·波格丹中将表示，他将尝试对F-35C的前起落架支柱进行短期调整以解决问题，但实际上可能需要长期修复，例如重新设计整个前起落架组件。这种情况不太可能在2019年之前开始 - 同年海军已表示有意宣布F-35C准备战斗。</p>
<p>到那时，海军很可能在舰队中拥有36架F-35C，其中每架都需要更换前起落架，但需要确定成本。</p>
<p>F-35C的问题不仅限于飞行的开始。就像喷气式飞机需要从航空母舰起飞一样，它也需要在着陆期间停止帮助。这是通过横跨甲板的电缆实现的。当一架喷气式飞机降落时，飞机上的一个挂钩抓住其中一根电缆，该电缆使用船内的液压发动机吸收能量并使喷气机停止运转。</p>
<p><em>他的测试团队发现F-35C的制动装置上的钩点磨损速度比预期快三倍。 虽然它应该至少持续15次着陆，但在测试中持续时间最长的一个钩点是5。 据报道，该计划正在考虑重新设计制动装置以使其更加坚固。</em><br><br><br>F-35C还有待解决的另一个结构性问题涉及机翼。在试飞期间，工程师发现机翼末端的强度不足以支撑AIM-9X短程空对空导弹的重量。 F-35C的机翼两端折叠，以便在飞机载体的甲板和机库的拥挤范围内节省空间。当导弹经过机翼折叠时，当飞机难以操纵和着陆时，重量超过结构限制。<br><br>根据DOT＆E的说法，在问题得到解决之前，“F-35C对于导弹运输和就业将具有有限的飞行范围，这将对机动，[和]近距离接合产生不利影响。”这甚至比F-35的其他固有机动限制。问题非常严重，波格丹将军承认F-35C将需要一个完全重新设计的外翼。<br><br>发射和恢复飞机只是海军航空挑战的一部分。维护人员还必须能够在海上保持喷气式飞机的飞行性。机组必须能够执行的关键维护功能之一是发动机拆卸和安装（R＆I）。 2016年8月，Crews在乔治华盛顿号航空母舰上进行了第一次R＆I概念验证演示。<br><br>机组人员需要55个小时来完成发动机交换，这比在传统飞机上执行相同操作所花费的时间要长得多。例如，F / A-18上的发动机可以在六到八个小时内更换。 DOT＆E指出，为了安全起见，机组人员花时间执行所有必要步骤，并指出随着机组人员获得更多经验，未来的迭代可能会更快一些。<br><br>也就是说，机组人员充分利用了整个机库海湾空间，这是他们乘坐飞机机翼时不会有的东西。这可能加快了演示期间的过程。</p>
<p>更换F-35中的发动机比F / A-18更复杂。工作人员必须拆除几个皮肤面板和一个称为尾钩支架的大型结构件，以便拆卸发动机，从而在维护机库中需要更多空间。这些部件以及与之相关的所有管子和电线必须妥善储存，以防止损坏，同时还要占用额外的空间。</p>
<p>维护人员必须在存在全空气翼的情况下执行此过程，以便了解系统是否在操作上合适。并且该过程必须变得非常有效，以产生战斗所需的出击率。</p>
<p>在乔治华盛顿试验期间发现的另一个问题涉及F-35C计算机生成的大量数据文件的传输。</p>
<p>F-35计划依赖于自动后勤信息系统（ALIS），这是庞大而复杂的计算机系统，所有F-35都用于任务规划，维护诊断，维护计划，零件订购等。为了正常工作，系统必须在船上和船外通过网络移动大量数据。</p>
<p>在华盛顿试验期间，机组人员必须通过船舶的卫星网络传输中等大小的200 MB ALIS文件。花了两天时间。带宽限制和不稳定的连接极大地阻碍了数据的传输。许多这样的变速器 - 甚至更大的变速器 - 将需要支持整个机翼。<br><br>此外，舰队经常在“排放控制”或无线电静音期间运行，以避免将其位置泄露给敌人，进一步阻碍了保持F-35飞行所需的数据传输。</p>
<p>乔治华盛顿的审判产生了大量令人讨厌的新闻报道。至少在公开场合，海军声称成功了。然而，有证据表明海军对该计划不太兴奋，因为上面讨论过这类问题，当然还有成本 - 服务购买F-35C的速度很慢。</p>
<p>虽然空军准备在2017年购买44架新的F-35，但海军只会买两架。海军还在其2017年无资金优先顺序（“愿望”）名单中要求另外增加14架F / A-18，并再增加两架F-35C。此外，这是服务没有急于过早宣布战斗准备的唯一变种。<br><br>五角大楼的一些领导人表示，海军的变种是唯一一个受特朗普政府下令审查的威胁，而国防部长詹姆斯马蒂斯目前正在进行审查。这可能证明是该计划的一部分，其中寻求F-35的可行替代方案。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371695072-55a96b06-342b-48ae-9d76-cf8961517f70.png#width=826" alt><br><a name="w1shub"></a></p>
<h3 id="关于F-35唯一隐形的东西-价格标签"><a href="#关于F-35唯一隐形的东西-价格标签" class="headerlink" title="关于F-35唯一隐形的东西 - 价格标签"></a><a href="#w1shub"></a>关于F-35唯一隐形的东西 - 价格标签</h3><p>自大选以来，人们对F-35的进一步采购和可负担性表示了很多看法。唐纳德·特朗普总统在就职典礼前对一系列推文中的价值提出了质疑，但当他宣布洛克希德·马丁从最新一批F-35的价格中削减6亿美元时，他希望该计划能够大幅改变。 。<br><br>洛克希德·马丁公司及其在日本特许厅内的合作伙伴已表示价格会降低，这主要是由于制造业的效率提高。<br><br>从表面上看，这对美国纳税人来说似乎是一个巨大的发展，但现在任何“节省下来”的钱最终都会花费更多，因为我们购买了一堆未经测试的原型，后来需要进行大量昂贵的改造。如果洛克希德·马丁公司和联合计划办公室能够在计划完成测试和评估过程之前批准对400架F-35进行为期三年的“大量购买”，那么这个问题就更加复杂了。<br><br>报刊上报的价格通常基于空军常规起飞变型F-35A的成本 - 这三种变型中成本最低。此外，这个成本数字只是对未来成本的估计，假设从现在开始，F-35的一切都将完美运行 - 这不太可能，因为该计划进入其技术最具挑战性的测试阶段。<br><br>正如最新的DOT＆E报告显示，在F-35准备战斗之前，该计划还有很长的路要走。<br><br>联合计划办公室最近声称F-35A的价格在2016财年合同中每个都低于1亿美元。然而，在2016财年的立法中，国会为每架F-35A拨款1.196亿美元。<br><br>即使这个数字也不能说明整个故事 - 它只包括采购成本，而不是将F-35A带到最新批准的配置所需的成本，以及用于容纳和操作F-35A的额外军事建筑成本。<br><br>当然，1.196亿美元的价格标签不包括开发和测试F-35A的任何研发费用。海军陆战队F-35B和海军F-35C的2016年仅生产成本分别为1.664亿美元和1.852亿美元。<br><br>首先，它们不包括修复最近，当前和未来测试中发现的设计缺陷所需的成本 - 这不是一笔不大的金钱。它们也不包括计划的现代化努力的成本，例如飞机的第4座，将来将被纳入所有F-35A。政府问责办公室估计，该计划将在未来六年内至少花费30亿美元用于现代化工作。<br><br>据GAO称，例如，到目前为止所解决的一些问题的修改费用为4.267亿美元。这些飞机中的每一架都已经过修改，将来需要更多。空军已经承认必须改装交付给它和运营舰队的所有108架F-35A。随着已知问题得到修复并且发现新问题，这些成本将继续增长，并且它们是每架飞机成本的组成部分。<br><br>随着程序从测试的简单部分 - 开发或实验室测试 - 转移到未来几年的关键作战（运行）测试期，将会发现更多问题。<br><br>一个很好的例子发生在2016年底，当时工程师在F-35的油箱内发现了碎片。经过仔细检查，他们发现包裹在冷却剂管线周围的绝缘材料已经解体，因为分包商未能使用适当的密封剂。并且，当GAO估计将花费4.267亿美元来修复已经在仓库中的F-35A中的一些已知问题时，尚未发现冷却剂管线绝缘问题。<br><br>必须在已经生产和购买的飞机机队中设计，测试和实施对此问题和其他问题的修复。<br>其次，JPO，洛克希德马丁公司和五角大楼所使用的不完整的单位成本估算 - 他们所谓的“飞行​​”单位成本 - 不包括购买支持设备（工具，ALIS计算机，培训模拟器，初始备件）需要使F-35A机队能够运行。从字面上看，国防部的“飞行”成本并没有购买能够进行飞行操作的系统。<br><br>五角大楼已经承诺购买346架F-35，因为该计划已进入美国国防部委婉地称之为“低速初始生产”。<br><br>798喷气式飞机服务将在2018年至2021年期间大约450架次购买的798架喷气式飞机将占总采购量的近33％……所有这些都在该计划完成初步运行测试之前，并且发现了什么按预定工作，什么没有吨。<br><br>值得注意的是，真正的问题发现过程只会在2019年按计划开始运行测试时开始，或者更有可能在2020年或2021年开始，当时运营代表性飞机实际上已准备好进行测试。空军已经开始修改的108架飞机只是冰山一角，这个数字不包括数百架海军陆战队和海军飞机的类似修改。<br><br>拟议的“大宗购买”提出了许多其他问题。也许Gilmore所提出的最相关的问题是：<br><br>Block Buy是否与政府主张的“购买前飞行”方法一致，以及“美国法典”第10篇中规定的运营测试要求的理由，还是被视为“全额” “IOT＆E之前的决定是否已经完成并向国会报告，不符合法律规定？<br><br>只要符合某些标准，联邦法律允许多年合同购买政府财产。国会通常每年批准大多数武器购买计划，以确保对计划进行适当的监督，并保持对承包商表现令人满意的激励。<br><br>根据Title 10 U.S.C.，Section 2306b，对于有资格进行多年采购的计划，合同必须促进国家安全，应该节省大量资金，减少几率，并且设计稳定。 F-35似乎在前三个标准中至少有两个失败，并且肯定是第四个失败了。<br><br>关于F-35成本问题的一个重要部分是购买大型飞机是否合理，并担心以后修复尚未发现的问题的成本。这肯定是增加成本的好方法，但在临时中隐藏它。</p>
<p><em>实际操作F-35机队的成本仍然存在。 美国国防部估计，该计划50年的所有培训和运营运营 - 假设每架飞机的寿命为30年 - 将为1万亿美元，使购买和运营F-35的成本至少达到1.4万亿美元。</em><br><br><br>操作F-35的成本非常高，因为飞机与其他飞机相比非常复杂。根据空军自己的数据，2016财年，每架F-35飞机平均飞行163小时，每小时飞行4426美元。<br><br>为了进行比较，在同一年，机队中的每架F-16飞机平均飞行258小时，每飞行小时20,398美元。 A-10平均每小时飞行358小时，每小时17,227美元。虽然这些时间从未经过独立审核，而且无法确定它们是否完整，但现有数据表明F-35的飞行成本是飞机的两倍以上。<br><br>五角大楼隐藏F-35真正成本的一个更重要的方式是它推迟到第4区块开发和交付应该在第3区提供的许多关键能力。目前已计划但未包括根据政府问责局的数据，在F-35的官方成本估算中 - 或者甚至作为一个完整的单独收购计划 - 是一个由四部分组成的Block 4升级，至少耗资30亿美元。<br><br>此外，DOT＆E报告称“有17个记录的失败，无法满足规范要求，程序承认并打算寻求合同规范变更，以便结束SDD [系统开发和演示]。”<br><br>这意味着F-35计划无法提供17种关键作战能力，而且计划办公室正试图给洛克希德·马丁公司提供交付通行证，直到后期的高级开发过程。<br><br>虽然没有人公开声明现在不会包括哪17种战斗能力，但它们都是F-35应该拥有的所有功能，并且美国人民正在为此付出全价。因此，我们将来会支付更多的钱来升级现在购买的F-35，以便他们能够执行我们已经支付的功能。<br><br>2016年F-35A的1.196亿美元单位成本严重低估，多年来不会充分了解额外成本。那些假装2016年成本低于1亿美元的人只是在欺骗公众。</p>
<p><a name="q9x6ix"></a></p>
<h3 id="战斗力有效"><a href="#战斗力有效" class="headerlink" title="战斗力有效"></a><a href="#q9x6ix"></a>战斗力有效</h3><p>在每一流的空军中，出战优秀的战斗机飞行员要求他们每月至少飞行30个小时来磨练和提高他们的战斗技能。这就是F-35缺乏战斗力的最大原因：由于飞机前所未有的复杂性和相应的可靠性和维护负担，飞行员根本无法经常飞行以获得足够的实际飞行时间来发展他们需要的战斗技能。<br><br>如果飞行员无法获得足够的飞行时间，飞行员技能就会萎缩。即使拥有卓越的技术，训练有素的飞行员也不会受到训练有素的飞行员在飞行不太复杂的飞机上的影响。<br><br>飞行时间不足也会造成危险的安全状况，威胁到飞行员的训练生命。海军陆战队在过去一年中遭遇了九次严重的飞机坠毁事故，造成14人死亡。该军团的顶级飞行员最近表示，撞车事故的飙升主要是由于飞行员没有足够的飞行时间。<br><br>这种趋势将随着F-35而恶化。鉴于其固有的复杂性和相关的成本，F-35极不可能经常飞行以获得成功的飞行员。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545371909219-5c3e7d63-0c79-41a3-a751-dc3afbf49136.png#width=826" alt><br><a name="ebawca"></a></p>
<h3 id="F-35可以在需要的时候到达吗？"><a href="#F-35可以在需要的时候到达吗？" class="headerlink" title="F-35可以在需要的时候到达吗？"></a><a href="#ebawca"></a>F-35可以在需要的时候到达吗？</h3><p>即使这是一个很大的中频，F-35也可以像洛克希德·马丁所说的那样在战斗中表现出来 - 更不用说F-16，A-10和F-18的替代品应该如何表现 - 如果喷气式飞机不能满足他们需要的地方，那么该计划仍然毫无价值。<br><br>有几个因素导致难以及时部署F-35中队。一个是F-35的任务规划系统，它是ALIS网络的一部分。在完成战斗任务的细节 - 例如目标，预测的敌方雷达位置，要飞行的路线和武器装载 - 之后，需要将数据编程到飞机中。将该信息加载到墨盒上，然后将墨盒插入喷嘴中。<br><br>F-35飞行员在Offboard Mission Support（OMS）系统上对这些弹药筒进行编程。<br><br>DOT＆E发现的问题是，飞行员一直认为用于支持任务规划的系统“繁琐，无法使用，并且不适合操作使用。”他们报告说，构建任务计划文件所需的时间太长，以至于扰乱了超过一架飞机的任务计划周期。</p>
<p><em>这意味着当几架F-35接收任务时，如果分配了大量的计划时间，他们就无法足够快地完成所有飞行前过程以按时启动。</em><br><br><br>2016年2月和3月，空军在加利福尼亚的爱德华兹空军基地向爱达荷州的山地空军基地进行了部署演示，对F-35计划进行了重大测试。这是该服务首次尝试使用更新ALIS的版本 - 基于地面的计算机系统，用于诊断机械问题，订购和跟踪更换零件，并指导维修人员进行维修。<br><br>无论何时中队部署，都必须在部署F-35的任何地方建立一个ALIS枢纽。 Crews建立了一个ALIS标准操作单元（SOU），它由几个计算机设备组成。技术人员将使用这些设置一个小型主机，然后必须将其插入全球范围的ALIS网络。<br><br>工作人员花了几天时间让ALIS在本地基础网络上工作。经过大量的故障排除后，IT人员发现他们必须在Internet Explorer上更改多个设置，以便ALIS用户可以登录系统。这包括降低安全设置，DOT＆E以值得称道的轻描淡写的方式指出这是“可能与所需的网络安全和网络保护标准不兼容的行为”。<br><br>ALIS数据必须在中队所在的任何地方。在飞机被允许执行飞行任务之前，机组人员必须将数据从本垒站的中队主ALIS计算机传输到部署的ALIS SOU。在Mountain Home部署期间，此过程花了三天时间。这比之前的演示要快，但洛克希德·马丁为演习提供了8位额外的ALIS管理员。<br><br>目前还不清楚承包商或空军是否会在未来的部署中包含这种级别的支持。当演习结束时中队重新部署回爱德华兹时，管理员花了四天时间将所有数据传回主ALIS计算机。这种延迟将限制F-35在危机时刻快速部署的能力。</p>
<p><em>即使喷气机能够定位足够的时间来应对危机，长时间上传时间等问题也可能使它们在空中需要时保持在地面上。固定在地面上的飞机是目标，而不是资产。</em></p>
<p>另一个耗时的过程涉及向每个ALIS标准操作单元添加新飞机。每次将F-35从一个基座移动到ALIS已经启动的另一个基座时，必须将其导入该系统。这需要24小时。因此，当F-35部署到新基地时，整个一天会在处理数据时丢失。并且一次只能上传一架飞机。<br><br>如果整个中队（通常是12架飞机）需要被引导，整个过程将需要将近两周时间，迫使指挥官慢慢将他的F-35飞机投入战斗。<br><br>该计划的关键任务软件也出现延误。如前所述，F-35需要广泛的任务数据负载（MDL），以便飞机的传感器和任务系统正常运行。 MDL在某种程度上包括有关敌人和友好雷达系统的信息。他们发送喷气式传感器的搜索参数，以便他们正确识别威胁。这些需要更新以包含最新信息。它们也适用于每个主要地理区域。<br><br>MDL都在佛罗里达州埃格林空军基地的美国重编程实验室进行编程，然后发送给所有相关的中队。该实验室是整个F-35计划中最重要的组成部分之一。据DOT＆E称，该实验室必须能够“快速创建，测试和优化MDL，并在代表现实情景的压力条件下验证其功能，以确保F-35任务系统的正常运行和飞机的运行效率。战斗以及F-35与Block 3F的IOT＆E。“<br><br>官员们在2012年确定了该实验室管理层的严重缺陷。纳税人在2013年至2016年期间花费了4500万美元来解决这些问题。尽管有警告和额外资金，但实验室的开发仍然受到管理不善的困扰，这种管理不能阻止在当前的基本作战配置中“有效地创建，测试和优化运营飞机的MDL”。<br><br>需要升级实验室以支持F-35上使用的每个软件版本。该实验室目前配置为支持块2B和3i软件版本。 F-35的第一个完全战斗版软件将是Block 3F。该实验室需要进行重大更改以支持此版本，这对于战斗测试是必要的，更重要的是，完全战斗准备就绪。<br><br>实验室远远落后于一些必要的设备甚至尚未购买。例如，该设施还依赖于前面提到的专用射频发生器来重新创造潜在对手可能对F-35使用的那种信号。实验室将使用它们来测试MDL，然后将它们发送到机队飞机上，以确保喷气式飞机的传感器能​​够正确识别它们。<br><br>在急于假装初始作战能力的情况下，空军和海军陆战队实际上已经制造出一架完全未准备好面对敌人的飞机。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545372059027-ed5f34d0-f86c-471f-bcce-9c165524ebe0.png#width=826" alt></p>
<p><a name="shztbe"></a></p>
<h4 id="F-35可靠性问题"><a href="#F-35可靠性问题" class="headerlink" title="F-35可靠性问题"></a><a href="#shztbe"></a>F-35可靠性问题</h4><p>即使一架F-35中队能够到达需要的地方，在需要它的时候，如果它不能飞行任务会有什么好处呢？这是F-35计划中最持久的问题之一。</p>
<p>该车队的可靠性记录非常糟糕 - 它未能实现许多临时可靠性目标，并且在2016年之前仍然如此。随着该计划进入最重要的运行测试阶段，真正担心飞机会不能经常飞行以满足测试时间表。还有人担心喷气式飞机在召唤战斗服务时能够多久飞行一次。<br><br>“可用性”衡量飞机在现场执行至少一次指定任务的频率。由于大多数飞机在1991年在波斯湾的沙漠风暴行动中取得了成功，这些服务努力维持其飞机可持续作战行动的80％可用率。这与测试机队为满足这一需求所需的速度相同。 IOT＆E时间表。<br><br>到目前为止，F-35计划甚至无法实现其60％可用性的临时目标。</p>
<p>2016年度车队平均可用率为52％。这是近几年来的一次改善，但DOT＆E警告称“增长既不稳定也不连续。”而且增长曲线落后于进度。将用于运行测试的飞机需要配备专门的仪器来测量性能。目前有17架喷气式飞机驻扎在加州爱德华兹空军基地。 2016年前9个月，该测试车队的平均可用率为48％。</p>
<p>有几个因素拖累了F-35机队的可用率。许多飞机不得不被送回仓库进行大修，这是该程序高并发水平的结果。例如，15 F-35A需要被送回以纠正制造缺陷，其中喷射燃料箱内的泡沫绝缘材料使铸造碎屑劣化成燃料。<br><br>其他大修是必要的，因为存在基本的设计缺陷，包括不满足寿命要求的主要结构部件，还有一些是由于飞机首次建造时已知缺乏的战斗能力设计的持续改进所驱动。 ”</p>
<p>即使飞机没有进行大修，它们也不会飞得太多。在可用的飞机中，它们可以分为两类：任务能力和完全任务能力。使命能力的飞机是那些准备进行至少一种任务的飞机，即使它只是一项训练任务;具有完全任务能力的飞机是准备执行飞机宣布能够执行的所有任务的飞机。后者是战斗准备飞机的真正衡量标准。</p>
<p>Mission Capable和Full Mission Capable F-35的可用率在去年有所下降。 2016财年的机队任务能力率为62％，低于2015财年的65％[DG3]。完全任务能力率仅为29％，而前一年为46％。</p>
<p>Gilmore的报告引用了分布式孔径系统，电子战系统，电光靶向系统和雷达等主要作战系统的失效，这是导致能力下降的最高驱动因素。值得注意的是，这些系统据说为F-35提供了独特的作战能力，这就是将F-35保持在地面上的系统 - 没有任何能力。<br><br>根据最近发布的年度运营成本图表，平均而言，2016年空军的F-35飞机每周只能飞行两架次。相比之下，F-16平均每周近三架次，A-10机队平均近四架次。 F-35需要大量的维护才能实现。</p>
<p>虽然官方发布的公开声明说维护人员在喷气式飞机上工作是多么容易，但DOT＆E报告描绘了不同的画面。</p>
<p>供应链问题已经迫使维护者蚕食飞机;从一架飞机上取下部件安装在另一架飞机上，以确保至少有一架飞机飞行。食人化具有增加进行修复的总时间的效果，因为它增加了从供体喷射器剥离部件的额外步骤，而不仅仅是从盒子中取出新的或修复过的部件。它还需要将部件安装两次：首先是在修复的喷射器中，然后是在拆卸的喷射器中。</p>
<p>对于2016财年，维护人员不得不拆掉几乎每10架次飞行一次的部件，这远远超过了计划中每100架次不超过8次拆分行动的不起眼的目标。</p>
<p>随着产量的增加，供应问题可能会减少，但基本设计问题将持续存在。 一个典型的例子是F-35隐形涂层固有的独特维护要求。 对隐形飞机进行一些修理需要更长的时间，因为需要时间去除低可观察到的材料，修复破损的物体，然后修复隐形皮肤。<br><br>这些修复通常涉及使用需要时间进行化学固化的粘合剂。 其中一些材料可能需要长达168小时 - 一整周 - 才能完全干燥。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545372168522-b9bd672b-4a02-4ff4-b2f7-1b713cb14d88.png#width=826" alt></p>
<p><a name="rt9ngs"></a></p>
<h3 id="官员隐瞒F-35问题和纳税人延误的真相"><a href="#官员隐瞒F-35问题和纳税人延误的真相" class="headerlink" title="官员隐瞒F-35问题和纳税人延误的真相"></a><a href="#rt9ngs"></a>官员隐瞒F-35问题和纳税人延误的真相</h3><p>当洛克希德·马丁在17年前首次赢得合同时，预计F-35将在2008年开始运行测试。一旦他们未能达到这一目标，2017年应该是战斗测试过程开始的重要一年。我们现在知道，这个过程几乎肯定会被推迟到2019年……甚至2020年。<br><br>DOT＆E报告的第一页列出了F-35的13个未解决的主要问题，这些问题将阻止该计划于2017年8月开始进行战斗测试。但你不会从负责官员的公开评论中得知这些问题。该程序。<br><br>在2月份众议院军事委员会小组委员会作证时，尽管DOT＆E报告在不到一个月前发布，但官员却没有向国会提出任何这些问题。<br><br>F-35的挑战规模在今年的DOT＆E分析中很容易量化。根据该报告，F-35仍然有276个“至关重要的”缺陷 - 这些必须在开发过程结束之前修复，因为它们可能“导致IOT＆E或战斗期间的操作任务失败”。<br><br>在276个中，有72个被列为“优先级1”，这些服务关键缺陷会阻止服务在固定之前部署喷气式飞机。<br><br>关于F-35在战斗中的缺点已经有很多，但基本机身仍然存在结构性问题。这方面的一个例子是喷气机的垂直尾翼和机身之间的连接接头失效。这是一个长期存在的问题，因为在原始设计中发现了缺点。<br><br>工程师在2010年的早期结构测试中发现用于加固接头的套管过早磨损。该接头在2014年进行了重新设计并纳入新飞机。        2016年9月，检查员发现重新设计的接头在经过250小时的飞行测试后失败了 - 远远低于JSF合同中规定的8,000个工作小时数。<br><br>2016年F-35任务系统的测试继续落后于计划。项目经理确定并预算基线测试点，或“在特定飞行测试条件下的性能离散测量”。<br><br>这些用于确定系统是否符合合同规范。测试团队还会出于各种原因添加非基线测试点，以全面评估整个系统。示例包括添加测试点以准备稍后的更复杂的测试，在软件更新后重新测试系统以确保新软件不会改变先前的结果，或“发现测试点”，这些测试点被添加以识别在其他测试期间发现问题的根本原因。<br><br>该计划为2016年F-35的任务系统预算了3,578个测试点。测试团队无法完成所有测试，完成3,041，同时还在全年增加了250个未预算的测试点。<br><br>尽管计划有所下滑，但F-35计划办公室已表示希望跳过许多所需的测试点，而是依赖测试以前飞行的数据 - 测试飞机使用早期软件版本 - 作为升级系统软件工作的证据。但DOT＆E警告说，较新的软件版本可能表现不同，使早期的结果没有实际意义。程序管理员基本上想要宣布开发测试过程并继续进行操作测试，即使他们还没有完成所有必要的步骤。<br><br>这是一个风险很大的举动。 DOT＆E警告说遵循这个计划。</p>
<p><em>“可能会导致IOT&amp;E失败，导致需要进行额外的后续运行测试，最重要的是，将Block 3F运送到能力严重不足的现场 - 如果需要F-35，该部门必须具备的能力 与当前威胁作斗争。“</em><br><br><br>程序办公室似乎在试图测试许多可能使F-35如此不可或缺的能力方面拖延了下来。<br><br>一个例子是开发验证模拟器（VSim）需要多长时间。洛克希德·马丁公司的工程师在2001年负责创建VSim设施，该设施旨在成为一个超现实的，经过全面测试验证的“人在环，任务系统软件在环仿真，旨在满足Block 3F IOT＆E的运行测试要求。“<br><br>也就是说，它旨在在虚拟现实中测试那些复杂而严谨的场景，这些场景在现实生活中不可能或太危险，而不是真正的战争。<br><br>承包商远远落后于建设计划，JPO在2015年放弃了VSim。相反，海军航空系统司令部的任务是建立一个政府运行的联合仿真环境（JSE）来执行VSim的任务。承包商应该提供飞机和传感器模型，但到目前为止“F-35模型的谈判尚未成功。”<br><br>这阻碍了该计划设计虚拟世界，其中F-35和敌方飞机和防御装置在现实世界中相互作用，造成进一步的延误。<br><br>没有经过适当准备的JSE，F-35无法完全测试。必须根据飞行试验期间收集的真实数据设计模拟，否则模拟只会测试承包商所说的喷气机可以做什么。<br><br>例如，真正的F-35必须飞越一个测试范围，我们的敌人使用相同的雷达系统是活跃的，以便它可以收集有关喷气式飞机的机载传感器如何反应的数据。该数据用于验证仿真软件。这是一个非常复杂的过程需要时间。正如DOT＆E报道的那样，“此前的努力已经花费了数年时间，因此NAVAIR不太可能及时按计划完成项目，以支持物联网和E。”<br><br>该计划还制定计划，以便在计划最需要时减少测试人员和测试飞机的数量。这些计划将使测试飞机的数量从18个减少到9个，测试人员从1,768减少到600。<br><br>吉尔摩尔在空军国际奥委会声明后不久报道该计划将无法在必要的最终配置中生产足够的F-35以进行操作测试。<br><br>“由于在开发测试期间需要很长的程序延迟和发现，因此需要进行大量修改，以便将装配过程中连接到飞行测试仪器的OT飞机纳入所需的生产代表配置中，”报告指出。<br><br>接着说，对23架飞机进行了超过155次改装，专门用于即将进行的战斗（“作战”）测试，其中一些甚至尚未签约，这意味着IOT和E的开始将更进一步延迟。</p>
<p><em>联合计划办公室不仅没有遵守其同意的运行测试计划，而且未能资助和测试进行测试所必需的设备。 这包括没有资金用于飞行测试数据采集记录和遥测舱，这是一种安装在F-35上的仪器，用于模拟飞机的武器。</em><br><br><br>这对于报告和分析每个模拟武器射击的结果至关重要。 在飞机在接合和武器测试期间飞行的条件下，在吊舱功能和安全性被清除之前，不能进行此类测试。<br><br>五角大楼和承包商是否会继续忽视有关F-35在测试中的表现以及看似无休止的延误的令人不快的信息，并试图在美国人民和他们的心中产生错误的印象，还有待观察。政策制定者。<br><br>在最近特朗普总统和五角大楼之间的交流中，似乎没有人在日本特别行政区引导总统注意除波格丹将军以外的任何人。 很明显，他没有和任何批评该计划的人谈过，比如Gilmore。 根据这份报告的结果，如果他有，那么很难看出任何人都可以诚实地说F-35是“太棒了”。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545372354038-3b796934-db9d-4360-a02c-bc95e017fa8c.png#width=826" alt></p>
<p><a name="solnez"></a></p>
<h4 id="向前进"><a href="#向前进" class="headerlink" title="向前进"></a><a href="#solnez"></a>向前进</h4><p>DOT＆E的最新报告更加证明F-35计划将在未来几年继续大量耗费时间和资源，并将为我们的武装部队提供一台二战战斗机，其执行任务不如它本来要取代的“传统”飞机。上天捍卫国家的男男女女应该得到更好的待遇。<br><br>尽管在华盛顿有传统智慧，但服务并不一定要坚持使用F-35。其他选项确实存在。<br><br>1.为了填补空对空中的近期空洞，启动一项计划，对所有可用的F-16A和F-18进行翻新和升级，使用寿命更长的机身和更高的推力F-110-GE- 132（F-16）和F-404-GE-402（F-18）发动机。使用功能更强大的现成电子系统升级其电子系统。</p>
<p>这将使我们的战斗机在空对空作战中比后来的F-16和F-18型号或F-35更有效。如果需要增加力量，从boneyard添加机身。最重要的是，将飞行员训练时间提高到每月30小时的最低可接受水平，部分原因是现在不购买欠发达的F-35而节省了资金。</p>
<p>2.为了填补近距离空中支援部队中更为严重的近期空洞，完成空军拒绝重新训练的100架A-10的重绕，然后通过整修/扩大现有的仅272架A-10的力量。将boneyard中所有可用的A-10重新调整为A-10C标准。</p>
<p>3.立即进行三个新的竞争性原型飞行计划，设计和建造一个更致命，更生存的近距离空中支援飞机，以取代A-10，并设计和建造两个不同的空对空战斗机，这些战斗机更小，更具战斗力 - 比F-16，F-22和F-18更有效。对所有配备雷达导弹和隐身对抗措施的合格敌人进行测试。<br><br>这些程序应该遵循20世纪70年代轻型战斗机和A-X计划的模式，特别是在实弹，现实情景竞争飞越测试方面。这些计划产生了F-16和A-10两架无可争议的高效飞机，每架飞机都比当时五角大楼的首选飞机便宜。他们在不到10年的时间内进行了测试，但不超过25年。</p>
<p>4.绝对最低限度，F-35测试程序已经到位，JPO和Gilmore同意必须执行以便在进一步生产之前理解这架飞机能够胜任和不能胜任的事情。这意味着暂停进一步的F-35生产，直到这些测试完成并诚实地报告给国防部长，总统和国会。</p>
<p><a name="9mgopz"></a></p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a><a href="#9mgopz"></a>结论</h4><p>F-35计划办公室已达到关键决策点。 现在需要采取大胆行动来挽救联合攻击战斗机这样的国家灾难。</p>
<p>政府应继续审查F-35计划。 但官员们不应该只是与将军和高管交谈，因为他们没有动力去讲述真相，因为他们在确保程序存活方面有着既得的经济利益 - 无论能力如何。</p>
<p>正如本报告所示，他们并没有讲述整个故事。 从其他观点来看，还有更多人在食物链中走下坡路。 他们是拥有真实故事的人。 而且，正如上述建议所示，仍有选择。</p>
<p>对于该计划进行重大改变还为时不晚，正如其维护者所声称的那样。</p>
<p>Dan Grazier是政府监督项目的Jack Shanahan研究员，这篇文章最初出现在那里。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/21/yuque/F-35是1.4万亿美元的国家灾难/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/眼底水肿病变区域自动分割" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/21/yuque/眼底水肿病变区域自动分割/">眼底水肿病变区域自动分割</a>
    </h1>
  

        
        <a href="/2018/12/21/yuque/眼底水肿病变区域自动分割/" class="archive-article-date">
  	<time datetime="2018-12-21T01:28:40.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-21</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/ShawnBIT/AI-Challenger-Retinal-Edema-Segmentation" target="_blank" rel="noopener">链接</a><br><a name="4np7ea"></a></p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a><a href="#4np7ea"></a>背景</h3><ul>
<li><p>视网膜水肿是一种眼疾，可导致视力模糊，影响正常生活。</p>
</li>
<li><p>OCT（光学相干断层扫描）可用于帮助医生判断视网膜水肿。</p>
</li>
<li><p>早期发现水肿症状可以在疾病的治疗中发挥关键作用。</p>
</li>
<li><p>我们的任务是设计算法，以自动检测视网膜水肿的类型，并根据OCT图像划分视网膜水肿区域。</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545362517258-ac147a70-c630-4ba9-9596-429ce59aaf05.png#width=323" alt><br><a name="tevkbe"></a></p>
<h3 id="数据统计信息"><a href="#数据统计信息" class="headerlink" title="数据统计信息"></a><a href="#tevkbe"></a>数据统计信息</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363237135-d6a8f54b-194f-41e9-8998-42c2794fd373.png#width=745" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363261530-b1e78b3c-8896-4cf1-b15a-3f6656d5c9b7.png#width=634" alt><br><a name="2obuzk"></a></p>
<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a><a href="#2obuzk"></a>数据可视化</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363314506-c30f03d8-0647-4d55-ad77-3398e87c36a3.png#width=566" alt></p>
<ul>
<li><p>视网膜边缘弯曲</p>
</li>
<li><p>病变之间的包含关系</p>
</li>
</ul>
<p><a name="1xo7et"></a></p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a><a href="#1xo7et"></a>数据处理</h3><p>堆叠上部和下部切片以形成三通道输入<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363525794-cfdcff1f-fcfa-45bf-a7bd-86330b40bd22.png#width=826" alt><br>正则化<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363568658-bfc944e3-aee5-41cb-9e36-87604ccbc425.png#width=826" alt><br>数据增强（仅随机水平翻转）<br><a name="gpzgyi"></a></p>
<h3 id="问题和挑战"><a href="#问题和挑战" class="headerlink" title="问题和挑战"></a><a href="#gpzgyi"></a>问题和挑战</h3><p>这两项任务（分割和检测）如何相互促进？ •多任务学习框架<br><br><br><br>多尺度的视网膜水肿病变<br><br>•UNet和UNet ++<br><br>三种视网膜水肿样本不平衡<br><br>•指数对数损失<br><br>如何扩大感受野以检测边缘弯曲？<br><br>•扩张模块</p>
<p><a name="xtsmsg"></a></p>
<h3 id="基线"><a href="#基线" class="headerlink" title="基线"></a><a href="#xtsmsg"></a>基线</h3><p>分段 -  UNet<br><br>•下采样16×<br><br>•频道：[16,32,64,128,256]•输入：原始图像<br>检测 -  ResNet18<br><br>•调整大小224 * 224<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363778350-5ef98156-3f52-4bc4-81f7-ac1ff58bd5cb.png#width=826" alt><br><a name="12ngly"></a></p>
<h3 id="多任务框架"><a href="#多任务框架" class="headerlink" title="多任务框架"></a><a href="#12ngly"></a>多任务框架</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363824351-0b2aef87-4aa5-4d54-9da2-950238d08bc2.png#width=826" alt></p>
<ul>
<li><p>共同学习分割和检测</p>
</li>
<li><p>减少时间和计算成本</p>
</li>
<li><p>改善两项任务的效果</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363883446-cddecdd3-133c-4d7a-a48f-7ee3ce3c782e.png#width=826" alt><br><a name="45hwtb"></a></p>
<h3 id="UNet-和DeepSupervision"><a href="#UNet-和DeepSupervision" class="headerlink" title="UNet ++和DeepSupervision"></a><a href="#45hwtb"></a>UNet ++和DeepSupervision</h3><ul>
<li><p>密集的未来联系</p>
</li>
<li><p>深度监督</p>
</li>
<li><p>更有效地融合低级和高级功能</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545363984175-b5734e15-36a3-4a26-b548-247f21e500b5.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364004491-f0fd9292-3bf9-4d45-b764-712c129650ff.png#width=826" alt><br><a name="mcxadi"></a></p>
<h3 id="Exponential-Logarithmic-Loss"><a href="#Exponential-Logarithmic-Loss" class="headerlink" title="Exponential Logarithmic Loss"></a><a href="#mcxadi"></a>Exponential Logarithmic Loss</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364043982-9f418634-6d4e-421e-8ed9-0b5bdd78898b.png#width=593" alt><br>x : pixel position d il : Kronecker delta<br>i : label e : pseudocount for<br>l : ground truth label at x<br>p i ( x ) : Softmax probability which acts as the portion of pixel<br>x owned by label i<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364142210-f09d6809-e1cf-428e-a8ec-e1e7a750d02a.png#width=462" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364180115-716d53bf-c3dd-46fb-bc90-f18e23866c24.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364198735-685fb3a5-9739-4820-9578-63357d19b78b.png#width=771" alt><br>小物件怎么样？<br><br>细分？（2,3级）<br><a name="ymmopw"></a></p>
<h3 id="扩张模块"><a href="#扩张模块" class="headerlink" title="扩张模块"></a><a href="#ymmopw"></a>扩张模块</h3><p>感受野计算公式<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364372343-71750b48-5889-4c2f-8d71-4ba7d15c8b40.png#width=504" alt><br>UNet编码器的接收域<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364421357-aae861a2-0977-4ad8-9b07-c0f7af8ab715.png#width=398" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364444046-68a08360-b0a9-4c89-b14d-85c7707c6eab.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364472055-2f2fcbe0-1c8c-4719-bcab-3d7860331235.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364500288-3ff90cf8-acdc-47e6-80d7-9df1b0f98cb0.png#width=778" alt><br>大对象怎么样？<br><br>细分？（第1类）<br><a name="hvmwoz"></a></p>
<h3 id="实验摘要"><a href="#实验摘要" class="headerlink" title="实验摘要"></a><a href="#hvmwoz"></a>实验摘要</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364586336-051b209a-18bc-4c1f-959a-656c8b1ab2e2.png#width=826" alt><br>Memory : 7.3 G(batch=8), Inference time : 9.5 s/patient<br><a name="d14kdy"></a></p>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><a href="#d14kdy"></a>可视化</h3><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545364667315-bb12e09f-92b4-4d6c-9427-10ce7851f341.png#width=826" alt><br><a name="f7g5bu"></a></p>
<h3 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a><a href="#f7g5bu"></a>未来工作</h3><ul>
<li><p>检测框架（Mask R-CNN）</p>
</li>
<li><p>3D语义分割模型</p>
</li>
<li><p>在骨干中使用Res-block或Dense-block</p>
</li>
<li><p>考虑病变的关系</p>
</li>
</ul>
<p><a name="db4gcg"></a></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><a href="#db4gcg"></a>结论</h3><p>构建端到端的多任务框架，可以同时检测和分割视网膜水肿病变。<br><br>•使用最新的UNet ++模型更好地集成高级和低级功能。<br><br>使用新的指数对数损失函数来增强两种类型的小病变的分割。<br><br>•引入扩张卷积模块，显着增加模型的感受野。<br><br>只有随机水平翻转数据增强，没有后期处理。<br><br>•测试装置上单个模型的骰子为0.736。 测试集上的融合模型的骰子为0.744，检测AUC为0.986。 另外，当我们设置批次为8时，推理阶段的记忆为7.3G，每个患者的推理时间为9.5s。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/21/yuque/眼底水肿病变区域自动分割/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/语义分割 -  U-Net（第1部分）" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/yuque/语义分割 -  U-Net（第1部分）/">语义分割 -  U-Net（第1部分）</a>
    </h1>
  

        
        <a href="/2018/12/20/yuque/语义分割 -  U-Net（第1部分）/" class="archive-article-date">
  	<time datetime="2018-12-20T07:47:26.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066" target="_blank" rel="noopener">链接</a></p>
<p>这里再次写信给我6个月前的自我……<br><br>在这篇文章中，我将主要关注语义分割，像素分类任务和特定的算法。 我将提供一些关于我最近一直在努力的案例的演练。<br><br>根据定义，语义分割是将图像划分为连贯的部分。 例如，对属于我们的数据集中的人，汽车，树或任何其他实体的每个像素进行分类。</p>
<p><a name="h3mvhf"></a></p>
<h4 id="语义分割与实例分割"><a href="#语义分割与实例分割" class="headerlink" title="语义分割与实例分割"></a><a href="#h3mvhf"></a>语义分割与实例分割</h4><p>与实例分割相比，语义分割相对容易。</p>
<p>在实例分割中，我们的目标不仅是对每个人，汽车或树进行像素预测，而且还分别将每个实体识别为人1，人2，树1，树2，汽车1，汽车2，汽车3 等等。 用于实例分割的现有技术算法是Mask-RCNN：具有多个子网络一起工作的两阶段方法：RPN（区域提议网络），FPN（特征金字塔网络）和FCN（完全卷积网络）[5， 6,7,8]。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292254188-fd4348f0-a59e-441b-b0f9-9ef087b52277.png#width=640" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292265350-a6d4cf06-7a19-4037-8f98-3faa8427aa38.png#width=643" alt><br><a name="7zozib"></a></p>
<h4 id="案例研究：Data-Science-Bowl-2018"><a href="#案例研究：Data-Science-Bowl-2018" class="headerlink" title="案例研究：Data Science Bowl 2018"></a><a href="#7zozib"></a>案例研究：Data Science Bowl 2018</h4><p>数据科学比赛2018刚刚结束，我从中学到了很多东西。也许我学到的最重要的一课，即使是深入学习，与传统的ML相比，更自动化的技术，前后处理对于获得好的结果可能是至关重要的。这些是从业者获得的重要技能，它们定义了您构建和建模问题的方式。<br><br>我不会详细讨论这个特定的比赛，因为对于任务本身和整个比赛中使用的方法都有大量的讨论和解释。但我会简要提及获胜的解决方案，因为它与这篇文章的基础有关。 [13]<br><br>数据科学比赛2018就像其他数据科学比赛一样，由Booz Allen基金会组织。今年的任务是在给定的显微镜图像中识别细胞核并独立地为每个细胞核提供掩模。<br><br>现在，花一两个时间来猜测这个任务需要哪种类型的细分;语义还是实例？<br><br>这是一个掩盖图像样本，它是原始的显微镜图像。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292383227-8c6f4c64-4e97-4006-90c9-558b1d59c4e0.png#width=729" alt><br>虽然起初听起来像是语义分段任务，但这里的任务是实例分割。我们需要独立地处理图像中的每个核，并将它们识别为核1，核2，核3，……类似于我们对汽车1，汽车2，人1等的示例。也许这项任务的动机是跟踪细胞样本中细胞核的大小，数量和特征。自动化该跟踪过程并进一步加速用于治疗各种疾病的不同治疗方法的实验是非常重要的。<br><br>现在，您可能会认为如果本文是关于语义分段的，并且如果Data Science Bowl 2018是实例分段任务的一个示例，那么为什么我一直在谈论这个特定的竞争。如果你正在考虑这个问题，那么你肯定是对的，而且这次比赛的最终目标确实不是语义分割的一个例子。但是，随着我们将继续前进，您将看到如何将此实例分段问题实际转变为多类语义分段任务。这是我尝试但在实践中失败的方法，但也成为获胜解决方案的高级动机。<br><br>在这3个月的比赛期间，只有两个模型（或其变体）在论坛中共享或至少明确讨论过; Mask-RCNN和U-Net。正如我之前提到的，Mask-RCNN是最先进的物体检测算法，它可以检测单个物体并预测其掩模，如实例分割。 Mask-RCNN的实施和培训更加困难，因为它采用了两阶段学习方法，您首先优化RPN（区域提案网络），然后同时预测边界框，类和掩模。<br><br>另一方面，U-Net是一种非常流行的端到端编码器 - 解码器网络，用于语义分割[9]。它最初是发明并首次用于生物医学图像分割，这是我们为Data Science Bowl所做的一项非常类似的任务。竞争中没有银弹，没有任何一个没有邮政或预处理的建筑或建筑设计中的任何小调整都没有得到最高分。我没有机会为这次比赛尝试Mask-RCNN，所以我在U-Net周围进行了实验并学到了很多东西。<br><br>此外，由于我们的主题是语义分割，我将把Mask-RCNN留给其他博客文章进行解释。但是如果你仍然坚持在自己的CV应用程序中尝试它们，这里有两个流行的github存储库，在Tensorflow和PyTorch中实现。 [10,11]<br><br>现在，我们可以继续使用U-Net并深入了解它的细节……<br><br>这是开始的架构：<br><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292433921-5a4506b4-63e9-48d0-adce-6d7e750d5209.png#width=826" alt><br>对于熟悉传统卷积神经网络的人来说，架构的第一部分（表示为DOWN）将是熟悉的。第一部分被调用，或者您可能认为它是编码器部分，您应用卷积块，然后进行maxpool下采样，将输入图像编码为多个不同级别的要素表示。<br><br>网络的第二部分包括上采样和连接，然后是常规卷积操作。 CNN中的上采样可能是一些读者的新概念，但这个想法相当简单：我们正在扩展要素尺寸以使用左边相应的连接块来满足相同的尺寸。您可能会看到灰色和绿色箭头，我们将两个要素图连接在一起。与其他完全卷积分割网络相比，U-Net在这种意义上的主要贡献在于，在网络中进行上采样和深入研究的同时，我们将更高分辨率的特征从下部与上采样特征连接起来，以便更好地定位和学习表示。以下卷积。由于上采样是稀疏操作，因此我们需要从早期阶段获得良好的优先级以更好地表示本地化。在FPN（特征金字塔网络）中也可以看到组合匹配级别的类似想法。 [7]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292475699-4aa5a478-7954-41a5-ab80-9e8a6125d4e4.png#width=765" alt><br>我们可以在下部定义一个操作块作为卷积→下采样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a sample down block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_conv_bn_relu</span><span class="params">(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,  stride=stride, padding=padding, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    ]</span><br><span class="line">self.down1 = nn.Sequential(</span><br><span class="line">    *make_conv_bn_relu(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> ),</span><br><span class="line">    *make_conv_bn_relu(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> ),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convolutions followed by a maxpool</span></span><br><span class="line">down1 = self.down1(x)</span><br><span class="line">out1   = F.max_pool2d(down1, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>类似地，我们可以将一个操作块定义为上采样→连接→卷积。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a sample up block</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_conv_bn_relu</span><span class="params">(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,  stride=stride, padding=padding, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    ]</span><br><span class="line">self.up4 = nn.Sequential(</span><br><span class="line">    *make_conv_bn_relu(<span class="number">128</span>,<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> ),</span><br><span class="line">    *make_conv_bn_relu(<span class="number">64</span>,<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span> )</span><br><span class="line">)</span><br><span class="line">self.final_conv = nn.Conv2d(<span class="number">32</span>, num_classes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># upsample out_last, concatenate with down1 and apply conv operations</span></span><br><span class="line">out   = F.upsample(out_last, scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>)  </span><br><span class="line">out   = torch.cat([down1, out], <span class="number">1</span>)</span><br><span class="line">out   = self.up4(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># final 1x1 conv for predictions</span></span><br><span class="line">final_out = self.final_conv(out)</span><br></pre></td></tr></table></figure></p>
<p>通过仔细检查图形，您可能会注意到输出尺寸（388 x 388）与原始输入（572 x 572）不同。如果您希望获得一致的大小，您可以应用填充卷积来保持跨越级联级别的维度，就像我们在上面的示例代码中所做的那样。<br><br>当提到这样的上采样时，您可能会遇到以下任一项：转置卷积，上卷积，反卷积或上移。包括我自己和PyTorch文档在内的许多人都不喜欢反卷积这个术语，因为在上采样阶段我们实际上正在进行常规的卷积运算，并且没有任何关于它的信息。如果您不熟悉基本的卷积运算及其算术，我会强烈建议您访问此处。 [12]<br><br>我将解释从最简单到更复杂的上采样方法。以下是在PyTorch中对2D张量进行上采样的三种方法：<br><a name="qab0nn"></a></p>
<h4 id="最近邻"><a href="#最近邻" class="headerlink" title="最近邻"></a><a href="#qab0nn"></a>最近邻</h4><p>这是在将张量调整（转换）为更大张量时找到丢失像素值的最简单方法，例如， 2x2到4x4,5x5或6x6。<br><br>让我们使用Numpy逐步实现这个基本的计算机视觉算法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_interpolate</span><span class="params">(A, new_size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Nearest Neighbor Interpolation, Step by Step</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># get sizes</span></span><br><span class="line">    old_size = A.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate row and column ratios</span></span><br><span class="line">    row_ratio, col_ratio = new_size[<span class="number">0</span>]/old_size[<span class="number">0</span>], new_size[<span class="number">1</span>]/old_size[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define new pixel row position i</span></span><br><span class="line">    new_row_positions = np.array(range(new_size[<span class="number">0</span>]))+<span class="number">1</span></span><br><span class="line">    new_col_positions = np.array(range(new_size[<span class="number">1</span>]))+<span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># normalize new row and col positions by ratios</span></span><br><span class="line">    new_row_positions = new_row_positions / row_ratio</span><br><span class="line">    new_col_positions = new_col_positions / col_ratio</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply ceil to normalized new row and col positions</span></span><br><span class="line">    new_row_positions = np.ceil(new_row_positions)</span><br><span class="line">    new_col_positions = np.ceil(new_col_positions)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># find how many times to repeat each element</span></span><br><span class="line">    row_repeats = np.array(list(Counter(new_row_positions).values()))</span><br><span class="line">    col_repeats = np.array(list(Counter(new_col_positions).values()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># perform column-wise interpolation on the columns of the matrix</span></span><br><span class="line">    row_matrix = np.dstack([np.repeat(A[:, i], row_repeats) </span><br><span class="line">                            <span class="keyword">for</span> i <span class="keyword">in</span> range(old_size[<span class="number">1</span>])])[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># perform column-wise interpolation on the columns of the matrix</span></span><br><span class="line">    nrow, ncol = row_matrix.shape</span><br><span class="line">    final_matrix = np.stack([np.repeat(row_matrix[i, :], col_repeats)</span><br><span class="line">                             <span class="keyword">for</span> i <span class="keyword">in</span> range(nrow)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_matrix</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_interpolate</span><span class="params">(A, new_size)</span>:</span></span><br><span class="line">    <span class="string">"""Vectorized Nearest Neighbor Interpolation"""</span></span><br><span class="line"></span><br><span class="line">    old_size = A.shape</span><br><span class="line">    row_ratio, col_ratio = np.array(new_size)/np.array(old_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># row wise interpolation </span></span><br><span class="line">    row_idx = (np.ceil(range(<span class="number">1</span>, <span class="number">1</span> + int(old_size[<span class="number">0</span>]*row_ratio))/row_ratio) - <span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># column wise interpolation</span></span><br><span class="line">    col_idx = (np.ceil(range(<span class="number">1</span>, <span class="number">1</span> + int(old_size[<span class="number">1</span>]*col_ratio))/col_ratio) - <span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    final_matrix = A[:, row_idx][col_idx, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_matrix</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292610278-9d12de49-89e6-454e-a3ed-35ea21ccbf82.png#width=826" alt><br><strong>[PyTorch]</strong> F.upsample(…, mode = “nearest”)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)</span><br><span class="line">&gt;&gt;&gt; input</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1  2</span><br><span class="line">  3  4</span><br><span class="line">[torch.FloatTensor of size (1,1,2,2)]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;nearest&apos;)</span><br><span class="line">&gt;&gt;&gt; m(input)</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1  1  2  2</span><br><span class="line">  1  1  2  2</span><br><span class="line">  3  3  4  4</span><br><span class="line">  3  3  4  4</span><br><span class="line">[torch.FloatTensor of size (1,1,4,4)]</span><br></pre></td></tr></table></figure></p>
<p>双线性插值<br><br>双线性插值算法的计算效率低于最近邻居，但它是一种更精确的近似。 根据距离计算单个像素值作为所有其他值的加权平均值。<br><br>[PyTorch] F.upsample（…，mode =“bilinear”）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)</span><br><span class="line">&gt;&gt;&gt; input</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1  2</span><br><span class="line">  3  4</span><br><span class="line">[torch.FloatTensor of size (1,1,2,2)]</span><br><span class="line">&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;)</span><br><span class="line">&gt;&gt;&gt; m(input)</span><br><span class="line"></span><br><span class="line">(0 ,0 ,.,.) =</span><br><span class="line">  1.0000  1.2500  1.7500  2.0000</span><br><span class="line">  1.5000  1.7500  2.2500  2.5000</span><br><span class="line">  2.5000  2.7500  3.2500  3.5000</span><br><span class="line">  3.0000  3.2500  3.7500  4.0000</span><br><span class="line">[torch.FloatTensor of size (1,1,4,4)]</span><br></pre></td></tr></table></figure></p>
<p>转置卷积<br><br>在转置卷积中，我们通过反向传播学习权重。 在论文中，我遇到了针对各种情况的所有这些上采样方法，并且在实践中，您可能会更改您的体系结构并尝试所有这些以查看哪种方法最适合您自己的问题。 我个人更喜欢转置卷积，因为我们对它有更多的控制权，但你也可以选择双线性插值或最近邻居。<br><strong>[PyTorch]</strong> nn.ConvTranspose2D(…, stride=…, padding=…)<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292702247-9ed06c87-3b1d-49f0-9794-88a906ebb639.png#width=826" alt><br>如果我们回到最初的案例，数据科学碗，在竞争中使用香草U-Net方法的主要缺点是重叠核。 如上图所示，如果您创建一个二元蒙版并将其用作目标，U-Net肯定会预测类似于此的东西，并且您将拥有多个核的组合掩模，这些核重叠或彼此非常接近。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292738956-2056a9a8-93c3-46b1-aca5-6dfc1da18863.png#width=730" alt><br>关于重叠实例问题，U-Net论文的作者使用加权交叉熵来强调学习细胞的边界。 此方法帮助他们分离重叠的实例。 基本的想法是更多地加权边界，并推动网络在近距离实例之间找到学习差距。[9]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292766571-1bab28fe-4b7d-4e4d-b158-f641816a25cd.png#width=780" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292777101-f5f1af70-43dc-40e4-a20c-6bb04d662c03.png#width=826" alt><br>这种问题的另一种解决方案是许多竞争者使用的方法，包括获胜的解决方案，是将二进制掩码转换为多类目标。 U-Net的优点在于，您可以构建网络以根据需要输出任意数量的通道，并通过在最后一层使用1x1卷积来表示任何通道中的任何类。<br>引自Data Science Bowl获奖解决方案：</p>
<blockquote>
<p>用于具有S形激活的网络的2通道掩模，即（掩模 - 边界，边界）或用于具有softmax激活的网络的3通道掩模，即（掩模 - 边界，边界，1  - 掩模 - 边界）<br>2通道全面罩即（面罩，边框）</p>
</blockquote>
<p>在进行这些预测之后，诸如分水岭的经典图像处理算法可以用于后处理以进一步分割单个核。[14]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545292929262-dda5f714-55e9-4a52-b5ef-7402a732ea29.png#width=320" alt><br>这是第一次正式的计算机视觉竞赛，我有勇气参加Kaggle，它是一个数据科学碗。虽然我只在前20％（这被认为是平均分数）完成了比赛，但我感到很高兴参加数据科学碗并学习如果我实际上没有参与的话我可能永远不会学到的东西并试着靠自己。积极学习远比观看或阅读来自在线资源的类似方法更富有成效。<br><br>作为一名刚刚开始用Fast.ai开始练习数月的深度学习练习者，这对我来说是一个重要的一步，也是我永无止境的旅程，在获得经验方面非常有价值。所以，对于那些在你之前从未见过或已经解决的挑战感到暗示的人，我强烈建议你专门去处理这些类型的挑战，以便感受学习以前你不知道的东西的乐趣。<br><br>我在本次比赛中学到的另一个有价值的教训是，在计算机视觉中（这也适用于NLP）竞赛，通过眼睛检查每一个预测非常重要，看看哪些有效，哪些无效。如果您的数据足够小，您应该去检查每个输出。如果出现问题，这将允许您进一步提出更好的想法，甚至调试代码。<br><a name="79zwnl"></a></p>
<h4 id="转移学习及其他"><a href="#转移学习及其他" class="headerlink" title="转移学习及其他"></a><a href="#79zwnl"></a>转移学习及其他</h4><p>到目前为止，我们已经定义了vanilla U-Net的构建块，并提到了我们如何操纵目标来解决实例分割问题。现在我们可以进一步讨论这些类型的编码器 - 解码器网络的灵活性。通过灵活性，我的意思是你拥有它的自由以及你可以对它进行设计的创造力。<br><br>任何在某些时候进行深度学习的人都会转移学习，因为这是一个非常强大的想法。简而言之，转移学习是使用预训练网络的概念，该网络在许多样本上进行训练，以完成我们所面临的类似任务，但缺少相同数量的数据。即使有足够的数据传输，学习也可以在一定程度上提升性能，不仅适用于计算机视觉任务，也适用于NLP。<br><br>迁移学习也被证明是U-Net类似架构的强大技术。我们之前已经定义了U-Net的两个主要组件;向上和向上。我们这次将这些部分重新编号为编码器和解码器。编码器部分基本上接受输入并将其编码在低维特征空间中，该特征空间表示我们在较低维度中的输入。现在想象用你最喜欢的ImageNet获胜者替换这个编码器; VGG，ResNet，Inception，NasNet，……你想要的。这些网络经过精心设计，可以做一个常见的事情：以最佳方式编码自然图像进行分类，ImageNet上的预训练重量等待您在线抓取它们。<br><br>那么为什么不使用这些架构之一作为我们的编码器并以与原始U-Net相同的方式构建解码器，但更好的是，使用类固醇。<br><br>TernausNet是Kaggle Carvana挑战赛的赢家架构，它使用与VGG11相同的传输学习理念作为编码器。 [15,16]<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545293023504-202250cf-02aa-47c8-bc29-5661cddacdad.png#width=832" alt><br><a name="4rcezx"></a></p>
<h4 id="Fast-ai：动态U-Net"><a href="#Fast-ai：动态U-Net" class="headerlink" title="Fast.ai：动态U-Net"></a><a href="#4rcezx"></a>Fast.ai：动态U-Net</h4><p>受到TernausNet论文和许多其他优秀资源的启发，我想概括为U-Net架构使用预训练或自定义编码器的想法。 所以，我想出了一个通用的架构：动态Unet。<br><br>Dynamic Unet是这个想法的一个实现，它通过为您完成所有计算和匹配，自动为任何给定的编码器创建解码器部分。 编码器既可以是现成的预训练网络，也可以是您自己定义的任何自定义架构。<br><br>它是用PyTorch编写的，目前在Fast.ai库中。 您可以参考此笔记本以查看其中的操作或查看来源。 Dynamic Unet的主要目标是节省实践者的时间，并允许使用尽可能少的代码更容易地使用不同的编码器进行实验。<br><br>在第2部分中，我将解释体积数据的3D编码器解码器模型，例如MRI扫描，并给出我一直在研究的真实世界的例子。</p>
<p><strong>References</strong></p>
<p>[5] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01497</a><br>[6] Mask R-CNN: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">https://arxiv.org/abs/1703.06870</a><br>[7] Feature Pyramid Networks for Object Detection: <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a><br>[8] Fully Convolutional Networks for Semantic Segmentation: <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf</a><br>[9] U-net: Convolutional Networks for Biomedical Image Segmentation: <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">https://arxiv.org/abs/1505.04597</a><br>[10] Tensorflow Mask-RCNN: <a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="noopener">https://github.com/matterport/Mask_RCNN</a><br>[11] Pytorch Mask-RCNN:_ _<a href="https://github.com/multimodallearning/pytorch-mask-rcnn" target="_blank" rel="noopener">https://github.com/multimodallearning/pytorch-mask-rcnn</a><br>[12] Convolution Arithmetic: <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic</a><br>[13] Data Science Bowl 2018 Winning Solution, ods-ai: <a href="https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741" target="_blank" rel="noopener">https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741</a><br>[14] Watershed Algorithm <a href="https://docs.opencv.org/3.3.1/d3/db4/tutorial_py_watershed.html" target="_blank" rel="noopener">https://docs.opencv.org/3.3.1/d3/db4/tutorial_py_watershed.html</a><br>[15] Carvana Image Masking Challenge: <a href="https://www.kaggle.com/c/carvana-image-masking-challenge" target="_blank" rel="noopener">https://www.kaggle.com/c/carvana-image-masking-challenge</a><br>[16] TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation: <a href="https://arxiv.org/abs/1801.05746" target="_blank" rel="noopener">https://arxiv.org/abs/1801.05746</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/20/yuque/语义分割 -  U-Net（第1部分）/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/">Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict</a>
    </h1>
  

        
        <a href="/2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/" class="archive-article-date">
  	<time datetime="2018-12-20T07:28:04.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>分享，重用和分解模型复杂性的有效方法</p>
<p>更新到 Pytorch 4.1版本</p>
<p>你可以找到代码 <a href="https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict" target="_blank" rel="noopener">here</a></p>
<p>Pytorch是一个开源的深度学习框架，提供了创建ML模型的智能方法。 即使文档制作完好，我仍然发现大多数人仍然能够编写错误的而不是有组织的PyTorch代码。</p>
<p>今天，我们将看到如何使用PyTorch的三个主要构建块：Module，Sequential和ModuleList。 我们将从一个例子开始，迭代地我们将使它变得更好。<br><br><br><br>所有这四个类都包含在torch.nn中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Module</span></span><br><span class="line"><span class="comment"># nn.Sequential</span></span><br><span class="line"><span class="comment"># nn.Module</span></span><br></pre></td></tr></table></figure>
<p><a name="puggyn"></a></p>
<h2 id="模块：主要构建块"><a href="#模块：主要构建块" class="headerlink" title="模块：主要构建块"></a><a href="#puggyn"></a>模块：主要构建块</h2><p>Module是主要的构建块，它定义了所有神经网络的基类，你必须将它子类化。<br><br><br><br>让我们创建一个经典的CNN分类器作为示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.sigmoid(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (fc1): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这是一个非常简单的分类器，其编码部分使用两层3x3 convs + batchnorm + relu和一个带有两个线性层的解码部分。 如果您不是PyTorch的新手，您可能以前看过这种类型的编码，但有两个问题。<br><br><br><br>如果我们想要添加一个图层，我们必须在<strong>init</strong>和forward函数中再次编写大量代码。 此外，如果我们有一些我们想要在另一个模型中使用的公共块，例如 3x3 conv + batchnorm + relu，我们必须再写一次。</p>
<p><a name="kpmyrn"></a></p>
<h2 id="Sequential：堆叠和合并图层"><a href="#Sequential：堆叠和合并图层" class="headerlink" title="Sequential：堆叠和合并图层"></a><a href="#kpmyrn"></a>Sequential：堆叠和合并图层</h2><p><a name="di8wcs"></a></p>
<h2 id><a href="#" class="headerlink" title></a><a href="#di8wcs"></a></h2><p>Sequential是一个模块的容器，可以堆叠在一起并同时运行。<br><br><br><br>你可以注意到我们必须将自己的一切存储起来。 我们可以使用Sequential来改进我们的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_block1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv_block2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv_block1(x)</span><br><span class="line">        x = self.conv_block2(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (conv_block1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): BatchNorm2d(<span class="number">32</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (conv_block2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): Sigmoid()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>你觉得好多了？<br><br><br><br>您是否注意到conv_block1和conv_block2看起来几乎相同？ 我们可以创建一个重新生成nn.Sequential的函数来简化代码！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_f, out_f, *args, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_f, out_f, *args, **kwargs),</span><br><span class="line">        nn.BatchNorm2d(out_f),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>然后我们可以在我们的模块中调用此函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_block1 = conv_block(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv_block2 = conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv_block1(x)</span><br><span class="line">        x = self.conv_block2(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (conv_block1): Sequential(</span><br><span class="line">    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    (2): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (conv_block2): Sequential(</span><br><span class="line">    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    (2): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Even cleaner! Still <code>conv_block1</code> and <code>conv_block2</code> are almost the same! We can merge them using <code>nn.Sequential</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            conv_block(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>self.encoder</code> now holds booth <code>conv_block</code>. We have decoupled logic for our model and make it easier to read and reuse. Our <code>conv_block</code> function can be imported and used in another model.</p>
<p><a name="h5bzub"></a></p>
<h2 id="Dynamic-Sequential-create-multiple-layers-at-once"><a href="#Dynamic-Sequential-create-multiple-layers-at-once" class="headerlink" title="Dynamic Sequential: create multiple layers at once"></a><a href="#h5bzub"></a>Dynamic Sequential: create multiple layers at once</h2><p>What if we can to add a new layers in <code>self.encoder</code>, hardcoded them is not convinient:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.encoder = nn.Sequential(</span><br><span class="line">            conv_block(in_c, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            conv_block(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line"></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>Would it be nice if we can define the sizes as an array and automatically create all the layers without writing each one of them? Fortunately we can create an array and pass it to <code>Sequential</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">        </span><br><span class="line">        conv_blocks = [conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.enc_sizes, self.enc_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(*conv_blocks)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Let’s break it down. We created an array <code>self.enc_sizes</code> that holds the sizes of our encoder. Then we create an array <code>conv_blocks</code> by iterating the sizes. Since we have to give booth a in size and an outsize for each layer we <code>zip</code>ed the size’array with itself by shifting it by one.</p>
<p>Just to be clear, take a look at the following example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sizes = [<span class="number">1</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> in_f,out_f <span class="keyword">in</span> zip(sizes, sizes[<span class="number">1</span>:]):</span><br><span class="line">    print(in_f,out_f)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 32</span><br><span class="line">32 64</span><br></pre></td></tr></table></figure>
<p>Then, since <code>Sequential</code> does not accept a list, we decompose it by using the <code>*</code> operator.</p>
<p>Tada! Now if we just want to add a size, we can easily add a new number to the list. It is a common practice to make the size a parameter.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        </span><br><span class="line">        conv_blokcs = [conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.enc_sizes, self.enc_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(*conv_blokcs)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, n_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>, <span class="number">128</span>], <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (2): Sequential(</span><br><span class="line">      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features=1024, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>We can do the same for the decoder part</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dec_block</span><span class="params">(in_f, out_f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Linear(in_f, out_f),</span><br><span class="line">        nn.Sigmoid()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, dec_sizes,  n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        self.dec_sizes = [<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, *dec_sizes]</span><br><span class="line"></span><br><span class="line">        conv_blokcs = [conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.enc_sizes, self.enc_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(*conv_blokcs)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        dec_blocks = [dec_block(in_f, out_f) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(self.dec_sizes, self.dec_sizes[<span class="number">1</span>:])]</span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(*dec_blocks)</span><br><span class="line">        </span><br><span class="line">        self.last = nn.Linear(self.dec_sizes[<span class="number">-1</span>], n_classes)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>], [<span class="number">1024</span>, <span class="number">512</span>], <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Sequential(</span><br><span class="line">    (0): Sequential(</span><br><span class="line">      (0): Linear(in_features=25088, out_features=1024, bias=True)</span><br><span class="line">      (1): Sigmoid()</span><br><span class="line">    )</span><br><span class="line">    (1): Sequential(</span><br><span class="line">      (0): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">      (1): Sigmoid()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (last): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>We followed the same pattern, we create a new block for the decoding part, linear + sigmoid, and we pass an array with the sizes. We had to add a <code>self.last</code> since we do not want to activate the output</p>
<p>Now, we can even break down our model in two! Encoder + Decoder</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_sizes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(enc_sizes, enc_sizes[<span class="number">1</span>:])])</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self.conv_blokcs(x)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dec_sizes, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(dec_sizes, dec_sizes[<span class="number">1</span>:])])</span><br><span class="line">        self.last = nn.Linear(dec_sizes[<span class="number">-1</span>], n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.dec_blocks()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, dec_sizes,  n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        self.dec_sizes = [<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, *dec_sizes]</span><br><span class="line"></span><br><span class="line">        self.encoder = MyEncoder(self.enc_sizes)</span><br><span class="line">        </span><br><span class="line">        self.decoder = MyDecoder(dec_sizes, n_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.flatten(<span class="number">1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>], [<span class="number">1024</span>, <span class="number">512</span>], <span class="number">10</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): MyEncoder(</span><br><span class="line">    (conv_blokcs): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): ReLU()</span><br><span class="line">      )</span><br><span class="line">      (1): Sequential(</span><br><span class="line">        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): ReLU()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): MyDecoder(</span><br><span class="line">    (dec_blocks): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">        (1): Sigmoid()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (last): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Be aware that <code>MyEncoder</code> and <code>MyDecoder</code> could also be functions that returns a <code>nn.Sequential</code>. I prefer to use the first pattern for models and the second for building blocks.</p>
<p>By diving our module into submodules it is easier to <strong>share</strong> the code, <strong>debug</strong> it and <strong>test</strong> it.</p>
<p><a name="8xg9py"></a></p>
<h2 id="ModuleList-when-we-need-to-iterate"><a href="#ModuleList-when-we-need-to-iterate" class="headerlink" title="ModuleList : when we need to iterate"></a><a href="#8xg9py"></a>ModuleList : when we need to iterate</h2><p><code>ModuleList</code> allows you to store <code>Module</code> as a list. It can be useful when you need to iterate through layer and store/use some information, like in U-net.</p>
<p>The main difference between <code>Sequential</code> is that <code>ModuleList</code> have not a <code>forward</code> method so the inner layers are not connected. Assuming we need each output of each layer in the decoder, we can store it by:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([nn.Linear(in_f, out_f) <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(sizes, sizes[<span class="number">1</span>:])])</span><br><span class="line">        self.trace = []</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">            self.trace.append(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = MyModule([<span class="number">1</span>, <span class="number">16</span>, <span class="number">32</span>])</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model(torch.rand((<span class="number">4</span>,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">[print(trace.shape) <span class="keyword">for</span> trace <span class="keyword">in</span> model.trace]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 16])</span><br><span class="line">torch.Size([4, 32])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[None, None]</span><br></pre></td></tr></table></figure>
<p><a name="f7epth"></a></p>
<h2 id="ModuleDict-when-we-need-to-choose"><a href="#ModuleDict-when-we-need-to-choose" class="headerlink" title="ModuleDict: when we need to choose"></a><a href="#f7epth"></a>ModuleDict: when we need to choose</h2><p>What if we want to switch to <code>LearkyRelu</code> in our <code>conv_block</code>? We can use <code>ModuleDict</code> to create a dictionary of <code>Module</code> and dynamically switch <code>Module</code> when we want</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_f, out_f, activation=<span class="string">'relu'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">    </span><br><span class="line">    activations = nn.ModuleDict([</span><br><span class="line">                [<span class="string">'lrelu'</span>, nn.LeakyReLU()],</span><br><span class="line">                [<span class="string">'relu'</span>, nn.ReLU()]</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_f, out_f, *args, **kwargs),</span><br><span class="line">        nn.BatchNorm2d(out_f),</span><br><span class="line">        activations[activation]</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(conv_block(<span class="number">1</span>, <span class="number">32</span>,<span class="string">'lrelu'</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">print(conv_block(<span class="number">1</span>, <span class="number">32</span>,<span class="string">'relu'</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (2): LeakyReLU(negative_slope=0.01)</span><br><span class="line">)</span><br><span class="line">Sequential(</span><br><span class="line">  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (2): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a name="c51cxn"></a></p>
<h2 id="Final-implementation"><a href="#Final-implementation" class="headerlink" title="Final implementation"></a><a href="#c51cxn"></a>Final implementation</h2><p>Let’s wrap it up everything!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_f, out_f, activation=<span class="string">'relu'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">    activations = nn.ModuleDict([</span><br><span class="line">                [<span class="string">'lrelu'</span>, nn.LeakyReLU()],</span><br><span class="line">                [<span class="string">'relu'</span>, nn.ReLU()]</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_f, out_f, *args, **kwargs),</span><br><span class="line">        nn.BatchNorm2d(out_f),</span><br><span class="line">        activations[activation]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dec_block</span><span class="params">(in_f, out_f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Linear(in_f, out_f),</span><br><span class="line">        nn.Sigmoid()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_sizes, *args, **kwargs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, *args, **kwargs) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(enc_sizes, enc_sizes[<span class="number">1</span>:])])</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self.conv_blokcs(x)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dec_sizes, n_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) </span><br><span class="line">                       <span class="keyword">for</span> in_f, out_f <span class="keyword">in</span> zip(dec_sizes, dec_sizes[<span class="number">1</span>:])])</span><br><span class="line">        self.last = nn.Linear(dec_sizes[<span class="number">-1</span>], n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.dec_blocks()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNNClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, enc_sizes, dec_sizes,  n_classes, activation=<span class="string">'relu'</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.enc_sizes = [in_c, *enc_sizes]</span><br><span class="line">        self.dec_sizes = [<span class="number">32</span> * <span class="number">28</span> * <span class="number">28</span>, *dec_sizes]</span><br><span class="line"></span><br><span class="line">        self.encoder = MyEncoder(self.enc_sizes, activation=activation)</span><br><span class="line">        </span><br><span class="line">        self.decoder = MyDecoder(dec_sizes, n_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        </span><br><span class="line">        x = x.flatten(<span class="number">1</span>) <span class="comment"># flat</span></span><br><span class="line">        </span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = MyCNNClassifier(<span class="number">1</span>, [<span class="number">32</span>,<span class="number">64</span>], [<span class="number">1024</span>, <span class="number">512</span>], <span class="number">10</span>, activation=<span class="string">'lrelu'</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MyCNNClassifier(</span><br><span class="line">  (encoder): MyEncoder(</span><br><span class="line">    (conv_blokcs): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): LeakyReLU(negative_slope=0.01)</span><br><span class="line">      )</span><br><span class="line">      (1): Sequential(</span><br><span class="line">        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): LeakyReLU(negative_slope=0.01)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): MyDecoder(</span><br><span class="line">    (dec_blocks): Sequential(</span><br><span class="line">      (0): Sequential(</span><br><span class="line">        (0): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">        (1): Sigmoid()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (last): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a name="8gyuvn"></a></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><a href="#8gyuvn"></a>Conclusion</h2><p>So, in summary.</p>
<ul>
<li><p>Use <code>Module</code> when you have a big block compose of multiple smaller blocks</p>
</li>
<li><p>Use <code>Sequential</code> when you want to create a small block from layers</p>
</li>
<li><p>Use <code>ModuleList</code> when you need to iterate through some layers or building blocks and do something</p>
</li>
<li><p>Use <code>ModuleDict</code> when you need to parametise some blocks of your model, for example an activation function</p>
</li>
</ul>
<p>That’s all folks!</p>
<p>Thank you for reading</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/使用新的fastai库实现世界一流的结果" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/yuque/使用新的fastai库实现世界一流的结果/">使用新的fastai库实现世界一流的结果</a>
    </h1>
  

        
        <a href="/2018/12/20/yuque/使用新的fastai库实现世界一流的结果/" class="archive-article-date">
  	<time datetime="2018-12-20T06:20:52.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://towardsdatascience.com/transfer-learning-using-the-fastai-library-d686b238213e" target="_blank" rel="noopener">链接</a></p>
<p><a name="hmoufh"></a></p>
<h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a><a href="#hmoufh"></a>动机</h3><p>我目前正在做一个fast.ai Live MOOC，名为《Practical Deep learning for Coders》，将于2019年1月在fast.ai网站上公开发布。 以下代码基于该课程的第1课。 我将使用位于Pytorch 1.0顶部的fastai V1库。 fastai库提供了许多有用的功能，使我们能够快速，轻松地构建神经网络并训练我们的模型。 我正在撰写此博客，作为在数据集上试验课程示例的一部分，该数据集在结构和复杂性上有所不同，并表明使用fastai库是多么容易。<br>在下面的例子中，您将看到在<a href="https://plantvillage.psu.edu/" target="_blank" rel="noopener">PlantVintage</a>数据集上进行转移学习并获得世界级结果是多么荒谬。 PlantVintage数据包含植物叶片的图像，其中包括通常在作物上发现的38种疾病类别，是来自斯坦福大学背景图像开放数据集的一个背景类别–DAGS。 我从这个<a href="https://github.com/MarkoArsenovic/DeepLearning_PlantDiseases" target="_blank" rel="noopener">Github Repo</a>上给出的链接下载了数据。 我对这个例子特别感兴趣，因为在我写这篇博客的时候，我为一个帮助农民发展业务的组织工作，提供产品和技术解决方案，以便更好地管理农场。 让我们开始吧！<br>PS：这个博客也在我的<a href="https://github.com/aayushmnit" target="_blank" rel="noopener">GitHub</a>个人资料中作为jupyter笔记本发布。<br><a name="5dzgih"></a></p>
<h3 id="导入快速AI库"><a href="#导入快速AI库" class="headerlink" title="导入快速AI库"></a><a href="#5dzgih"></a>导入快速AI库</h3><p>让我们导入fastai库并将我们的batch_size参数定义为64.通常，图像数据库很大，所以我们需要使用批量将这些图像提供给GPU，批量大小64意味着我们将一次提供64个图像来更新深度参数 学习模式。 如果由于GPU RAM较小而导致内存不足，则可以将批量大小减小到32或16。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br><span class="line">bs =<span class="number">64</span></span><br></pre></td></tr></table></figure></p>
<p><a name="vr3dmp"></a></p>
<h3 id="看数据"><a href="#看数据" class="headerlink" title="看数据"></a><a href="#vr3dmp"></a>看数据</h3><p>我们处理问题时首先要做的是查看数据。 在我们弄清楚如何解决问题之前，我们总是需要很好地理解问题是什么以及数据是什么样的。 查看数据意味着了解数据目录的结构，标签是什么以及一些示例图像是什么样的。 我们的数据已经在train和validation文件夹中分割，在每个子目录中，我们的文件夹名称代表该子文件夹中存在的所有图像的类名。 幸运的是，fastai库有一个方便的功能，<a href="https://docs.fast.ai/vision.data#ImageDataBunch.from_folder" target="_blank" rel="noopener">ImageDataBunch.from_folder</a>自动从文件夹名称中获取标签名称。 fastai库提供了很棒的文档来浏览它们的库函数，并提供有关如何使用它们的实例。 加载数据后，我们还可以使用.normalize到ImageNet参数来规范化数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Declaring path of dataset</span></span><br><span class="line">path_img = Path(<span class="string">'/home/jupyter/fastai_v3_experimentation/data/PlantVillage/'</span>)</span><br><span class="line"><span class="comment">## Loading data </span></span><br><span class="line">data = ImageDataBunch.from_folder(path=path_img, train=<span class="string">'train'</span>, valid=<span class="string">'valid'</span>, ds_tfms=get_transforms(),size=<span class="number">224</span>, bs=bs, check_ext=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">## Normalizing data based on Image net parameters</span></span><br><span class="line">data.normalize(imagenet_stats)</span><br></pre></td></tr></table></figure></p>
<p>要查看随机的图像样本，我们可以使用.show_batch（）函数ImageDataBunch类。 正如我们在下面所看到的，我们有一些不同作物上的疾病病例加上来自DAGS数据集的一些背景噪声图像，这些图像将作为噪声。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.show_batch(rows=<span class="number">3</span>, figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287666783-17e163b9-fba6-4ab9-9952-1168913169b0.png#width=826" alt><br>让我们打印数据库中存在的所有数据类。 总的来说，我们在动机部分中提到了39个课程中的图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(data.classes)</span><br><span class="line">len(data.classes),data.c</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287718206-c67e4447-9a83-4032-86c8-5d298c775b72.png#width=826" alt><br><a name="hzustc"></a></p>
<h3 id="使用预先训练的模型转移学习：ResNet-50"><a href="#使用预先训练的模型转移学习：ResNet-50" class="headerlink" title="使用预先训练的模型转移学习：ResNet 50"></a><a href="#hzustc"></a>使用预先训练的模型转移学习：ResNet 50</h3><p>现在我们将开始训练我们的模型。 我们将使用卷积神经网络骨干ResNet 50和具有单个隐藏层的完全连接头作为分类器。 如果您想了解所有架构细节，也可以阅读ResNet论文。 要创建转移学习模型，我们需要使用Learner类中的函数create_cnn，并从模型类中提供预先训练的模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## To create a ResNET 50 with pretrained weights</span></span><br><span class="line">learn = create_cnn(data, models.resnet50, metrics=error_rate)</span><br></pre></td></tr></table></figure></p>
<p>由create_cnn函数创建的ResNet50模型具有冻结的初始层，我们将学习最后完全连接的层的权重。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit_one_cycle(<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287818426-ef1da8b8-26e2-4bee-835a-abfa2386fa31.png#width=584" alt><br>正如我们在上面看到的那样，只使用默认设置运行五个周期，我们对这个细粒度分类任务的准确度在验证数据集上约为99.64％。 让我们保存模型，因为我们稍后会对其进行微调。 如果你想知道这个结果有多好，它已经超过了这个<a href="https://github.com/MarkoArsenovic/DeepLearning_PlantDiseases" target="_blank" rel="noopener">Github Page</a>的96.53％的浅层学习（仅培训最后一层）基准。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.save(<span class="string">'plant_vintage_stage1'</span>)</span><br></pre></td></tr></table></figure></p>
<p>FastAI库还提供了更快地探索结果的功能，并查找我们的模型是否正在学习应该学习的内容。 我们将首先看到模型最混淆的类别。 我们将尝试使用ClassificationInterpretation类来查看模型预测的是否合理。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassificationInterpretation.from_learner(learn)</span><br><span class="line">interp.plot_top_losses(<span class="number">4</span>, figsize=(<span class="number">20</span>,<span class="number">25</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545287958131-9082d640-9b93-482c-8ed4-7becd11c97b9.png#width=826" alt><br>在这种情况下，该模型在从玉米植株上的灰色叶斑病和番茄叶片中的早/晚疫病检测北叶枯病方面变得混乱，其在视觉上看起来非常相似。 这是我们的分类器正常工作的指示器。 此外，当我们绘制混淆矩阵时，我们可以看到大多数事物都被正确分类，并且它几乎是一个接近完美的模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.plot_confusion_matrix(figsize=(<span class="number">20</span>,<span class="number">20</span>), dpi=<span class="number">60</span>)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545288034116-dbb5f685-7a96-4031-8b2b-e54abae02d81.png#width=826" alt><br>所以到目前为止，我们只训练了最后的分类层，但是如果我们想要优化早期的层也会如此。 在迁移学习中，应谨慎调整初始图层，学习率应保持在较低水平。 FastAI库提供了一个功能，可以查看要训练的理想学习速率，让我们绘制它。 lr_find函数以多学习速率运行数据子集的模型，以确定哪种学习速率最佳。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.lr_find()</span><br><span class="line">learn.recorder.plot()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545288100487-9ddefc18-b8fb-4fe1-8232-8cc43139a730.png#width=447" alt><br>看起来我们应该保持低于10e-4的学习率。 对于网络中的不同层，我们可以使用切片函数来对数分布10e-6到10e-4之间的学习速率。 保持初始层的最低学习速率，并为后续层增加它。 让我们解冻所有层，以便我们可以使用unfreeze()函数训练整个模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.unfreeze()</span><br><span class="line">learn.fit_one_cycle(<span class="number">2</span>, max_lr=slice(<span class="number">1e-7</span>,<span class="number">1e-5</span>))</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545288161256-736a1108-3167-4d5f-be85-2ab6317a60c3.png#width=528" alt><br>正如我们通过训练所有层次所看到的，我们将准确度提高到99.7％，这与使用Inception-v3模型的Github基准测试中的99.76％相当。<br><a name="zv6xnx"></a></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><a href="#zv6xnx"></a>结论</h3><p>Fast.ai是Jeremy Howard和他的团队的一项出色的倡议，我相信fastai库可以通过使构建深度学习模型变得非常简单，真正实现将深度学习民主化的动机。<br><br>我希望你喜欢阅读，并随意使用我的代码为你的目的尝试。 此外，如果对代码或博客文章有任何反馈，请随时联系LinkedIn或发送电子邮件至<a href="mailto:aayushmnit@gmail.com" target="_blank" rel="noopener">aayushmnit@gmail.com</a>。<br><br>PS：如果你喜欢这些内容，请留下评论或鼓掌，并希望我更频繁地写这样的博客。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/20/yuque/使用新的fastai库实现世界一流的结果/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/XGBoost不是黑魔法" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/yuque/XGBoost不是黑魔法/">XGBoost不是黑魔法</a>
    </h1>
  

        
        <a href="/2018/12/20/yuque/XGBoost不是黑魔法/" class="archive-article-date">
  	<time datetime="2018-12-20T05:48:34.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://towardsdatascience.com/xgboost-is-not-black-magic-56ca013144b4" target="_blank" rel="noopener">链接</a></p>
<blockquote>
<p>而不是对缺失值进行估算并不总是正确的选择</p>
</blockquote>
<p><br>现在很容易在数据科学任务中获得不错的结果：只需要对流程有一个大致的了解，Python的基本知识和10分钟的时间来实例化XGBoost并适应模型。好的，如果这是你的第一次，那么你可能会花几分钟通过pip收集所需的包，但就是这样。这种方法的唯一问题是它运作得很好🤷🏻♂️：几年前我在大学竞赛中排名前5位，只是通过一些基本的特征工程将数据集提供给XGBoost，表现优于团队非常复杂的架构和数据管道。 XGBoost最酷的特征之一就是它如何处理缺失值：决定每个样本，这是最好的方法来判断它们。对于我在过去几个月中遇到的许多项目和数据集，此功能非常有用;为了更加值得以我的名义撰写的数据科学家的头衔，我决定深入挖掘，花几个小时阅读原始论文，试图了解XGBoost究竟是什么以及它如何处理它以某种神奇的方式缺少价值。<br><a name="9864342e"></a></p>
<h4 id="从决策树到XGBoost"><a href="#从决策树到XGBoost" class="headerlink" title="从决策树到XGBoost"></a><a href="#7vdtxk"></a>从决策树到XGBoost</h4><p>决策树可能是机器学习中最简单的算法：树的每个节点都是对特征的测试，每个分支代表测试的结果; leaves包含模型的输出，无论是离散标签还是实数。 决策树可能被描述为一个功能：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285357509-9eb19212-1f84-4151-925f-695f322ba1a9.png#align=left&amp;display=inline&amp;height=64&amp;originHeight=78&amp;originWidth=1000&amp;status=done&amp;width=826" alt><br>函数f根据从根到叶子的路径，根据树结构T分配m大小的样本x所遵循的权重w。<br>现在想象一下，不只有一棵决策树而且还有K; 最终产生的输出不再是与叶子相关的权重，而是与每棵树产生的叶子相关的权重之和。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285385079-d2797243-1542-4e33-8215-bde9adc10d39.png#align=left&amp;display=inline&amp;height=104&amp;originHeight=126&amp;originWidth=1000&amp;status=done&amp;width=826" alt></p>
<p>这些结构不是固定的，并且与网络结构不变的经典梯度下降框架中发生的不同，并且在每个步骤更新权重时，在每次迭代时添加新函数（树）以改善模型的性能。 为了避免过度拟合和/或非常复杂的结构，误差由两部分组成：第一部分对在第k次迭代中获得的模型的优度进行评分，第二部分在相关权重的大小中惩罚复杂性。 叶子和发达的树木的深度和结构。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285422201-bef8962d-373e-4670-8789-f8fffb46cbc2.png#align=left&amp;display=inline&amp;height=126&amp;originHeight=152&amp;originWidth=1000&amp;status=done&amp;width=826" alt><br>然后使用二阶梯度统计简化该目标函数，并且 - 不输入太多细节 - 可以直接用于以封闭形式计算与固定树结构相关联的最佳叶子权重。 权重可以直接与错误相关联，因此与所使用的固定结构的优点相关联（3）。<br>训练XGBoost是一个迭代过程，它在每个步骤计算第k个树的最佳可能分割，该第k个树枚举路径中该点仍然可用的所有可能结构。 所有可能的分裂的详尽列举非常适合本文的范围，但在实践中是不可行的，并且它被一个近似版本所取代，该版本不会尝试所有可能的分裂，而是根据百分位数列举相关的分裂。 每个功能分布。<br><a name="3ca234a7"></a></p>
<h3 id="XGBoost和缺失值：魔术发生的地方"><a href="#XGBoost和缺失值：魔术发生的地方" class="headerlink" title="XGBoost和缺失值：魔术发生的地方"></a><a href="#e5q4ag"></a>XGBoost和缺失值：魔术发生的地方</h3><p>一旦树结构被训练，就不难考虑测试集中是否存在缺失值：它足以将默认方向附加到每个决策节点。如果缺少样本的特征并且决策节点在该特征上分裂，则路径采用分支的默认方向并且路径继续。但是为每个分支分配默认方向更复杂，这可能是本文中最有趣的部分。<br>已经解释的拆分查找算法可以稍微调整一下，不仅返回每一步的最佳拆分，而且还返回分配给新插入的决策节点的默认方向。给定一个特征集I，枚举所有可能的分割，但是现在相应的丢失不会被计算一次而是两次，每个默认方向一次丢失该特征的缺失值。两者中最好的是根据特征m的值j进行分割时分配的最佳默认方向。最佳分割仍然是最大化计算分数的分割，但现在我们已经为其附加了默认方向。<br>这种算法被称为稀疏感知的分裂发现，它是XGBoost背后的许多魔力所在。最后不要太复杂。稀疏性感知方法仅保证在已经遍历的分裂的情况下平均采用默认方向导致最佳可能结果，并不保证已经遍历的分裂（可能通过采用默认方向来解决）是最好的考虑整个样本。如果样本中缺失值的百分比增加，则内置策略的性能可能会恶化很多。</p>
<blockquote>
<p><em>好的，默认方向是最佳选择，只要它到达当前位置，但考虑到当前样本的所有特征，无法保证当前位置是最佳情况。</em></p>
</blockquote>
<p>克服此限制意味着处理同时考虑其所有特征的样本，并直接处理同一实现中可能同时存在多个缺失值。<br><a name="14ec9de9"></a></p>
<h3 id="改变缺失值并改善表现"><a href="#改变缺失值并改善表现" class="headerlink" title="改变缺失值并改善表现"></a><a href="#z5afml"></a>改变缺失值并改善表现</h3><p>为了击败XGBoost内置策略，我们必须同时考虑样本的所有功能，并以某种方式处理可能存在的缺失值。 这种方法的一个很好的例子是K-Nearest Neighbors（KNN），它具有ad-hoc距离度量以正确处理缺失值。 一般而言，KNN是众所周知的算法，其将K（例如，3,10,50，……）最接近的样本检索到所考虑的样本。 它可以用于对看不见的输入进行分类或者用于估算缺失值，在分配给目标值的情况下，考虑K个最近邻居的均值或中值。 这种方法需要距离度量（或相应地，相似性度量）来实际对训练集中的所有样本进行排序并检索最相似的K.<br>要超越XGBoost内置默认策略，我们需要两件事：</p>
<ul>
<li>考虑缺失值的距离指标（感谢AirBnb的这篇<a href="https://medium.com/airbnb-engineering/overcoming-missing-values-in-a-random-forest-classifier-7b1fc1fc03ba" target="_blank" rel="noopener">文章</a>的灵感）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_with_miss</span><span class="params">(a,b,l=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>(len(a) != len(b)):</span><br><span class="line">        <span class="keyword">return</span> np.inf</span><br><span class="line">    ls = l * np.ones(len(a))</span><br><span class="line">    msk = ~ (np.isnan(a) | np.isnan(b))</span><br><span class="line">    res = np.sum((np.abs(a-b)[msk]))+np.sum((ls[~msk]))</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<ul>
<li>规范化数据集以获得有意义的距离，获得了不同域之间特征之间的差异（XGBoost并不严格要求，但KNN估算需要它！）。</li>
</ul>
<p>使用K个最接近样本的所述特征的中值来估算特征的缺失值，并且在非特定情况下，在K个检索的邻居中不发现至少一个非缺失值，整个列的中值 用来。<br><a name="cd474133"></a></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><a href="#kp0eey"></a>实验结果</h3><p>我使用scikit-learn中免费提供的三个众所周知的数据集（两个分类和一个回归）进行了一些测试。 通过k-fold交叉验证比较三种不同的插补策略，测量了性能：</p>
<ul>
<li><p>XGBoost算法中内置的默认值</p>
</li>
<li><p>一个简单的列中位插值</p>
</li>
<li><p>一个KNN，如前一段所述</p>
</li>
</ul>
<p>对于KNN案例，我已经绘制了针对考虑的缺失值百分比获得的最佳性能，其中k与（要考虑的邻居的数量）和λ（当至少一个特征缺失时要添加到距离的常数） 两个样本）。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285831963-0ba52730-2b35-4d6a-9581-f84258cd72e8.png#align=left&amp;display=inline&amp;height=206&amp;originHeight=249&amp;originWidth=1000&amp;status=done&amp;width=826" alt><br>使用稀疏性感知KNN来估算缺失值与其他两种方法的表现一致。 差异的程度当然是数据集依赖的。 作为第一个天真的结论：数据集的质量越低，更好的插补策略的影响越大。 如图2所示，内置策略最终具有接近于平凡的列中值插值的性能。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285859844-3faa8e2e-ebc5-4521-adb5-28b05b251281.png#align=left&amp;display=inline&amp;height=183&amp;originHeight=222&amp;originWidth=1000&amp;status=done&amp;width=826" alt><br>看看k和λ如何影响最终结果以及如何引入惩罚因素不仅仅是纸上谈兵，这是非常有趣的。 距离度量不仅丢弃缺失值而且还为每一个增加权重对于用该方法获得的性能是至关重要的，即使其值与缺失值的增加百分比不直接相关。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545285901042-1590c9c8-9d75-42b3-b76d-3810e74a8f62.png#align=left&amp;display=inline&amp;height=247&amp;originHeight=299&amp;originWidth=1000&amp;status=done&amp;width=826" alt><br>测试表明，根据经验，缺失值的数量越多，为更好的插补而考虑的邻居数量越多。 再次，一个非常直观的结论。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/20/yuque/XGBoost不是黑魔法/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/认识和实现：批量标准化" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/19/yuque/认识和实现：批量标准化/">认识和实现：批量标准化</a>
    </h1>
  

        
        <a href="/2018/12/19/yuque/认识和实现：批量标准化/" class="archive-article-date">
  	<time datetime="2018-12-19T14:43:17.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-19</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b" target="_blank" rel="noopener">链接</a><br>在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。<br><a name="worndf"></a></p>
<h3 id="批量归一化的直观解释"><a href="#批量归一化的直观解释" class="headerlink" title="批量归一化的直观解释"></a><a href="#worndf"></a>批量归一化的直观解释</h3><p><a name="iegmed"></a></p>
<h3 id="训练中的问题"><a href="#训练中的问题" class="headerlink" title="训练中的问题"></a><a href="#iegmed"></a>训练中的问题</h3><p>问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。<br>不同批次分布的另一个影响是消失的梯度。 消失的梯度问题是一个大问题，特别是对于S形激活函数。 如果g（x）表示sigmoid激活函数，则为| x | 增加，g’（x）趋于零。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230849299-89520b64-5633-46b5-a7ab-8343e37bce6d.png#width=731" alt><br>问题2.当输入分布变化时，神经元输出也会变化。 这导致神经元输出偶尔波动到S形函数的可饱和区域。 在那里，神经元既不能更新自己的权重，也不能将梯度传递回先前的层。 我们如何保持神经元输出不变为可饱和区域？</p>
<p>如果我们可以将神经元输出限制在零附近的区域，我们可以确保每个层在反向传播期间都会传回一个实质的梯度。 这将导致更快的训练时间和更准确的结果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230974089-30772f12-78be-446d-ac94-1f658676b64c.png#width=731" alt><br><a name="laeagu"></a></p>
<h4 id="批量标准作为解决方案。"><a href="#批量标准作为解决方案。" class="headerlink" title="批量标准作为解决方案。"></a><a href="#laeagu"></a>批量标准作为解决方案。</h4><p>批量标准化减轻了不同层输入的影响。 通过归一化神经元的输出，激活函数将仅接收接近零的输入。 这确保了非消失的梯度，解决了第二个问题。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231047448-2503dee7-466f-4472-a4d3-d4051a58af2f.png#width=747" alt><br>批量归一化将层输出转换为单位高斯分布。 当这些输出通过激活功能馈送时，层激活也将变得更加正常分布。<br>由于一层的输出是下一层的输入，因此层输入现在具有明显较小的批次间差异。 通过减少层输入的变化分布，我们解决了第一个问题。<br><a name="6w4qpz"></a></p>
<h3 id="数学解释"><a href="#数学解释" class="headerlink" title="数学解释"></a><a href="#6w4qpz"></a>数学解释</h3><p>通过批量归一化，我们为每个激活函数寻找以零为中心的单位方差分布。 在训练期间，我们采用激活输入x并将其减去批次均值μ以实现零中心分布。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231155587-c517033a-5650-4858-8dc8-fe71e96bb8f0.png#width=204" alt><br>接下来，我们取x并将其除以批量方差和一个小数，以防止除以零σ+ε。 这可确保所有激活输入分布都具有单位差异。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231189951-b72be3c8-1a47-42a5-bd6d-04e2ba9a6006.png#width=215" alt><br>最后，我们将x hat进行线性转换以缩放并移动批量标准化的输出。 尽管在反向传播期间网络发生了变化，但仍能确保保持这种正常化效果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231250609-5da7503e-4334-4d00-9e24-61689bab04a2.png#width=231" alt><br>在测试模型时，我们不使用批处理均值或方差，因为这会破坏模型。 （提示：单个观察的平均值和方差是多少？）相反，我们计算训练群体的移动平均值和方差估计值。 这些估计值是培训期间计算的所有批次平均值和方差的平均值。<br><a name="kg00mo"></a></p>
<h3 id="批量标准化的好处"><a href="#批量标准化的好处" class="headerlink" title="批量标准化的好处"></a><a href="#kg00mo"></a>批量标准化的好处</h3><p>批量标准化的好处如下。<br>1.有助于防止具有可饱和非线性（sigmoid，tanh等）的网络中的消失梯度<br>通过批量标准化，我们确保任何激活函数的输入不会变为可饱和区域。 批量归一化将这些输入的分布转换为单位高斯（零中心和单位方差）。<br>2.规范模型<br>也许。 Ioffe和Svegeddy提出了这一主张，但没有就此问题进行广泛撰写。 也许这是归一化层输入的结果？<br>3.允许更高的学习率<br>通过在训练期间防止梯度消失的问题，我们可以设置更高的学习率。 批量标准化还降低了对参数标度的依赖性。 较大的学习速率可以增加层参数的规模，这导致梯度在反向传播期间回传时放大。 我需要阅读更多关于此的内容。<br><a name="5qelzv"></a></p>
<h3 id="在Keras实施"><a href="#在Keras实施" class="headerlink" title="在Keras实施"></a><a href="#5qelzv"></a>在Keras实施</h3><p>引入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import keras</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.image as mpimg</span><br><span class="line"></span><br><span class="line">from keras.models import Model, Sequential</span><br><span class="line">from keras.layers import Input</span><br><span class="line"></span><br><span class="line">from keras.callbacks import ModelCheckpoint, EarlyStopping</span><br><span class="line">from keras.layers import BatchNormalization</span><br><span class="line">from keras.layers import GlobalAveragePooling2D</span><br><span class="line">from keras.layers import Activation</span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Dense</span><br><span class="line">from keras.layers import MaxPooling2D, Dropout, Flatten</span><br><span class="line"></span><br><span class="line">import time</span><br></pre></td></tr></table></figure></p>
<p>数据加载和预处理<br>在这笔记本中，我们使用Cifar 100数据集，因为它具有相当的挑战性，并且不会永远用于训练。 唯一的预处理是零中心和图像变化发生器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from keras.datasets import cifar100</span><br><span class="line">from keras.utils import np_utils</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=&apos;fine&apos;)</span><br><span class="line"></span><br><span class="line">#scale and regularize the dataset</span><br><span class="line">x_train = (x_train-np.mean(x_train))</span><br><span class="line">x_test = (x_test - x_test.mean())</span><br><span class="line"></span><br><span class="line">x_train = x_train.astype(&apos;float32&apos;)</span><br><span class="line">x_test = x_test.astype(&apos;float32&apos;)</span><br><span class="line"></span><br><span class="line">#onehot encode the target classes</span><br><span class="line">y_train = np_utils.to_categorical(y_train)</span><br><span class="line">y_test = np_utils.to_categorical(y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">        shear_range=0.2,</span><br><span class="line">        zoom_range=0.2,</span><br><span class="line">        horizontal_flip=True)</span><br><span class="line"></span><br><span class="line">train_datagen.fit(x_train)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow(x_train,</span><br><span class="line">                                     y = y_train,</span><br><span class="line">                                    batch_size=80,)</span><br></pre></td></tr></table></figure></p>
<p><a name="kl8xlg"></a></p>
<h3 id="在Keras中构建模型"><a href="#在Keras中构建模型" class="headerlink" title="在Keras中构建模型"></a><a href="#kl8xlg"></a>在Keras中构建模型</h3><p>我们的架构将包括堆叠的3x3卷积，然后是最大池化和dropout。 每个网络中有5个卷积块。 最后一层是一个完全连接的层，有100个节点和softmax激活。<br>我们将构建4个不同的卷积网络，每个网络都具有sigmoid或ReLU激活以及批量标准化或不标准化。 我们将比较每个网络的验证损失。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">def conv_block_first(model, bn=True, activation=&quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    The first convolutional block in each architecture. Only    separate so we can specify the input shape.</span><br><span class="line">    &quot;&quot;&quot;    </span><br><span class="line">   #First Stacked Convolution</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;, input_shape =   x_train.shape[1:]))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line">    #Second Stacked Convolution</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(MaxPooling2D())</span><br><span class="line">    model.add(Dropout(0.15))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def conv_block(model, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Generic convolutional block with 2 stacked 3x3 convolutions, max pooling, dropout, </span><br><span class="line">    and an optional Batch Normalization.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(MaxPooling2D())</span><br><span class="line">    model.add(Dropout(0.15))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def conv_block_final(model, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    I bumped up the number of filters in the final block. I made this separate so that I might be able to integrate Global Average Pooling later on. </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Conv2D(100,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(100,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def fn_block(model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    I&apos;m not going for a very deep fully connected block, mainly so I can save on memory.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Dense(100, activation = &quot;softmax&quot;))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def build_model(blocks=3, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Builds a sequential network based on the specified parameters.</span><br><span class="line">    </span><br><span class="line">    blocks: number of convolutional blocks in the network, must be greater than 2.</span><br><span class="line">    bn: whether to include batch normalization or not.</span><br><span class="line">    activation: activation function to use throughout the network.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model = Sequential()</span><br><span class="line"></span><br><span class="line">    model = conv_block_first(model, bn=bn, activation=activation)</span><br><span class="line"></span><br><span class="line">    for block in range(1,blocks-1):</span><br><span class="line">        model = conv_block(model, bn=bn, activation = activation)</span><br><span class="line"></span><br><span class="line">    model = conv_block_final(model, bn=bn, activation=activation)</span><br><span class="line">    model = fn_block(model)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def compile_model(model, optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;]):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compiles a neural network.</span><br><span class="line">    </span><br><span class="line">    model: the network to be compiled.</span><br><span class="line">    optimizer: the optimizer to use.</span><br><span class="line">    loss: the loss to use.</span><br><span class="line">    metrics: a list of keras metrics.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.compile(optimizer = optimizer,</span><br><span class="line">                 loss = loss,</span><br><span class="line">                 metrics = metrics)</span><br><span class="line">    return model</span><br><span class="line">#COMPILING THE 4 MODELS</span><br><span class="line">sigmoid_without_bn = build_model(blocks = 5, bn=False, activation = &quot;sigmoid&quot;)</span><br><span class="line">sigmoid_without_bn = compile_model(sigmoid_without_bn)</span><br><span class="line"></span><br><span class="line">sigmoid_with_bn = build_model(blocks = 5, bn=True, activation = &quot;sigmoid&quot;)</span><br><span class="line">sigmoid_with_bn = compile_model(sigmoid_with_bn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">relu_without_bn = build_model(blocks = 5, bn=False, activation = &quot;relu&quot;)</span><br><span class="line">relu_without_bn = compile_model(relu_without_bn)</span><br><span class="line"></span><br><span class="line">relu_with_bn = build_model(blocks = 5, bn=True, activation = &quot;relu&quot;)</span><br><span class="line">relu_with_bn = compile_model(relu_with_bn)</span><br></pre></td></tr></table></figure></p>
<p><a name="c4xpfd"></a></p>
<h3 id="模特训练"><a href="#模特训练" class="headerlink" title="模特训练"></a><a href="#c4xpfd"></a>模特训练</h3><p>没有批量标准化的Sigmoid<br>训练陷入困境。 有100个课程，这个模型从未达到比随机猜测更好的性能（10％准确度）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history1 = sigmoid_without_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        epochs=20,</span><br><span class="line">        verbose=0,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231717659-060d4678-06bf-4045-abc1-cd7ce7cb6c77.png#width=743" alt><br><a name="m0uloq"></a></p>
<h3 id="具有批量标准化的Sigmoid"><a href="#具有批量标准化的Sigmoid" class="headerlink" title="具有批量标准化的Sigmoid"></a><a href="#m0uloq"></a>具有批量标准化的Sigmoid</h3><p>与没有批量标准化不同，该模型在训练期间开始实施。 这可能是批量标准化减轻消失梯度的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history2 = sigmoid_with_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        verbose=0,</span><br><span class="line">        epochs=20,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231810973-aebf7896-6c50-4d6c-8372-702891142c6f.png#width=724" alt><br><a name="w5h1ml"></a></p>
<h3 id="没有批量标准化的ReLU"><a href="#没有批量标准化的ReLU" class="headerlink" title="没有批量标准化的ReLU"></a><a href="#w5h1ml"></a>没有批量标准化的ReLU</h3><p>在没有批量规范的情况下实施ReLU导致一些初始收益，然后收敛到非最优的局部最小值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history3 = relu_without_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        epochs=20,</span><br><span class="line">        verbose=0,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231864866-451c1cd9-d6ae-4f7e-9aa2-1da0b34f889b.png#width=737" alt><br>具有批量标准化的ReLU<br>与sigmoid模型一样，批量标准化提高了该网络的训练能力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history4 = relu_with_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        verbose=0,</span><br><span class="line">        epochs=20,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231931611-3c026d90-61b0-41df-b4dc-7a538bf01ce6.png#width=730" alt><br><a name="e5gkmq"></a></p>
<h3 id="比较架构"><a href="#比较架构" class="headerlink" title="比较架构"></a><a href="#e5gkmq"></a>比较架构</h3><p>我们在这里清楚地看到批量标准化的好处。 没有批量标准化的ReLU和S形模型都无法保持训练性能提升。 这可能是渐变消失的结果。 具有批量标准化的体系结构训练得更快，并且比没有批量标准化的体系结构表现更好。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231970491-afc0113d-a1fb-4218-90d5-7104d35cb98b.png#width=724" alt><br><a name="0457"></a></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><a href="#0457"></a>Conclusion</h3><p>结论<br>批量标准化减少了训练时间并提高了神经网络的稳定性。 此效果适用于sigmoid和ReLU激活功能。 原帖可以在我的<a href="https://www.harrisonjansma.com/" target="_blank" rel="noopener">网站</a>上找到，代码可以在我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/tree/master/07-28-18-Implementing-Batch-Norm" target="_blank" rel="noopener">GitHub</a>上找到。<br><a name="f6b6"></a></p>
<h3 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a><a href="#f6b6"></a>Resources</h3><ul>
<li><p>Original paper by Ioffe and Szegedy. <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">here.</a></p>
</li>
<li><p>Insert a batch normalization before or after nonlinearities? <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="noopener">Usage explanation</a></p>
</li>
<li><p>For an explanation of the math and implementation in TensorFlow. <a href="https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8" target="_blank" rel="noopener">Pitfalls of Batch Norm</a></p>
</li>
<li><p>Also this post <a href="https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73" target="_blank" rel="noopener">How to use Batch Normalization with TensorFlow and tf.keras</a></p>
</li>
</ul>
<p><a name="3277"></a></p>
<h3 id="Further-reading"><a href="#Further-reading" class="headerlink" title="Further reading"></a><a href="#3277"></a>Further reading</h3><p>Below are some more recent research papers that extend Ioffe and Svegedy’s work.<br><a href="https://arxiv.org/abs/1702.03275v2" target="_blank" rel="noopener">[1]</a> How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)<br><a href="https://arxiv.org/abs/1702.03275v2" target="_blank" rel="noopener">[2]</a> Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models<br><a href="https://arxiv.org/abs/1607.06450v1" target="_blank" rel="noopener">[3]</a> Layer Normalization<br><a href="https://arxiv.org/abs/1602.07868v3" target="_blank" rel="noopener">[4]</a> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks<br><a href="https://arxiv.org/abs/1803.08494v3" target="_blank" rel="noopener">[5]</a> Group Normalization</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/19/yuque/认识和实现：批量标准化/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/不要在卷积网络中使用Dropout" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/19/yuque/不要在卷积网络中使用Dropout/">不要在卷积网络中使用Dropout</a>
    </h1>
  

        
        <a href="/2018/12/19/yuque/不要在卷积网络中使用Dropout/" class="archive-article-date">
  	<time datetime="2018-12-19T14:04:13.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-19</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16" target="_blank" rel="noopener">链接</a></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545228300354-eac11f1e-9363-4b7f-a68d-3b8e81fd80e0.png#width=747" alt><br>如果您想知道如何使用dropout，这里有您要的答案。</p>
<p>我注意到有很多资源可以用来学习深度学习的内容和原因。 不幸的是，当需要制作模型时，他们很少有资源来解释何时以及如何。<br>我正在为试图实施深度学习的其他数据科学家撰写本文。 因此，您不必像我一样通过研究文章和Reddit讨论。<br>在本文中，您将了解为什么dropout在卷积体系结构中不再受欢迎。</p>
<p><a name="drs4ub"></a></p>
<h4 id="DROPOUT"><a href="#DROPOUT" class="headerlink" title="DROPOUT"></a><a href="#drs4ub"></a>DROPOUT</h4><p>如果你正在读这篇文章，我认为你已经了解了什么是dropout，以及它在正则化神经网络方面的作用。 如果您想要复习，请阅读Amar Budhiraja的这篇文章。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545228517714-8daebec1-a6f7-4fcc-b4a8-8c826e025c76.png#width=747" alt><br>通常，当我们的网络存在过度拟合的风险时，我们只需要实现正规化。 如果网络太大，如果您训练时间过长，或者您没有足够的数据，则会发生这种情况。<br>如果在卷积网络末端有完全连接的层，则实现dropout很容易。<br><a name="qseuim"></a></p>
<h4 id="使用Keras"><a href="#使用Keras" class="headerlink" title="使用Keras"></a><a href="#qseuim"></a>使用Keras</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=None, seed=None)</span><br></pre></td></tr></table></figure>
<p>以0.5的dropout率开始并将其调低，直到性能最大化。 （<a href="https://www.reddit.com/r/MachineLearning/comments/3oztvk/why_50_when_using_dropout/" target="_blank" rel="noopener">资源</a>）<br><a name="dx1bhs"></a></p>
<h4 id="例如"><a href="#例如" class="headerlink" title="例如"></a><a href="#dx1bhs"></a>例如</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model=keras.models.Sequential()</span><br><span class="line">model.add(keras.layers.Dense(150, activation=&quot;relu&quot;))</span><br><span class="line">model.add(keras.layers.Dropout(0.5))</span><br></pre></td></tr></table></figure>
<p>请注意，这仅适用于您的convnet的完全连接区域。 对于所有其他地区，您不应使用dropout。<br>相反，您应该在卷积之间插入批量标准化。 这将使您的模型正常化，并使您的模型在训练期间更加稳定。<br><a name="042axx"></a></p>
<h4 id="批正则化"><a href="#批正则化" class="headerlink" title="批正则化"></a><a href="#042axx"></a>批正则化</h4><p>批标准化是规范卷积网络的另一种方法。<br>除了正则化效应之外，批量归一化还可以使您的卷积网络在训练期间抵抗消失的梯度。 这可以减少训练时间并获得更好的性能。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545229002104-b14e8bf0-9ba5-4470-85c3-ac86fed74c75.png#width=747" alt><br>批量标准化可以消除消失的梯度<br><br><br></p>
<p><a name="uat1zt"></a></p>
<h4 id="Keras实施"><a href="#Keras实施" class="headerlink" title="Keras实施"></a><a href="#uat1zt"></a>Keras实施</h4><p>要在Keras中实现批量标准化，请使用以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.BatchNormalization()</span><br></pre></td></tr></table></figure></p>
<p>构建具有批量规范化的卷积体系结构时：</p>
<ul>
<li><p>在卷积和激活层之间插入批量标准化层。 （<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="noopener">资源</a>）</p>
</li>
<li><p>您可以在此功能中调整一些超参数，并使用它们。</p>
</li>
</ul>
<p>您也可以在激活功能之后插入批量标准化，但根据我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/blob/master/08-12-18%20Batch%20Norm%20vs%20Dropout/08-12-18%20Batch%20Norm%20vs%20Dropout.ipynb" target="_blank" rel="noopener">经验</a>，这两种方法都具有相似的性能。<br><a name="i9denc"></a></p>
<h4 id="例如-1"><a href="#例如-1" class="headerlink" title="例如"></a><a href="#i9denc"></a>例如</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(&quot;relu&quot;))</span><br></pre></td></tr></table></figure>
<p>批量标准化取代了dropout。<br>即使您不需要担心过度拟合，实现批量标准化也有很多好处。 正因为如此，它的正规化效应，批量归一化已经在很大程度上取代了现代卷积体系结构中的dropout。<br>“我们提出了一种使用批量规范化网络构建，训练和执行推理的算法。 由此产生的网络可以通过饱和非线性进行训练，更能容忍增加的训练率，并且通常不需要Dropout进行正规化。“ -  Ioffe and Svegedy 2015<br>至于为什么dropout在最近的应用中失宠，主要有两个原因。<br><strong>首先</strong>，在对卷积层进行正则化时，dropout通常不太有效。<br>原因？ 由于卷积层具有很少的参数，因此它们开始时需要较少的正则化。 此外，由于在特征图中编码的空间关系，激活可以变得高度相关。 这使得dropout无效。（<a href="https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/" target="_blank" rel="noopener">资源</a>）<br><strong>其次</strong>，擅长正规化的dropout现在已经过时了。<br>像VGG16这样在网络末端包含完全连接的层的大型模型。 对于这样的模型，过度拟合是通过在完全连接的层之间包括dropout来解决的。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545229692686-cc1c012c-7f9a-4865-b6eb-fe547e0f1213.png#width=470" alt><br>不幸的是，<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">最近的架构</a>远离了这个完全连接块。<br>通过用全局平均池替换密集层，现代的网络可以减少模型大小，同时提高性能。<br>我将在未来再写一篇文章，详细说明如何在卷积网络中实现全球平均汇集。 在此之前，我建议阅读<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">ResNet论文</a>，以了解GAP的好处。<br><a name="btyoyw"></a></p>
<h4 id="一个实验"><a href="#一个实验" class="headerlink" title="一个实验"></a><a href="#btyoyw"></a>一个实验</h4><p>我创建了一个实验来测试批量标准化是否会减少在卷积之间插入时的泛化错误。 （<a href="https://github.com/harrisonjansma/Research-Computer-Vision/blob/master/08-12-18%20Batch%20Norm%20vs%20Dropout/08-12-18%20Batch%20Norm%20vs%20Dropout.ipynb" target="_blank" rel="noopener">链接</a>）<br>我构建了5个相同的卷积体系结构，并在卷积之间插入了dropout，批量规范或任何（控制）。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545229984300-3501e371-42ce-41de-8aec-2d6962959cd3.png#width=500" alt><br>通过在Cifar100数据集上训练每个模型，我获得了以下结果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230018898-2895598f-bf57-42f7-ad4f-b7440add1220.png#width=724" alt><br>批量标准化模型的良好表现说明应在卷积之间使用批量标准化。<br>此外，不应在卷基层之间放置dropout，因为dropout的模型往往比控制模型表现更差。<br>有关更多信息，请查看我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/blob/master/08-12-18%20Batch%20Norm%20vs%20Dropout/08-12-18%20Batch%20Norm%20vs%20Dropout.ipynb" target="_blank" rel="noopener">GitHub</a>上的完整文章。<br>小贴士<br>如果你想知道是否应该在卷积网络中实现dropout，现在你知道了。 仅在完全连接的层上使用dropout，并在卷积之间实现批量标准化。<br>如果您想了解有关批量标准化的更多信息，请阅读：<br><a href="https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b" target="_blank" rel="noopener">https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/19/yuque/不要在卷积网络中使用Dropout/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/理解二进制交叉熵、对数损失函数:一种可视化解释" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/">理解二进制交叉熵、对数损失函数:一种可视化解释</a>
    </h1>
  

        
        <a href="/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/" class="archive-article-date">
  	<time datetime="2018-12-19T08:27:20.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-19</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" target="_blank" rel="noopener">链接</a></p>
<p><a name="61a3ec66"></a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><a href="#ba72yr"></a>介绍</h2><p>如果您正在训练二进制分类器，则可能使用二进制交叉熵/对数损失作为损失函数。<br>你有没有想过使用这种损失函数究竟是什么意思？ 问题是，考虑到今天的库和框架的易用性，很容易忽略所使用的损失函数的真正含义。<br>动机<br>我正在寻找一篇博文，以一种视觉上清晰简洁的方式解释二进制交叉熵/对数损失背后的概念，所以我可以在Data Science Retreat向我的学生展示它。 由于我找不到任何符合我目的的东西，我自己负责编写任务:-)。<br>一个简单的分类问题<br>让我们从10个随机点开始：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</span><br></pre></td></tr></table></figure></p>
<p>这是我们唯一的特征：x。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545208575711-9a5c4e19-f272-468f-b36e-0d7074d62586.png#align=left&amp;display=inline&amp;height=111&amp;originHeight=111&amp;originWidth=625&amp;status=done&amp;width=625" alt><br>现在，让我们为我们的点分配一些颜色：红色和绿色。 这些是我们的标签。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545208606509-b2a7b1cd-b1cf-4ca4-baca-88196d101a0b.png#align=left&amp;display=inline&amp;height=110&amp;originHeight=110&amp;originWidth=643&amp;status=done&amp;width=643" alt><br>因此，我们的分类问题非常简单：鉴于我们的特征x，我们需要预测其标签：红色或绿色。</p>
<p>由于这是一个二元分类，我们也可以将这个问题描述为：“是点绿色”，或者更好的是，“点是绿色的概率是多少”？ 理想情况下，绿点的概率为1.0（绿色），而红点的概率为0.0（绿色）。</p>
<p>在此设置中，绿点属于正类（YES，它们是绿色），而红点属于负类（NO，它们不是绿色）。<br>如果我们拟合模型来执行此分类，它将预测每个点的绿色概率。 根据我们对点的颜色的了解，我们如何评估预测概率的优劣（或差）？ 这是损失功能的全部目的！ 它应该为错误预测返回高值，为良好预测返回低值。</p>
<p>对于像我们的例子那样的二进制分类，典型的损失函数是二进制交叉熵/对数损失函数。<br><a name="6dab43bd"></a></p>
<h2 id="损失函数：二进制交叉熵-对数损失函数"><a href="#损失函数：二进制交叉熵-对数损失函数" class="headerlink" title="损失函数：二进制交叉熵/对数损失函数"></a><a href="#gh52id"></a>损失函数：二进制交叉熵/对数损失函数</h2><p>如果你观察这个损失函数，这就是你会发现的：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545208847996-2aba3677-2186-4f8c-bdab-ae8590ad7c6b.png#align=left&amp;display=inline&amp;height=79&amp;originHeight=79&amp;originWidth=548&amp;status=done&amp;width=548" alt><br>Binary Cross-Entropy / Log Loss<br>其中y是标签（绿点为1，红点为0），p(y)是所有N点的点为绿色的预测概率。</p>
<p>阅读这个公式，它告诉你，对于每个绿点（y = 1），它将log（p（y））添加到损失中，即它是绿色的对数概率。 相反，它为每个红点（y = 0）添加log（1-p（y）），即，它为红色的对数概率。 不一定很难，当然也不是那么直观……</p>
<p>此外，熵与这一切有什么关系？ 为什么我们首先记录概率？ 这些是有效的问题，我希望在下面的“给我看数学”部分回答它们。</p>
<p><a name="9acf4584"></a></p>
<h3 id="计算损失-可视化方法"><a href="#计算损失-可视化方法" class="headerlink" title="计算损失-可视化方法"></a><a href="#sbsnmi"></a>计算损失-可视化方法</h3><p>首先，让我们根据他们的类别（正面或负面）分割点数，如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214792607-9e822a37-5f9e-46cb-bc0b-0a68f744faff.png#align=left&amp;display=inline&amp;height=224&amp;originHeight=224&amp;originWidth=620&amp;status=done&amp;width=620" alt><br>现在，让我们训练一个Logistic回归来对我们的点进行分类。 拟合回归是一个S形曲线，表示任何给定x点的绿色概率。 它看起来像这样：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214823897-b1ef51c7-4c5f-4ca4-8543-81231a9f1918.png#align=left&amp;display=inline&amp;height=215&amp;originHeight=215&amp;originWidth=686&amp;status=done&amp;width=686" alt><br>那么，对于属于正类（绿色）的所有点，我们的分类器给出的预测概率是多少？ 这些是S形曲线下的绿色条形，在与这些点对应的x坐标处。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214857312-a0a2e0e3-5317-4fe4-b9a8-64d6f07dbdf1.png#align=left&amp;display=inline&amp;height=209&amp;originHeight=209&amp;originWidth=683&amp;status=done&amp;width=683" alt><br>好的，到目前为止，真好！ 负面阶级的观点怎么样？ 请记住，S形曲线下的绿色条表示给定点为绿色的概率。 那么，给定点是红色的概率是多少？ 红色条在S形曲线上方，当然:-)<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214885683-601d1b29-50b1-4f89-98d8-242b19e28808.png#align=left&amp;display=inline&amp;height=209&amp;originHeight=209&amp;originWidth=678&amp;status=done&amp;width=678" alt><br>总而言之，我们最终会得到这样的结论：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214919348-98029aeb-bbdd-4969-b2bd-c5d5a5b9b83d.png#align=left&amp;display=inline&amp;height=210&amp;originHeight=210&amp;originWidth=683&amp;status=done&amp;width=683" alt><br>条形表示与每个点的相应真实类别相关联的预测概率！</p>
<p>好的，我们有预测的概率……通过计算二进制交叉熵/对数损失来评估它们的时间！</p>
<p>这些概率就是我们所需要的，所以，让我们摆脱x轴并将条带彼此相邻：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545214956710-95a7d392-f358-48ef-bf1e-6fc94c658e98.png#align=left&amp;display=inline&amp;height=262&amp;originHeight=262&amp;originWidth=425&amp;status=done&amp;width=425" alt><br>好吧，吊杆不再有意义了，所以让我们重新定位它们：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215026705-0bd2aed9-8956-416f-9449-899f50e0ef36.png#align=left&amp;display=inline&amp;height=254&amp;originHeight=254&amp;originWidth=434&amp;status=done&amp;width=434" alt><br>既然我们正试图计算损失，我们需要惩罚不好的预测，对吧？ 如果与真实类相关的概率为1.0，我们需要将其损失为零。 相反，如果这个概率很低，比如0.01，我们需要它的损失是巨大的！<br>事实证明，对于这个目的，取概率的（负）对数就足够了（因为0.0和1.0之间的值的对数是负的，我们采用负对数来获得损失的正值）。<br>实际上，我们使用日志的原因来自交叉熵的定义，请查看下面的“给我看数学”部分了解更多详情。<br>下图给出了一个清晰的图片 - 如果真实类的预测概率接近零，则损失呈指数增长：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215109720-0cebdd78-1004-4e66-8dba-ed416c518790.png#align=left&amp;display=inline&amp;height=307&amp;originHeight=307&amp;originWidth=418&amp;status=done&amp;width=418" alt><br>很公平！ 让我们采用概率的（负）对数 - 这些是每个点的相应损失。<br>最后，我们计算所有这些损失的平均值。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215139483-4a54d212-a279-400b-afe6-df918499a792.png#align=left&amp;display=inline&amp;height=257&amp;originHeight=257&amp;originWidth=441&amp;status=done&amp;width=441" alt><br>瞧！ 我们已经成功计算了这个玩具示例的二进制交叉熵/对数损失。 它是0.3329！<br>告诉我代码<br>如果你想仔细检查我们找到的值，只需运行下面的代码并亲自看看:-)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.metrics import log_loss</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])</span><br><span class="line">y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])</span><br><span class="line"></span><br><span class="line">logr = LogisticRegression(solver=&apos;lbfgs&apos;)</span><br><span class="line">logr.fit(x.reshape(-1, 1), y)</span><br><span class="line"></span><br><span class="line">y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()</span><br><span class="line">loss = log_loss(y, y_pred)</span><br><span class="line"></span><br><span class="line">print(&apos;x = &#123;&#125;&apos;.format(x))</span><br><span class="line">print(&apos;y = &#123;&#125;&apos;.format(y))</span><br><span class="line">print(&apos;p(y) = &#123;&#125;&apos;.format(np.round(y_pred, 2)))</span><br><span class="line">print(&apos;Log Loss / Cross Entropy = &#123;:.4f&#125;&apos;.format(loss))</span><br></pre></td></tr></table></figure></p>
<p>告诉我数学（真的吗？！）<br>除了笑话之外，这篇文章并不打算在数学上倾向于……但是对于你们这些人，我的读者，想要了解熵的作用，所有这些中的对数，我们在这里:-)<br>如果你想深入了解信息理论，包括所有这些概念 - 熵，交叉熵等等 - 检查Chris Olah的帖子，它非常详细！<br><a name="b2c9e438"></a></p>
<h4 id="分布"><a href="#分布" class="headerlink" title="分布"></a><a href="#gs7ekn"></a>分布</h4><p>让我们从分配点开始吧。 由于y表示我们的点的类（我们有3个红点和7个绿点），这就是它的分布，我们称之为q（y），如下所示：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215306977-97044aee-62cc-46c7-b7b2-cc4e60a53c39.png#align=left&amp;display=inline&amp;height=284&amp;originHeight=284&amp;originWidth=432&amp;status=done&amp;width=432" alt></p>
<p>熵<br>熵是与给定分布q（y）相关的不确定性的度量。<br>如果我们所有的积分都是绿色怎么办 那个分布的不确定性是什么？ ZERO，对吗？ 毕竟，对于一个点的颜色毫无疑问：它总是绿色的！ 所以，熵是零！<br>另一方面，如果我们确切地知道一半的点是绿色而另一半点是红色的呢？ 那是最糟糕的情况，对吗？ 猜测点的颜色绝对没有优势：它完全是随机的！ 对于这种情况，熵由下面的公式给出（我们有两个类别（颜色） - 红色或绿色 - 因此，2）：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215372228-11ebb814-6d1e-4797-b42e-94b77db29f6d.png#align=left&amp;display=inline&amp;height=43&amp;originHeight=43&amp;originWidth=166&amp;status=done&amp;width=166" alt><br>对于中间的其他情况，我们可以使用下面的公式计算分布的熵，如q（y），其中C是类的数量：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215399649-c50c5363-f426-4527-bf68-134aee0b35a4.png#align=left&amp;display=inline&amp;height=80&amp;originHeight=80&amp;originWidth=306&amp;status=done&amp;width=306" alt><br>因此，如果我们知道随机变量的真实分布，我们就可以计算其熵。 但是，如果是这样的话，为什么首先要费心去训练分类器呢？ 毕竟，我们知道真正的分布……<br>但是，如果我们不这样做呢？ 我们可以尝试用其他一些分布近似真实分布，比如p（y）吗？ 我们当然可以！:-)<br><a name="bf08985d"></a></p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a><a href="#ekaaou"></a>交叉熵</h4><p>让我们假设我们的观点遵循其他分布p（y）。 但我们知道它们实际上来自真实（未知）分布q（y），对吧？<br>如果我们像这样计算熵，我们实际上是在计算两个分布之间的交叉熵：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215448740-5ec55a28-5c13-46a0-b6d1-197063d62b05.png#align=left&amp;display=inline&amp;height=85&amp;originHeight=85&amp;originWidth=320&amp;status=done&amp;width=320" alt><br>如果我们奇迹般地将p（y）与q（y）完美匹配，则交叉熵和熵的计算值也将匹配。<br>由于这可能永远不会发生，因此交叉熵将具有比在真实分布上计算的熵更大的BIGGER值。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215480123-9afd0d8f-9398-450a-b194-4ab6348a4685.png#align=left&amp;display=inline&amp;height=47&amp;originHeight=47&amp;originWidth=208&amp;status=done&amp;width=208" alt><br>事实证明，交叉熵和熵之间的这个区别有一个名字……<br><a name="bbec16ca"></a></p>
<h4 id="Kullback-Leibler发散"><a href="#Kullback-Leibler发散" class="headerlink" title="Kullback-Leibler发散"></a><a href="#fk7aht"></a>Kullback-Leibler发散</h4><p>Kullback-Leibler Divergence，简称“KL Divergence”，衡量两种发行版之间的差异：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215542635-c4a2d275-64a9-45c9-b68e-3f99fce5fcf5.png#align=left&amp;display=inline&amp;height=89&amp;originHeight=89&amp;originWidth=619&amp;status=done&amp;width=619" alt><br>这意味着，p（y）越接近q（y），发散越低，因此交叉熵越低。<br>所以，我们需要找到一个好的p（y）来使用……但是，这是我们的分类器应该做的，不是吗？！ 确实如此！ 它寻找最好的p（y），这是最小化交叉熵的那个。<br><a name="7162a4e0"></a></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><a href="#xokywp"></a>损失函数</h4><p>在训练期间，分类器使用其训练集中的N个点中的每一个来计算交叉熵损失，有效地拟合分布p（y）！ 由于每个点的概率是1 / N，因此交叉熵由下式给出：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215580283-c3d40241-87ec-4541-aaf9-971fc6cc90e7.png#align=left&amp;display=inline&amp;height=85&amp;originHeight=85&amp;originWidth=411&amp;status=done&amp;width=411" alt><br>还记得上面的图6到10吗？ 我们需要在与每个点的真实类相关联的概率之上计算交叉熵。 这意味着使用绿色条形作为正类（y = 1）中的点，使用红色条形作为负类中的点（y = 0），或者数学上说：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215611182-db1eab68-632e-45e8-ac03-396234f18194.png#align=left&amp;display=inline&amp;height=75&amp;originHeight=75&amp;originWidth=230&amp;status=done&amp;width=230" alt><br>最后一步是计算两个类中所有点的平均值，正面和负面：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215637206-7fda25fc-c763-46c0-9eca-6e4fe9cec776.png#align=left&amp;display=inline&amp;height=92&amp;originHeight=92&amp;originWidth=592&amp;status=done&amp;width=592" alt><br>最后，通过一些操作，我们可以采用相同的公式，从正面或负面的类中获取任何一点：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545215662641-4915d2cb-3c29-4fc8-a535-c0db8b055951.png#align=left&amp;display=inline&amp;height=79&amp;originHeight=79&amp;originWidth=548&amp;status=done&amp;width=548" alt><br>瞧！ 我们回到二进制交叉熵/日志丢失的原始公式:-)<br><a name="fef1217c"></a></p>
<h3 id="最后的想法"><a href="#最后的想法" class="headerlink" title="最后的想法"></a><a href="#3038bg"></a>最后的想法</h3><p>我真的希望这篇文章能够对一个经常被认为理所当然的概念，即二元交叉熵作为损失函数的概念有所启发。 此外，我也希望它能够向您展示机器学习和信息理论如何联系在一起。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-yuque/关于目标检测，所有你应该知道的深度学习模型" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/">关于目标检测，所有你应该知道的深度学习模型</a>
    </h1>
  

        
        <a href="/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/" class="archive-article-date">
  	<time datetime="2018-12-19T07:22:45.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-12-19</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://medium.com/@syshen/%E7%89%A9%E9%AB%94%E5%81%B5%E6%B8%AC-object-detection-740096ec4540" target="_blank" rel="noopener">链接</a></p>
<p><a name="e60a"></a></p>
<h3 id="Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO"><a href="#Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO" class="headerlink" title="Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO"></a><a href="#e60a"></a>Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO</h3><p>这篇是简介一些用来辨识影像中物体的AI 模型。</p>
<p>在前面有提到，透过CNN模型，你可以输入一张图片，<a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5" target="_blank" rel="noopener">得到该图片属于哪种类别的结果</a>，这过程我们把他称作分类(Classification)。</p>
<p>但在真实世界的应用情境通常要从一张图片中辨识所有出现的物体， 并且标示出位置来(标出位置称之为Object Localization)。你一定在网路上看过类似底下的影片，这段影片可以看出中国闭路摄影机(CCTV)发展的概况，不只是可以框出影像中每个物件，辨别物件种类，侦测出移动物体的动量，甚至是人脸辨识，实现楚门世界的恶梦。要做到这就需要靠深度学习中的Object Detection 演算法，这也是最近几年来深度学习最蓬勃发展的一块领域。<br><a href="https://youtu.be/aE1kA0Jy0Xg" target="_blank" rel="noopener">https://youtu.be/aE1kA0Jy0Xg</a><br>基本的想法是，既然CNN 对于物体的分类又快又好，那我们可不可以拿CNN 来扫描并辨识图片中的任何物体？答案当然是 — 可以。</p>
<p>最简单的作法就是用Sliding Windows 的概念，也就是用一个固定大小的框框，逐一的扫过整张图片，每次框出来的图像丢到CNN 中去判断类别。由于物体的大小是不可预知的，所以还要用不同大小的框框去侦测。但是Sliding Window 是非常暴力的作法，对单一影像我们需要扫描非常多次，每扫一次都需要算一次CNN，这将会耗费大量的运算资源，而且速度慢，根本无法拿来应用！<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204251780-a74eec91-8d1c-42c3-b27a-da11667ac2c3.png#width=826" alt></p>
<p>所以后来就有人提出了R-CNN (Regions with CNN)<br><a name="68d0"></a></p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="#68d0"></a>R-CNN</h3><p>与其用Sliding Window的方式扫过一轮，<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a>的作法是预先筛选出约2000个可能的区域，再将这2000区域个别去作分类，所以他的演算法流程如下：</p>
<ol>
<li><p>产生一群约2000 个可能的区域(Region Proposals)</p>
</li>
<li><p>经由一个预先训练好的CNN 模型如AlexNet 撷取特征，将结果储存起来。</p>
</li>
<li><p>然后再以SVM (Support Vector Machine) 分类器来区分是否为物体或者背景。</p>
</li>
<li><p>最后经由一个线性回归模型来校正bounding box 位置。</p>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204292906-82d75e3e-ada2-4402-8a3e-e1f746718a78.png#width=826" alt><br><a name="9185"></a></p>
<h4 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a><a href="#9185"></a>Selective Search</h4><p>R-CNN用来筛选Region Proposals的方法称之为<a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">Selective Search</a>，而Selective Search又是基于Felzenszwal于2004年发表的论文<a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf" target="_blank" rel="noopener">Graph Base Image Segmentation</a>。</p>
<p>图像经由Graph Base Image Segmentation 可以切出数个Segment 来，如下图：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204317496-a09e245d-e3f5-4fa9-9e62-3fbcd048e2c9.png#width=554" alt><br>而Selective Search 的作法是将Segment 的结果先各自画出bounding box，然后以一个回圈，每次合并相似度最高的两个box，直到整张图合并成单一个box 为止，在这过程中的所有box 便是selective search 出来的region proposals。Selective Search 的演算法如下：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204363747-83bbc0c6-5f23-4b0c-bae2-3d3d18561a34.png#width=826" alt><br>取自Selective Search 论文。先以Graph base image segmentation 取得一些区域，计算每个区域间的相似度，每次合并相似度最高的两个区域，直到整张图片成为单一区域为止。<br>但是R-CNN 存在一些问题，速度仍然不够快：</p>
<ol>
<li><p>R-CNN 一开始必须先产生约2000 个区域，每个区域都要丢进CNN 中去撷取特征，所以需要跑过至少2000 次的CNN</p>
</li>
<li><p>R-CNN 的model 是分开成三部份，分别是用来取出特征的CNN，分类的SVM，以及优化bounding box 的线性回归。所以R-CNN 不容易作训练。</p>
</li>
</ol>
<p>所以R-CNN的其中一个作者<a href="http://www.rossgirshick.info/" target="_blank" rel="noopener">Ross Girshick (RBG大神)</a>在2015年又提出了一个改良版本，并称之为Fast R-CNN。<br><a name="1250"></a></p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="#1250"></a>Fast R-CNN</h3><p><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a>的想法很简单，在R-CNN中，2000多个区域都要个别去运算CNN，这些区域很多都是重叠的，也就是说这些重叠区域的CNN很多都是重复算的。所以Fast R-CNN的原则就是全部只算一次CNN就好，CNN撷取出来的特征可以让这2000多个区域共用！</p>
<p>Fast R-CNN 采用的作法就是RoIPooling (Region of Interest Pooling)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204447478-48e75d73-6502-4714-a7e2-b1064c212381.png#width=826" alt><br>Fast RCNN 一样要预选Region proposals，但是只做一次CNN。在跑完Convolution layers 的最后一层时，会得到一个HxW 的feature map，同时也要将region proposals 对应到HxW 上，然后在feature map 上取各自region 的MaxPooling，每个region 会得到一个相同大小的矩阵(例如2x2)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204932699-604244a0-ec32-4161-b825-10e50a4d088b.png#width=800" alt><br>from <a href="https://blog.deepsense.ai/region-of-interest-pooling-explained/" target="_blank" rel="noopener">https://blog.deepsense.ai/region-of-interest-pooling-explained/</a><br>然后各自连接上FC 网路，以及softmax 去作分类。在分类的同时也作bounding box 的线性回归运算。</p>
<p>Fast RCNN 的优点是：</p>
<ol>
<li><p>只需要作一次CNN，有效解省运算时间</p>
</li>
<li><p>使用单一网络，简化训练过程</p>
</li>
</ol>
<p><a name="fcaf"></a></p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="#fcaf"></a>Faster R-CNN</h3><p>不管是R-CNN还是Fast R-CNN都还是要先透过selective search预选region proposals，这是一个缓慢的步骤。在2015年时，Microsoft的<a href="http://shaoqingren.com/" target="_blank" rel="noopener">Shaoqing Ren</a> , <a href="http://kaiminghe.com/" target="_blank" rel="noopener">Kaiming He</a> , <a href="http://www.rossgirshick.info/" target="_blank" rel="noopener">Ross Girshick</a> ,以及<a href="http://www.jiansun.org/" target="_blank" rel="noopener">Jian Sun</a>提出了<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>，一个更快的R-CNN。<br>Faster R-CNN 的想法也很直觉，与其预先筛选region proposals，到不如从CNN 的feature map 上选出region proposals。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204990054-48efb89e-132f-444c-938d-7e4441882e73.png#width=650" alt><br><a name="fef5"></a></p>
<h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a><a href="#fef5"></a>Region Proposal Network</h4><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205011580-f995c601-05b6-4d1c-9958-ca8d13714c32.png#width=826" alt><br>RPN (Region Proposal Network) 也是一个Convolution Network，Input 是之前CNN 输出的feature map，输出是一个bounding box 以及该bounding box 包含一个物体的机率。</p>
<p>RPN 在feature map 上取sliding window，每个sliding window 的中心点称之为anchor point，然后将事先准备好的k 个不同尺寸比例的box 以同一个anchor point 去计算可能包含物体的机率(score) ，取机率最高的box。这k 个box 称之为anchor box。所以每个anchor point 会得到2k 个score，以及4k 个座标位置(box 的左上座标，以及长宽，所以是4 个数值)。在Faster R-CNN 论文里，预设是取3 种不同大小搭配3 种不同长宽比的anchor box，所以k 为3x3 = 9 。</p>
<p>经由RPN 之后，我们便可以得到一些最有可能的bounding box，虽然这些bounding box 不见得精确，但是透过类似于Fast RCNN 的RoIPooling， 一样可以很快的对每个region 分类，并找到最精确的bounding box 座标。<br><a name="83ca"></a></p>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a><a href="#83ca"></a>Mask R-CNN</h3><p>前述几个方法都是在找到物体外围的bounding box，bounding box基本上都是方形，另外一篇有趣的论文是Facebook AI researcher <a href="http://kaiminghe.com/" target="_blank" rel="noopener">Kaiming He</a>所提出的<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>，透过Mask R-CNN不只是找到bounding box，可以做到接近pixel level的遮罩(图像分割Image segmentation)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205050916-f7cefd4c-be49-44ba-8786-bb25170f7e4d.png#width=826" alt><br>要了解Mask R-CNN 如何取遮罩，要先看一下FCN (Fully Convolutional Network)</p>
<p><a name="28c4"></a></p>
<h4 id="FCN-Fully-Convolutional-Network-for-Image-Segmentation"><a href="#FCN-Fully-Convolutional-Network-for-Image-Segmentation" class="headerlink" title="FCN (Fully Convolutional Network) for Image Segmentation"></a><a href="#28c4"></a>FCN (Fully Convolutional Network) for Image Segmentation</h4><p>有别于CNN 网络最后是连上一个全连接(Fully Connected)的网络，FCN (Fully Convolutional Network)最后接上的是一个卷积层。一般的CNN 只能接受固定大小的Input，但是FCN 则能接受任何大小的Input，例如W x H 。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205073642-70d8970e-7b91-4772-93fe-4c805439a274.png#width=826" alt><br><em>图上方一般的CNN 网络，只能接受大小固定的输入，得到单一维度的输出，分别代表每个类别的机率。图下则是FCN 网路，最后两层由卷积取代，输出为hxwx 1000，代表每个pixel 种类的机率，可以视为一个heapmap。</em></p>
<p>在CNN 的过程中会一直作downsampling，所以FCN 最后的输出可能为H/32 x W/32，实际上得到的会是一个像heapmap 的结果。但是由于这过程是downsampling，所以Segment 的结果是比较粗糙，为了让Segment 的效果更好，要再做upsampling，来补足像素。upsamping 的作法是取前面几层的结果来作差补运算。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205110140-81193d10-5537-493b-9d73-b9b4353ac2d1.png#width=826" alt><br>FCN 的结果会跟前面几层的输出作差补运算<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205127937-663202b8-0a40-43e5-a833-7854a202f6f9.png#width=826" alt><br>Mask R-CNN是建构于Faster R-CNN之上，如果是透过RoIPooling取得Region proposals之后，针对每个region会再跑FCN取得遮罩分割，但是由于RoIPooling在做Max pooling时，会使用最近插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Nearest Neighbor Interpolation</a> )取得数值，所以出来的遮罩会有偏移现象，再加上pooling下来的结果，会让region的尺寸出现非整数的情况，然后取整数的结果就是没办法做到Pixel层级的遮罩。所以Mask R-CNN改采用双线性插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Bilinear Interpolation</a> )来改善RoIPooling，称之为RoIAlign，RoIAlign会让遮罩位置更准确。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205148755-2522b72b-9b05-4e9b-af6f-ef024168767e.png#width=826" alt><br>Mask RCNN 架构，将原有的RoIPooling 改成 RoIAlign。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205172687-cd927423-a678-4557-8914-e933152be762.png#width=826" alt><br>Fast R-CNN 的RoIPool。将一个7x5 的Anchor box 取2x2 的MaxPool，由于使用最近插值法，会有偏差。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205202715-768d38b1-ac5b-48f7-91fd-35ef7abb8a64.png#width=826" alt></p>
<p>RoIAlign的作法是使用双线性插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Bilinear Interpolation</a> )，减少mis-alignment的问题。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205230066-e554096e-51e2-492f-8e1d-fbd2fd312f9d.png#width=826" alt><br><a name="c62f"></a></p>
<h3 id="YOLO-You-Only-Look-Once"><a href="#YOLO-You-Only-Look-Once" class="headerlink" title="YOLO: You Only Look Once"></a><a href="#c62f"></a>YOLO: You Only Look Once</h3><p><a href="https://pjreddie.com/media/files/papers/yolo.pdf" target="_blank" rel="noopener">YOLO</a>有个很讨喜的名字，取自<a href="https://zh.wikipedia.org/wiki/YOLO" target="_blank" rel="noopener">You Only Live Once</a>，但用在Object detection上则为You only look once，意思是说YOLO模型的特性只需要对图片作一次CNN便能够判断里面的物体类别跟位置，大大提升辨识速度。</p>
<p>R-CNN 的概念是先提出几个可能包含物体的Region proposal，再针对每个region 使用CNN 作分类，最后再以regression 修正bounding box 位置，速度慢且不好训练。YOLO 的好处是单一网路设计，判断的结果会包含bounding box 位置，以及每个bounding box 所属类别及概率。整个网路设计是end-to-end 的，容易训练，而且速度快。</p>
<ol>
<li><p>YOLO 速度快，在Titan X GPU 上可以达到每秒45 祯的速度，简化版的YOLO 甚至可以达到150 fps 的速度。这意味着YOLO 已经可以对影像作即时运算了。准确度(mAP) 也狠甩其他深度学习模型好几条街。看看底下YOLO2 的demo 视频，这侦测速度会吓到吃手手了<a href="https://youtu.be/VOC3huqHrss" target="_blank" rel="noopener">https://youtu.be/VOC3huqHrss</a></p>
</li>
<li><p>有别于R-CNN 都是先提region 再做判断，看的范围比较小，容易将背景的background patch 看成物体。YOLO 在训练跟侦测时都是一次看整张图片，背景错误侦测率(background error, 抑或false positive) 都只有Fast R-CNN 的一半。</p>
</li>
<li><p>YOLO 的泛用性也比R-CNN 或者DPM 方式来得好很多，在新的domain 使用YOLO 依旧可以很稳定。<br>YOLO 的概念是将一张图片切割成S x S 个方格，每个方格以自己为中心点各自去判断B 个bounding boxes 中包含物体的confidence score 跟种类。<br>confidence score = Pr(Object) * IOU (ground truth)<br>如果该bounding box 不包含任何物体(Pr(Object) = 0)，confidence score 便为零，而IOU 则为bounding box 与ground truth 的交集面积，交集面积越大，分数越高。<br>每个方格预测的结果包含5 个数值，x 、y 、w 、 h 跟confidence，x 与y 是bounding box 的中间点，w 与h 是bounding box 的宽跟高。</p>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205319660-33410751-7339-4534-a37e-f2d61752ae1b.png#width=826" alt><br>S = 7，B = 2，PASCAL VOC label 20 种种类，所以tensor 为S x S x (5 * B + C) = 7 x 7 x 30<br>YOLO 的网路设计包含了24 个卷积层，跟2 层的FC 网络。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205351976-8c342184-e75a-4ac0-add0-a2e06853c86d.png#width=826" alt><br>另外一个版本的YOLO Fast 则只有9 个卷积层，不过最后的输出都是7x7x30 的tensor。</p>
<p><a name="e2e8"></a></p>
<h4 id="YOLO-的缺点"><a href="#YOLO-的缺点" class="headerlink" title="YOLO 的缺点"></a><a href="#e2e8"></a>YOLO 的缺点</h4><ol>
<li><p>由于YOLO 对于每个方格提两个bounding box 去作侦测，所以不容易去区分两个相邻且中心点又非常接近的物体</p>
</li>
<li><p>只有两种bounding box，所以遇到长宽比不常见的物体的检测率较差</p>
</li>
</ol>
<p><a name="5aaa"></a></p>
<h4 id="YOLO-与其他模型的比较"><a href="#YOLO-与其他模型的比较" class="headerlink" title="YOLO 与其他模型的比较"></a><a href="#5aaa"></a>YOLO 与其他模型的比较</h4><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205373222-774ef6c0-1da2-49fa-a9c6-1aa802765f10.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205389614-e1f9cac9-d060-463b-b15f-cae65400191c.png#width=826" alt><br><a name="da63"></a></p>
<h3 id="YOLO2"><a href="#YOLO2" class="headerlink" title="YOLO2"></a><a href="#da63"></a>YOLO2</h3><p><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO2</a>建构于YOLO之上，但是有更好的准确度，更快速的判断速度，能够判断更多的物件种类(多达9000种)，所以是<strong>更好(Better)、更快(Faster)、更强大(Stronger)</strong>！<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205410945-959e3ef1-9aac-408a-bde4-e16a2f47fb24.png#width=826" alt><br>YOLO2 在准确度上比YOLO 好，且追上什至超越其他模型像是Faster R-CNN 或者SSD 等，速度还是别人的2–10 倍以上。</p>
<p>YOLO2 采用了许多改善方式，例如batch normalization、anchor box 等，使用了这些改良方式让YOLO2 不管在辨识速度还是准确率上都有了提升，此外对于不同图档大小也有很好的相容性，提供了在速度与准确性上很好的平衡，所以也很适合运用在一些便宜的GPU 或者CPU 上，依旧提供水准以上的速度与准确率。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205442472-9616bcc1-7bd3-4ecf-a655-b4ec3dde4948.png#width=826" alt><br><a name="ccdb"></a></p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a><a href="#ccdb"></a>结语</h3><p>物体辨识(Object detection)的进展飞快，为了整理这篇大概也看了七八篇论文，还有很多都还没涵盖到的，例如SSD ( <a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">Single Shot Mulitbox Detector</a> )。如果想更了解AI在Computer Vision最近几年的发展，也可以参考这篇<a href="http://www.themtank.org/a-year-in-computer-vision" target="_blank" rel="noopener">搜文</a> <a href="http://www.themtank.org/a-year-in-computer-vision" target="_blank" rel="noopener">A Year in Computer vision</a>，内容涵盖了Classification、Object detection、Object tracking、Segmentation、Style transfer、Action recognition、3D object、Human post recognition等等，看完会大致知道在Computer Vision中有哪些AI所做的努力，以及各自的进展。</p>
<p>Google的Tensorflow也有提供<a href="https://github.com/tensorflow/models/tree/master/research/object_detection" target="_blank" rel="noopener">Object detection API</a>，透过使用API ，不用理解这些模型的实作也能快速实作出速度不错涵盖率又广的object detection。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205466794-6c46e6e8-6b5c-4375-ab82-46fb5c460706.png#width=826" alt></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2019 Zhos
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br>1、请确保node版本大于6.2<br>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br> npm i hexo-generator-json-content --save<br><br>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接1</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接2</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接3</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接4</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接5</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接6</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>