<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="链接在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。 批量归一化的直观解释 训练中的问题问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。不同批次分布">
<meta property="og:type" content="article">
<meta property="og:title" content="认识和实现：批量标准化">
<meta property="og:url" content="http://zhos.me/2018/12/19/yuque/认识和实现：批量标准化/index.html">
<meta property="og:site_name" content="A+">
<meta property="og:description" content="链接在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。 批量归一化的直观解释 训练中的问题问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。不同批次分布">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230849299-89520b64-5633-46b5-a7ab-8343e37bce6d.png#width=731">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230974089-30772f12-78be-446d-ac94-1f658676b64c.png#width=731">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231047448-2503dee7-466f-4472-a4d3-d4051a58af2f.png#width=747">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231155587-c517033a-5650-4858-8dc8-fe71e96bb8f0.png#width=204">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231189951-b72be3c8-1a47-42a5-bd6d-04e2ba9a6006.png#width=215">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231250609-5da7503e-4334-4d00-9e24-61689bab04a2.png#width=231">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231717659-060d4678-06bf-4045-abc1-cd7ce7cb6c77.png#width=743">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231810973-aebf7896-6c50-4d6c-8372-702891142c6f.png#width=724">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231864866-451c1cd9-d6ae-4f7e-9aa2-1da0b34f889b.png#width=737">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231931611-3c026d90-61b0-41df-b4dc-7a538bf01ce6.png#width=730">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231970491-afc0113d-a1fb-4218-90d5-7104d35cb98b.png#width=724">
<meta property="og:updated_time" content="2019-04-04T09:28:56.289Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="认识和实现：批量标准化">
<meta name="twitter:description" content="链接在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。 批量归一化的直观解释 训练中的问题问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。不同批次分布">
<meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230849299-89520b64-5633-46b5-a7ab-8343e37bce6d.png#width=731">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zhos.me/2018/12/19/yuque/认识和实现：批量标准化/">





  <title>认识和实现：批量标准化 | A+</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A+</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">武德</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/认识和实现：批量标准化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">认识和实现：批量标准化</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T22:43:17+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b" target="_blank" rel="noopener">链接</a><br>在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。<br><a name="worndf"></a></p>
<h3 id="批量归一化的直观解释"><a href="#批量归一化的直观解释" class="headerlink" title="批量归一化的直观解释"></a><a href="#worndf"></a>批量归一化的直观解释</h3><p><a name="iegmed"></a></p>
<h3 id="训练中的问题"><a href="#训练中的问题" class="headerlink" title="训练中的问题"></a><a href="#iegmed"></a>训练中的问题</h3><p>问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。<br>不同批次分布的另一个影响是消失的梯度。 消失的梯度问题是一个大问题，特别是对于S形激活函数。 如果g（x）表示sigmoid激活函数，则为| x | 增加，g’（x）趋于零。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230849299-89520b64-5633-46b5-a7ab-8343e37bce6d.png#width=731" alt><br>问题2.当输入分布变化时，神经元输出也会变化。 这导致神经元输出偶尔波动到S形函数的可饱和区域。 在那里，神经元既不能更新自己的权重，也不能将梯度传递回先前的层。 我们如何保持神经元输出不变为可饱和区域？</p>
<p>如果我们可以将神经元输出限制在零附近的区域，我们可以确保每个层在反向传播期间都会传回一个实质的梯度。 这将导致更快的训练时间和更准确的结果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545230974089-30772f12-78be-446d-ac94-1f658676b64c.png#width=731" alt><br><a name="laeagu"></a></p>
<h4 id="批量标准作为解决方案。"><a href="#批量标准作为解决方案。" class="headerlink" title="批量标准作为解决方案。"></a><a href="#laeagu"></a>批量标准作为解决方案。</h4><p>批量标准化减轻了不同层输入的影响。 通过归一化神经元的输出，激活函数将仅接收接近零的输入。 这确保了非消失的梯度，解决了第二个问题。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231047448-2503dee7-466f-4472-a4d3-d4051a58af2f.png#width=747" alt><br>批量归一化将层输出转换为单位高斯分布。 当这些输出通过激活功能馈送时，层激活也将变得更加正常分布。<br>由于一层的输出是下一层的输入，因此层输入现在具有明显较小的批次间差异。 通过减少层输入的变化分布，我们解决了第一个问题。<br><a name="6w4qpz"></a></p>
<h3 id="数学解释"><a href="#数学解释" class="headerlink" title="数学解释"></a><a href="#6w4qpz"></a>数学解释</h3><p>通过批量归一化，我们为每个激活函数寻找以零为中心的单位方差分布。 在训练期间，我们采用激活输入x并将其减去批次均值μ以实现零中心分布。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231155587-c517033a-5650-4858-8dc8-fe71e96bb8f0.png#width=204" alt><br>接下来，我们取x并将其除以批量方差和一个小数，以防止除以零σ+ε。 这可确保所有激活输入分布都具有单位差异。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231189951-b72be3c8-1a47-42a5-bd6d-04e2ba9a6006.png#width=215" alt><br>最后，我们将x hat进行线性转换以缩放并移动批量标准化的输出。 尽管在反向传播期间网络发生了变化，但仍能确保保持这种正常化效果。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231250609-5da7503e-4334-4d00-9e24-61689bab04a2.png#width=231" alt><br>在测试模型时，我们不使用批处理均值或方差，因为这会破坏模型。 （提示：单个观察的平均值和方差是多少？）相反，我们计算训练群体的移动平均值和方差估计值。 这些估计值是培训期间计算的所有批次平均值和方差的平均值。<br><a name="kg00mo"></a></p>
<h3 id="批量标准化的好处"><a href="#批量标准化的好处" class="headerlink" title="批量标准化的好处"></a><a href="#kg00mo"></a>批量标准化的好处</h3><p>批量标准化的好处如下。<br>1.有助于防止具有可饱和非线性（sigmoid，tanh等）的网络中的消失梯度<br>通过批量标准化，我们确保任何激活函数的输入不会变为可饱和区域。 批量归一化将这些输入的分布转换为单位高斯（零中心和单位方差）。<br>2.规范模型<br>也许。 Ioffe和Svegeddy提出了这一主张，但没有就此问题进行广泛撰写。 也许这是归一化层输入的结果？<br>3.允许更高的学习率<br>通过在训练期间防止梯度消失的问题，我们可以设置更高的学习率。 批量标准化还降低了对参数标度的依赖性。 较大的学习速率可以增加层参数的规模，这导致梯度在反向传播期间回传时放大。 我需要阅读更多关于此的内容。<br><a name="5qelzv"></a></p>
<h3 id="在Keras实施"><a href="#在Keras实施" class="headerlink" title="在Keras实施"></a><a href="#5qelzv"></a>在Keras实施</h3><p>引入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import keras</span><br><span class="line">from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.image as mpimg</span><br><span class="line"></span><br><span class="line">from keras.models import Model, Sequential</span><br><span class="line">from keras.layers import Input</span><br><span class="line"></span><br><span class="line">from keras.callbacks import ModelCheckpoint, EarlyStopping</span><br><span class="line">from keras.layers import BatchNormalization</span><br><span class="line">from keras.layers import GlobalAveragePooling2D</span><br><span class="line">from keras.layers import Activation</span><br><span class="line">from keras.layers import Conv2D, MaxPooling2D, Dense</span><br><span class="line">from keras.layers import MaxPooling2D, Dropout, Flatten</span><br><span class="line"></span><br><span class="line">import time</span><br></pre></td></tr></table></figure></p>
<p>数据加载和预处理<br>在这笔记本中，我们使用Cifar 100数据集，因为它具有相当的挑战性，并且不会永远用于训练。 唯一的预处理是零中心和图像变化发生器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from keras.datasets import cifar100</span><br><span class="line">from keras.utils import np_utils</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=&apos;fine&apos;)</span><br><span class="line"></span><br><span class="line">#scale and regularize the dataset</span><br><span class="line">x_train = (x_train-np.mean(x_train))</span><br><span class="line">x_test = (x_test - x_test.mean())</span><br><span class="line"></span><br><span class="line">x_train = x_train.astype(&apos;float32&apos;)</span><br><span class="line">x_test = x_test.astype(&apos;float32&apos;)</span><br><span class="line"></span><br><span class="line">#onehot encode the target classes</span><br><span class="line">y_train = np_utils.to_categorical(y_train)</span><br><span class="line">y_test = np_utils.to_categorical(y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">        shear_range=0.2,</span><br><span class="line">        zoom_range=0.2,</span><br><span class="line">        horizontal_flip=True)</span><br><span class="line"></span><br><span class="line">train_datagen.fit(x_train)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow(x_train,</span><br><span class="line">                                     y = y_train,</span><br><span class="line">                                    batch_size=80,)</span><br></pre></td></tr></table></figure></p>
<p><a name="kl8xlg"></a></p>
<h3 id="在Keras中构建模型"><a href="#在Keras中构建模型" class="headerlink" title="在Keras中构建模型"></a><a href="#kl8xlg"></a>在Keras中构建模型</h3><p>我们的架构将包括堆叠的3x3卷积，然后是最大池化和dropout。 每个网络中有5个卷积块。 最后一层是一个完全连接的层，有100个节点和softmax激活。<br>我们将构建4个不同的卷积网络，每个网络都具有sigmoid或ReLU激活以及批量标准化或不标准化。 我们将比较每个网络的验证损失。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">def conv_block_first(model, bn=True, activation=&quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    The first convolutional block in each architecture. Only    separate so we can specify the input shape.</span><br><span class="line">    &quot;&quot;&quot;    </span><br><span class="line">   #First Stacked Convolution</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;, input_shape =   x_train.shape[1:]))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line">    #Second Stacked Convolution</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(MaxPooling2D())</span><br><span class="line">    model.add(Dropout(0.15))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def conv_block(model, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Generic convolutional block with 2 stacked 3x3 convolutions, max pooling, dropout, </span><br><span class="line">    and an optional Batch Normalization.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(60,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(MaxPooling2D())</span><br><span class="line">    model.add(Dropout(0.15))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def conv_block_final(model, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    I bumped up the number of filters in the final block. I made this separate so that I might be able to integrate Global Average Pooling later on. </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Conv2D(100,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(100,3, padding = &quot;same&quot;))</span><br><span class="line">    if bn:</span><br><span class="line">        model.add(BatchNormalization())</span><br><span class="line">    model.add(Activation(activation))</span><br><span class="line"></span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def fn_block(model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    I&apos;m not going for a very deep fully connected block, mainly so I can save on memory.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.add(Dense(100, activation = &quot;softmax&quot;))</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def build_model(blocks=3, bn=True, activation = &quot;sigmoid&quot;):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Builds a sequential network based on the specified parameters.</span><br><span class="line">    </span><br><span class="line">    blocks: number of convolutional blocks in the network, must be greater than 2.</span><br><span class="line">    bn: whether to include batch normalization or not.</span><br><span class="line">    activation: activation function to use throughout the network.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model = Sequential()</span><br><span class="line"></span><br><span class="line">    model = conv_block_first(model, bn=bn, activation=activation)</span><br><span class="line"></span><br><span class="line">    for block in range(1,blocks-1):</span><br><span class="line">        model = conv_block(model, bn=bn, activation = activation)</span><br><span class="line"></span><br><span class="line">    model = conv_block_final(model, bn=bn, activation=activation)</span><br><span class="line">    model = fn_block(model)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">def compile_model(model, optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;]):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compiles a neural network.</span><br><span class="line">    </span><br><span class="line">    model: the network to be compiled.</span><br><span class="line">    optimizer: the optimizer to use.</span><br><span class="line">    loss: the loss to use.</span><br><span class="line">    metrics: a list of keras metrics.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model.compile(optimizer = optimizer,</span><br><span class="line">                 loss = loss,</span><br><span class="line">                 metrics = metrics)</span><br><span class="line">    return model</span><br><span class="line">#COMPILING THE 4 MODELS</span><br><span class="line">sigmoid_without_bn = build_model(blocks = 5, bn=False, activation = &quot;sigmoid&quot;)</span><br><span class="line">sigmoid_without_bn = compile_model(sigmoid_without_bn)</span><br><span class="line"></span><br><span class="line">sigmoid_with_bn = build_model(blocks = 5, bn=True, activation = &quot;sigmoid&quot;)</span><br><span class="line">sigmoid_with_bn = compile_model(sigmoid_with_bn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">relu_without_bn = build_model(blocks = 5, bn=False, activation = &quot;relu&quot;)</span><br><span class="line">relu_without_bn = compile_model(relu_without_bn)</span><br><span class="line"></span><br><span class="line">relu_with_bn = build_model(blocks = 5, bn=True, activation = &quot;relu&quot;)</span><br><span class="line">relu_with_bn = compile_model(relu_with_bn)</span><br></pre></td></tr></table></figure></p>
<p><a name="c4xpfd"></a></p>
<h3 id="模特训练"><a href="#模特训练" class="headerlink" title="模特训练"></a><a href="#c4xpfd"></a>模特训练</h3><p>没有批量标准化的Sigmoid<br>训练陷入困境。 有100个课程，这个模型从未达到比随机猜测更好的性能（10％准确度）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history1 = sigmoid_without_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        epochs=20,</span><br><span class="line">        verbose=0,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231717659-060d4678-06bf-4045-abc1-cd7ce7cb6c77.png#width=743" alt><br><a name="m0uloq"></a></p>
<h3 id="具有批量标准化的Sigmoid"><a href="#具有批量标准化的Sigmoid" class="headerlink" title="具有批量标准化的Sigmoid"></a><a href="#m0uloq"></a>具有批量标准化的Sigmoid</h3><p>与没有批量标准化不同，该模型在训练期间开始实施。 这可能是批量标准化减轻消失梯度的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history2 = sigmoid_with_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        verbose=0,</span><br><span class="line">        epochs=20,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231810973-aebf7896-6c50-4d6c-8372-702891142c6f.png#width=724" alt><br><a name="w5h1ml"></a></p>
<h3 id="没有批量标准化的ReLU"><a href="#没有批量标准化的ReLU" class="headerlink" title="没有批量标准化的ReLU"></a><a href="#w5h1ml"></a>没有批量标准化的ReLU</h3><p>在没有批量规范的情况下实施ReLU导致一些初始收益，然后收敛到非最优的局部最小值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history3 = relu_without_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        epochs=20,</span><br><span class="line">        verbose=0,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231864866-451c1cd9-d6ae-4f7e-9aa2-1da0b34f889b.png#width=737" alt><br>具有批量标准化的ReLU<br>与sigmoid模型一样，批量标准化提高了该网络的训练能力。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history4 = relu_with_bn.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=2000,</span><br><span class="line">        verbose=0,</span><br><span class="line">        epochs=20,</span><br><span class="line">        validation_data=(x_test, y_test),</span><br><span class="line">        callbacks = [model_checkpoint])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231931611-3c026d90-61b0-41df-b4dc-7a538bf01ce6.png#width=730" alt><br><a name="e5gkmq"></a></p>
<h3 id="比较架构"><a href="#比较架构" class="headerlink" title="比较架构"></a><a href="#e5gkmq"></a>比较架构</h3><p>我们在这里清楚地看到批量标准化的好处。 没有批量标准化的ReLU和S形模型都无法保持训练性能提升。 这可能是渐变消失的结果。 具有批量标准化的体系结构训练得更快，并且比没有批量标准化的体系结构表现更好。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545231970491-afc0113d-a1fb-4218-90d5-7104d35cb98b.png#width=724" alt><br><a name="0457"></a></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><a href="#0457"></a>Conclusion</h3><p>结论<br>批量标准化减少了训练时间并提高了神经网络的稳定性。 此效果适用于sigmoid和ReLU激活功能。 原帖可以在我的<a href="https://www.harrisonjansma.com/" target="_blank" rel="noopener">网站</a>上找到，代码可以在我的<a href="https://github.com/harrisonjansma/Research-Computer-Vision/tree/master/07-28-18-Implementing-Batch-Norm" target="_blank" rel="noopener">GitHub</a>上找到。<br><a name="f6b6"></a></p>
<h3 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a><a href="#f6b6"></a>Resources</h3><ul>
<li><p>Original paper by Ioffe and Szegedy. <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">here.</a></p>
</li>
<li><p>Insert a batch normalization before or after nonlinearities? <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="noopener">Usage explanation</a></p>
</li>
<li><p>For an explanation of the math and implementation in TensorFlow. <a href="https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8" target="_blank" rel="noopener">Pitfalls of Batch Norm</a></p>
</li>
<li><p>Also this post <a href="https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73" target="_blank" rel="noopener">How to use Batch Normalization with TensorFlow and tf.keras</a></p>
</li>
</ul>
<p><a name="3277"></a></p>
<h3 id="Further-reading"><a href="#Further-reading" class="headerlink" title="Further reading"></a><a href="#3277"></a>Further reading</h3><p>Below are some more recent research papers that extend Ioffe and Svegedy’s work.<br><a href="https://arxiv.org/abs/1702.03275v2" target="_blank" rel="noopener">[1]</a> How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)<br><a href="https://arxiv.org/abs/1702.03275v2" target="_blank" rel="noopener">[2]</a> Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models<br><a href="https://arxiv.org/abs/1607.06450v1" target="_blank" rel="noopener">[3]</a> Layer Normalization<br><a href="https://arxiv.org/abs/1602.07868v3" target="_blank" rel="noopener">[4]</a> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks<br><a href="https://arxiv.org/abs/1803.08494v3" target="_blank" rel="noopener">[5]</a> Group Normalization</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/19/yuque/不要在卷积网络中使用Dropout/" rel="next" title="不要在卷积网络中使用Dropout">
                <i class="fa fa-chevron-left"></i> 不要在卷积网络中使用Dropout
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/20/yuque/XGBoost不是黑魔法/" rel="prev" title="XGBoost不是黑魔法">
                XGBoost不是黑魔法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#批量归一化的直观解释"><span class="nav-number">1.</span> <span class="nav-text">批量归一化的直观解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练中的问题"><span class="nav-number">2.</span> <span class="nav-text">训练中的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#批量标准作为解决方案。"><span class="nav-number">2.1.</span> <span class="nav-text">批量标准作为解决方案。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数学解释"><span class="nav-number">3.</span> <span class="nav-text">数学解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#批量标准化的好处"><span class="nav-number">4.</span> <span class="nav-text">批量标准化的好处</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在Keras实施"><span class="nav-number">5.</span> <span class="nav-text">在Keras实施</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在Keras中构建模型"><span class="nav-number">6.</span> <span class="nav-text">在Keras中构建模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模特训练"><span class="nav-number">7.</span> <span class="nav-text">模特训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#具有批量标准化的Sigmoid"><span class="nav-number">8.</span> <span class="nav-text">具有批量标准化的Sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#没有批量标准化的ReLU"><span class="nav-number">9.</span> <span class="nav-text">没有批量标准化的ReLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#比较架构"><span class="nav-number">10.</span> <span class="nav-text">比较架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conclusion"><span class="nav-number">11.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Resources"><span class="nav-number">12.</span> <span class="nav-text">Resources</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Further-reading"><span class="nav-number">13.</span> <span class="nav-text">Further reading</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhos</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
