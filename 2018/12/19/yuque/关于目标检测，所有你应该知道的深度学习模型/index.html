<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="链接  Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO这篇是简介一些用来辨识影像中物体的AI 模型。 在前面有提到，透过CNN模型，你可以输入一张图片，得到该图片属于哪种类别的结果，这过程我们把他称作分类(Classification)。 但在真实世界的应用情境通常">
<meta property="og:type" content="article">
<meta property="og:title" content="关于目标检测，所有你应该知道的深度学习模型">
<meta property="og:url" content="http://zhos.me/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/index.html">
<meta property="og:site_name" content="武德">
<meta property="og:description" content="链接  Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO这篇是简介一些用来辨识影像中物体的AI 模型。 在前面有提到，透过CNN模型，你可以输入一张图片，得到该图片属于哪种类别的结果，这过程我们把他称作分类(Classification)。 但在真实世界的应用情境通常">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204251780-a74eec91-8d1c-42c3-b27a-da11667ac2c3.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204292906-82d75e3e-ada2-4402-8a3e-e1f746718a78.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204317496-a09e245d-e3f5-4fa9-9e62-3fbcd048e2c9.png#width=554">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204363747-83bbc0c6-5f23-4b0c-bae2-3d3d18561a34.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204447478-48e75d73-6502-4714-a7e2-b1064c212381.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204932699-604244a0-ec32-4161-b825-10e50a4d088b.png#width=800">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204990054-48efb89e-132f-444c-938d-7e4441882e73.png#width=650">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205011580-f995c601-05b6-4d1c-9958-ca8d13714c32.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205050916-f7cefd4c-be49-44ba-8786-bb25170f7e4d.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205073642-70d8970e-7b91-4772-93fe-4c805439a274.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205110140-81193d10-5537-493b-9d73-b9b4353ac2d1.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205127937-663202b8-0a40-43e5-a833-7854a202f6f9.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205148755-2522b72b-9b05-4e9b-af6f-ef024168767e.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205172687-cd927423-a678-4557-8914-e933152be762.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205202715-768d38b1-ac5b-48f7-91fd-35ef7abb8a64.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205230066-e554096e-51e2-492f-8e1d-fbd2fd312f9d.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205319660-33410751-7339-4534-a37e-f2d61752ae1b.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205351976-8c342184-e75a-4ac0-add0-a2e06853c86d.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205373222-774ef6c0-1da2-49fa-a9c6-1aa802765f10.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205389614-e1f9cac9-d060-463b-b15f-cae65400191c.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205410945-959e3ef1-9aac-408a-bde4-e16a2f47fb24.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205442472-9616bcc1-7bd3-4ecf-a655-b4ec3dde4948.png#width=826">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205466794-6c46e6e8-6b5c-4375-ab82-46fb5c460706.png#width=826">
<meta property="og:updated_time" content="2019-04-03T14:32:19.983Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="关于目标检测，所有你应该知道的深度学习模型">
<meta name="twitter:description" content="链接  Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO这篇是简介一些用来辨识影像中物体的AI 模型。 在前面有提到，透过CNN模型，你可以输入一张图片，得到该图片属于哪种类别的结果，这过程我们把他称作分类(Classification)。 但在真实世界的应用情境通常">
<meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204251780-a74eec91-8d1c-42c3-b27a-da11667ac2c3.png#width=826">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zhos.me/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/">





  <title>关于目标检测，所有你应该知道的深度学习模型 | 武德</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">武德</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">关于目标检测，所有你应该知道的深度学习模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T15:22:45+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://medium.com/@syshen/%E7%89%A9%E9%AB%94%E5%81%B5%E6%B8%AC-object-detection-740096ec4540" target="_blank" rel="noopener">链接</a></p>
<p><a name="e60a"></a></p>
<h3 id="Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO"><a href="#Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO" class="headerlink" title="Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO"></a><a href="#e60a"></a>Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO</h3><p>这篇是简介一些用来辨识影像中物体的AI 模型。</p>
<p>在前面有提到，透过CNN模型，你可以输入一张图片，<a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5" target="_blank" rel="noopener">得到该图片属于哪种类别的结果</a>，这过程我们把他称作分类(Classification)。</p>
<p>但在真实世界的应用情境通常要从一张图片中辨识所有出现的物体， 并且标示出位置来(标出位置称之为Object Localization)。你一定在网路上看过类似底下的影片，这段影片可以看出中国闭路摄影机(CCTV)发展的概况，不只是可以框出影像中每个物件，辨别物件种类，侦测出移动物体的动量，甚至是人脸辨识，实现楚门世界的恶梦。要做到这就需要靠深度学习中的Object Detection 演算法，这也是最近几年来深度学习最蓬勃发展的一块领域。<br><a href="https://youtu.be/aE1kA0Jy0Xg" target="_blank" rel="noopener">https://youtu.be/aE1kA0Jy0Xg</a><br>基本的想法是，既然CNN 对于物体的分类又快又好，那我们可不可以拿CNN 来扫描并辨识图片中的任何物体？答案当然是 — 可以。</p>
<p>最简单的作法就是用Sliding Windows 的概念，也就是用一个固定大小的框框，逐一的扫过整张图片，每次框出来的图像丢到CNN 中去判断类别。由于物体的大小是不可预知的，所以还要用不同大小的框框去侦测。但是Sliding Window 是非常暴力的作法，对单一影像我们需要扫描非常多次，每扫一次都需要算一次CNN，这将会耗费大量的运算资源，而且速度慢，根本无法拿来应用！<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204251780-a74eec91-8d1c-42c3-b27a-da11667ac2c3.png#width=826" alt></p>
<p>所以后来就有人提出了R-CNN (Regions with CNN)<br><a name="68d0"></a></p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="#68d0"></a>R-CNN</h3><p>与其用Sliding Window的方式扫过一轮，<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a>的作法是预先筛选出约2000个可能的区域，再将这2000区域个别去作分类，所以他的演算法流程如下：</p>
<ol>
<li><p>产生一群约2000 个可能的区域(Region Proposals)</p>
</li>
<li><p>经由一个预先训练好的CNN 模型如AlexNet 撷取特征，将结果储存起来。</p>
</li>
<li><p>然后再以SVM (Support Vector Machine) 分类器来区分是否为物体或者背景。</p>
</li>
<li><p>最后经由一个线性回归模型来校正bounding box 位置。</p>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204292906-82d75e3e-ada2-4402-8a3e-e1f746718a78.png#width=826" alt><br><a name="9185"></a></p>
<h4 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a><a href="#9185"></a>Selective Search</h4><p>R-CNN用来筛选Region Proposals的方法称之为<a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="noopener">Selective Search</a>，而Selective Search又是基于Felzenszwal于2004年发表的论文<a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf" target="_blank" rel="noopener">Graph Base Image Segmentation</a>。</p>
<p>图像经由Graph Base Image Segmentation 可以切出数个Segment 来，如下图：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204317496-a09e245d-e3f5-4fa9-9e62-3fbcd048e2c9.png#width=554" alt><br>而Selective Search 的作法是将Segment 的结果先各自画出bounding box，然后以一个回圈，每次合并相似度最高的两个box，直到整张图合并成单一个box 为止，在这过程中的所有box 便是selective search 出来的region proposals。Selective Search 的演算法如下：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204363747-83bbc0c6-5f23-4b0c-bae2-3d3d18561a34.png#width=826" alt><br>取自Selective Search 论文。先以Graph base image segmentation 取得一些区域，计算每个区域间的相似度，每次合并相似度最高的两个区域，直到整张图片成为单一区域为止。<br>但是R-CNN 存在一些问题，速度仍然不够快：</p>
<ol>
<li><p>R-CNN 一开始必须先产生约2000 个区域，每个区域都要丢进CNN 中去撷取特征，所以需要跑过至少2000 次的CNN</p>
</li>
<li><p>R-CNN 的model 是分开成三部份，分别是用来取出特征的CNN，分类的SVM，以及优化bounding box 的线性回归。所以R-CNN 不容易作训练。</p>
</li>
</ol>
<p>所以R-CNN的其中一个作者<a href="http://www.rossgirshick.info/" target="_blank" rel="noopener">Ross Girshick (RBG大神)</a>在2015年又提出了一个改良版本，并称之为Fast R-CNN。<br><a name="1250"></a></p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="#1250"></a>Fast R-CNN</h3><p><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a>的想法很简单，在R-CNN中，2000多个区域都要个别去运算CNN，这些区域很多都是重叠的，也就是说这些重叠区域的CNN很多都是重复算的。所以Fast R-CNN的原则就是全部只算一次CNN就好，CNN撷取出来的特征可以让这2000多个区域共用！</p>
<p>Fast R-CNN 采用的作法就是RoIPooling (Region of Interest Pooling)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204447478-48e75d73-6502-4714-a7e2-b1064c212381.png#width=826" alt><br>Fast RCNN 一样要预选Region proposals，但是只做一次CNN。在跑完Convolution layers 的最后一层时，会得到一个HxW 的feature map，同时也要将region proposals 对应到HxW 上，然后在feature map 上取各自region 的MaxPooling，每个region 会得到一个相同大小的矩阵(例如2x2)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204932699-604244a0-ec32-4161-b825-10e50a4d088b.png#width=800" alt><br>from <a href="https://blog.deepsense.ai/region-of-interest-pooling-explained/" target="_blank" rel="noopener">https://blog.deepsense.ai/region-of-interest-pooling-explained/</a><br>然后各自连接上FC 网路，以及softmax 去作分类。在分类的同时也作bounding box 的线性回归运算。</p>
<p>Fast RCNN 的优点是：</p>
<ol>
<li><p>只需要作一次CNN，有效解省运算时间</p>
</li>
<li><p>使用单一网络，简化训练过程</p>
</li>
</ol>
<p><a name="fcaf"></a></p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="#fcaf"></a>Faster R-CNN</h3><p>不管是R-CNN还是Fast R-CNN都还是要先透过selective search预选region proposals，这是一个缓慢的步骤。在2015年时，Microsoft的<a href="http://shaoqingren.com/" target="_blank" rel="noopener">Shaoqing Ren</a> , <a href="http://kaiminghe.com/" target="_blank" rel="noopener">Kaiming He</a> , <a href="http://www.rossgirshick.info/" target="_blank" rel="noopener">Ross Girshick</a> ,以及<a href="http://www.jiansun.org/" target="_blank" rel="noopener">Jian Sun</a>提出了<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>，一个更快的R-CNN。<br>Faster R-CNN 的想法也很直觉，与其预先筛选region proposals，到不如从CNN 的feature map 上选出region proposals。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545204990054-48efb89e-132f-444c-938d-7e4441882e73.png#width=650" alt><br><a name="fef5"></a></p>
<h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a><a href="#fef5"></a>Region Proposal Network</h4><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205011580-f995c601-05b6-4d1c-9958-ca8d13714c32.png#width=826" alt><br>RPN (Region Proposal Network) 也是一个Convolution Network，Input 是之前CNN 输出的feature map，输出是一个bounding box 以及该bounding box 包含一个物体的机率。</p>
<p>RPN 在feature map 上取sliding window，每个sliding window 的中心点称之为anchor point，然后将事先准备好的k 个不同尺寸比例的box 以同一个anchor point 去计算可能包含物体的机率(score) ，取机率最高的box。这k 个box 称之为anchor box。所以每个anchor point 会得到2k 个score，以及4k 个座标位置(box 的左上座标，以及长宽，所以是4 个数值)。在Faster R-CNN 论文里，预设是取3 种不同大小搭配3 种不同长宽比的anchor box，所以k 为3x3 = 9 。</p>
<p>经由RPN 之后，我们便可以得到一些最有可能的bounding box，虽然这些bounding box 不见得精确，但是透过类似于Fast RCNN 的RoIPooling， 一样可以很快的对每个region 分类，并找到最精确的bounding box 座标。<br><a name="83ca"></a></p>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a><a href="#83ca"></a>Mask R-CNN</h3><p>前述几个方法都是在找到物体外围的bounding box，bounding box基本上都是方形，另外一篇有趣的论文是Facebook AI researcher <a href="http://kaiminghe.com/" target="_blank" rel="noopener">Kaiming He</a>所提出的<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>，透过Mask R-CNN不只是找到bounding box，可以做到接近pixel level的遮罩(图像分割Image segmentation)。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205050916-f7cefd4c-be49-44ba-8786-bb25170f7e4d.png#width=826" alt><br>要了解Mask R-CNN 如何取遮罩，要先看一下FCN (Fully Convolutional Network)</p>
<p><a name="28c4"></a></p>
<h4 id="FCN-Fully-Convolutional-Network-for-Image-Segmentation"><a href="#FCN-Fully-Convolutional-Network-for-Image-Segmentation" class="headerlink" title="FCN (Fully Convolutional Network) for Image Segmentation"></a><a href="#28c4"></a>FCN (Fully Convolutional Network) for Image Segmentation</h4><p>有别于CNN 网络最后是连上一个全连接(Fully Connected)的网络，FCN (Fully Convolutional Network)最后接上的是一个卷积层。一般的CNN 只能接受固定大小的Input，但是FCN 则能接受任何大小的Input，例如W x H 。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205073642-70d8970e-7b91-4772-93fe-4c805439a274.png#width=826" alt><br><em>图上方一般的CNN 网络，只能接受大小固定的输入，得到单一维度的输出，分别代表每个类别的机率。图下则是FCN 网路，最后两层由卷积取代，输出为hxwx 1000，代表每个pixel 种类的机率，可以视为一个heapmap。</em></p>
<p>在CNN 的过程中会一直作downsampling，所以FCN 最后的输出可能为H/32 x W/32，实际上得到的会是一个像heapmap 的结果。但是由于这过程是downsampling，所以Segment 的结果是比较粗糙，为了让Segment 的效果更好，要再做upsampling，来补足像素。upsamping 的作法是取前面几层的结果来作差补运算。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205110140-81193d10-5537-493b-9d73-b9b4353ac2d1.png#width=826" alt><br>FCN 的结果会跟前面几层的输出作差补运算<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205127937-663202b8-0a40-43e5-a833-7854a202f6f9.png#width=826" alt><br>Mask R-CNN是建构于Faster R-CNN之上，如果是透过RoIPooling取得Region proposals之后，针对每个region会再跑FCN取得遮罩分割，但是由于RoIPooling在做Max pooling时，会使用最近插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Nearest Neighbor Interpolation</a> )取得数值，所以出来的遮罩会有偏移现象，再加上pooling下来的结果，会让region的尺寸出现非整数的情况，然后取整数的结果就是没办法做到Pixel层级的遮罩。所以Mask R-CNN改采用双线性插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Bilinear Interpolation</a> )来改善RoIPooling，称之为RoIAlign，RoIAlign会让遮罩位置更准确。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205148755-2522b72b-9b05-4e9b-af6f-ef024168767e.png#width=826" alt><br>Mask RCNN 架构，将原有的RoIPooling 改成 RoIAlign。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205172687-cd927423-a678-4557-8914-e933152be762.png#width=826" alt><br>Fast R-CNN 的RoIPool。将一个7x5 的Anchor box 取2x2 的MaxPool，由于使用最近插值法，会有偏差。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205202715-768d38b1-ac5b-48f7-91fd-35ef7abb8a64.png#width=826" alt></p>
<p>RoIAlign的作法是使用双线性插值法( <a href="http://monkeycoding.com/?tag=nearest-neighbor-interpolation" target="_blank" rel="noopener">Bilinear Interpolation</a> )，减少mis-alignment的问题。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205230066-e554096e-51e2-492f-8e1d-fbd2fd312f9d.png#width=826" alt><br><a name="c62f"></a></p>
<h3 id="YOLO-You-Only-Look-Once"><a href="#YOLO-You-Only-Look-Once" class="headerlink" title="YOLO: You Only Look Once"></a><a href="#c62f"></a>YOLO: You Only Look Once</h3><p><a href="https://pjreddie.com/media/files/papers/yolo.pdf" target="_blank" rel="noopener">YOLO</a>有个很讨喜的名字，取自<a href="https://zh.wikipedia.org/wiki/YOLO" target="_blank" rel="noopener">You Only Live Once</a>，但用在Object detection上则为You only look once，意思是说YOLO模型的特性只需要对图片作一次CNN便能够判断里面的物体类别跟位置，大大提升辨识速度。</p>
<p>R-CNN 的概念是先提出几个可能包含物体的Region proposal，再针对每个region 使用CNN 作分类，最后再以regression 修正bounding box 位置，速度慢且不好训练。YOLO 的好处是单一网路设计，判断的结果会包含bounding box 位置，以及每个bounding box 所属类别及概率。整个网路设计是end-to-end 的，容易训练，而且速度快。</p>
<ol>
<li><p>YOLO 速度快，在Titan X GPU 上可以达到每秒45 祯的速度，简化版的YOLO 甚至可以达到150 fps 的速度。这意味着YOLO 已经可以对影像作即时运算了。准确度(mAP) 也狠甩其他深度学习模型好几条街。看看底下YOLO2 的demo 视频，这侦测速度会吓到吃手手了<a href="https://youtu.be/VOC3huqHrss" target="_blank" rel="noopener">https://youtu.be/VOC3huqHrss</a></p>
</li>
<li><p>有别于R-CNN 都是先提region 再做判断，看的范围比较小，容易将背景的background patch 看成物体。YOLO 在训练跟侦测时都是一次看整张图片，背景错误侦测率(background error, 抑或false positive) 都只有Fast R-CNN 的一半。</p>
</li>
<li><p>YOLO 的泛用性也比R-CNN 或者DPM 方式来得好很多，在新的domain 使用YOLO 依旧可以很稳定。<br>YOLO 的概念是将一张图片切割成S x S 个方格，每个方格以自己为中心点各自去判断B 个bounding boxes 中包含物体的confidence score 跟种类。<br>confidence score = Pr(Object) * IOU (ground truth)<br>如果该bounding box 不包含任何物体(Pr(Object) = 0)，confidence score 便为零，而IOU 则为bounding box 与ground truth 的交集面积，交集面积越大，分数越高。<br>每个方格预测的结果包含5 个数值，x 、y 、w 、 h 跟confidence，x 与y 是bounding box 的中间点，w 与h 是bounding box 的宽跟高。</p>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205319660-33410751-7339-4534-a37e-f2d61752ae1b.png#width=826" alt><br>S = 7，B = 2，PASCAL VOC label 20 种种类，所以tensor 为S x S x (5 * B + C) = 7 x 7 x 30<br>YOLO 的网路设计包含了24 个卷积层，跟2 层的FC 网络。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205351976-8c342184-e75a-4ac0-add0-a2e06853c86d.png#width=826" alt><br>另外一个版本的YOLO Fast 则只有9 个卷积层，不过最后的输出都是7x7x30 的tensor。</p>
<p><a name="e2e8"></a></p>
<h4 id="YOLO-的缺点"><a href="#YOLO-的缺点" class="headerlink" title="YOLO 的缺点"></a><a href="#e2e8"></a>YOLO 的缺点</h4><ol>
<li><p>由于YOLO 对于每个方格提两个bounding box 去作侦测，所以不容易去区分两个相邻且中心点又非常接近的物体</p>
</li>
<li><p>只有两种bounding box，所以遇到长宽比不常见的物体的检测率较差</p>
</li>
</ol>
<p><a name="5aaa"></a></p>
<h4 id="YOLO-与其他模型的比较"><a href="#YOLO-与其他模型的比较" class="headerlink" title="YOLO 与其他模型的比较"></a><a href="#5aaa"></a>YOLO 与其他模型的比较</h4><p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205373222-774ef6c0-1da2-49fa-a9c6-1aa802765f10.png#width=826" alt><br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205389614-e1f9cac9-d060-463b-b15f-cae65400191c.png#width=826" alt><br><a name="da63"></a></p>
<h3 id="YOLO2"><a href="#YOLO2" class="headerlink" title="YOLO2"></a><a href="#da63"></a>YOLO2</h3><p><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO2</a>建构于YOLO之上，但是有更好的准确度，更快速的判断速度，能够判断更多的物件种类(多达9000种)，所以是<strong>更好(Better)、更快(Faster)、更强大(Stronger)</strong>！<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205410945-959e3ef1-9aac-408a-bde4-e16a2f47fb24.png#width=826" alt><br>YOLO2 在准确度上比YOLO 好，且追上什至超越其他模型像是Faster R-CNN 或者SSD 等，速度还是别人的2–10 倍以上。</p>
<p>YOLO2 采用了许多改善方式，例如batch normalization、anchor box 等，使用了这些改良方式让YOLO2 不管在辨识速度还是准确率上都有了提升，此外对于不同图档大小也有很好的相容性，提供了在速度与准确性上很好的平衡，所以也很适合运用在一些便宜的GPU 或者CPU 上，依旧提供水准以上的速度与准确率。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205442472-9616bcc1-7bd3-4ecf-a655-b4ec3dde4948.png#width=826" alt><br><a name="ccdb"></a></p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a><a href="#ccdb"></a>结语</h3><p>物体辨识(Object detection)的进展飞快，为了整理这篇大概也看了七八篇论文，还有很多都还没涵盖到的，例如SSD ( <a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">Single Shot Mulitbox Detector</a> )。如果想更了解AI在Computer Vision最近几年的发展，也可以参考这篇<a href="http://www.themtank.org/a-year-in-computer-vision" target="_blank" rel="noopener">搜文</a> <a href="http://www.themtank.org/a-year-in-computer-vision" target="_blank" rel="noopener">A Year in Computer vision</a>，内容涵盖了Classification、Object detection、Object tracking、Segmentation、Style transfer、Action recognition、3D object、Human post recognition等等，看完会大致知道在Computer Vision中有哪些AI所做的努力，以及各自的进展。</p>
<p>Google的Tensorflow也有提供<a href="https://github.com/tensorflow/models/tree/master/research/object_detection" target="_blank" rel="noopener">Object detection API</a>，透过使用API ，不用理解这些模型的实作也能快速实作出速度不错涵盖率又广的object detection。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545205466794-6c46e6e8-6b5c-4375-ab82-46fb5c460706.png#width=826" alt></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/19/yuque/目标检测和定位算法的演变/" rel="next" title="目标检测和定位算法的演变">
                <i class="fa fa-chevron-left"></i> 目标检测和定位算法的演变
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/" rel="prev" title="理解二进制交叉熵、对数损失函数:一种可视化解释">
                理解二进制交叉熵、对数损失函数:一种可视化解释 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Computer-vision-object-detection-models-R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-YOLO"><span class="nav-number">1.</span> <span class="nav-text">Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#R-CNN"><span class="nav-number">2.</span> <span class="nav-text">R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Selective-Search"><span class="nav-number">2.1.</span> <span class="nav-text">Selective Search</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fast-R-CNN"><span class="nav-number">3.</span> <span class="nav-text">Fast R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-number">4.</span> <span class="nav-text">Faster R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Region-Proposal-Network"><span class="nav-number">4.1.</span> <span class="nav-text">Region Proposal Network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mask-R-CNN"><span class="nav-number">5.</span> <span class="nav-text">Mask R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FCN-Fully-Convolutional-Network-for-Image-Segmentation"><span class="nav-number">5.1.</span> <span class="nav-text">FCN (Fully Convolutional Network) for Image Segmentation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YOLO-You-Only-Look-Once"><span class="nav-number">6.</span> <span class="nav-text">YOLO: You Only Look Once</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#YOLO-的缺点"><span class="nav-number">6.1.</span> <span class="nav-text">YOLO 的缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#YOLO-与其他模型的比较"><span class="nav-number">6.2.</span> <span class="nav-text">YOLO 与其他模型的比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YOLO2"><span class="nav-number">7.</span> <span class="nav-text">YOLO2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结语"><span class="nav-number">8.</span> <span class="nav-text">结语</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhos</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
