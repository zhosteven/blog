<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="medium有人可能会争辩说，最好是过拟合你的模型，然后对其进行逆向工程而不是相反。在这个项目中，我们可以看到将Dropout正则化实现到神经网络后的准确性和验证损失的差异。 我们将使用PyTorch库从头开始构建一个顺序神经网络，以便在fashion-MNIST数据集中对10个不同的类进行分类。 这个数据集是28x28灰度图像的衣服。 我们将深入研究dropout的方法，并证明它是否能防止过度拟">
<meta property="og:type" content="article">
<meta property="og:title" content="去除正则化会避免模型过拟合吗？">
<meta property="og:url" content="http://zhos.me/2018/12/19/yuque/去除正则化会避免模型过拟合吗？/index.html">
<meta property="og:site_name" content="武德">
<meta property="og:description" content="medium有人可能会争辩说，最好是过拟合你的模型，然后对其进行逆向工程而不是相反。在这个项目中，我们可以看到将Dropout正则化实现到神经网络后的准确性和验证损失的差异。 我们将使用PyTorch库从头开始构建一个顺序神经网络，以便在fashion-MNIST数据集中对10个不同的类进行分类。 这个数据集是28x28灰度图像的衣服。 我们将深入研究dropout的方法，并证明它是否能防止过度拟">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545194059986-5d2850f6-9304-4568-b09c-e84e6bf6c37e.png#width=699">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545194731399-7541a6eb-2940-4587-9157-62c90ab37010.png#width=611">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545195322539-23690b05-a8bf-47b0-92ab-a35a3245d1a2.png#width=122">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545195544515-f2c011c6-a9ad-4673-a266-508907165f04.png#width=684">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545195769635-b5ee1591-723d-4cb7-a10b-2b0a8740d404.png#width=554">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545196342869-4eae3b68-560b-4427-8e2f-34dec5ae5da0.png#width=687">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545196385024-ee9585c8-819c-4da5-aaa1-10592b806d57.png#width=555">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545196482739-f2d874ee-fbb1-4c28-915c-d2c72a8db458.png#width=624">
<meta property="og:updated_time" content="2019-04-03T14:28:47.411Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="去除正则化会避免模型过拟合吗？">
<meta name="twitter:description" content="medium有人可能会争辩说，最好是过拟合你的模型，然后对其进行逆向工程而不是相反。在这个项目中，我们可以看到将Dropout正则化实现到神经网络后的准确性和验证损失的差异。 我们将使用PyTorch库从头开始构建一个顺序神经网络，以便在fashion-MNIST数据集中对10个不同的类进行分类。 这个数据集是28x28灰度图像的衣服。 我们将深入研究dropout的方法，并证明它是否能防止过度拟">
<meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2018/png/219582/1545194059986-5d2850f6-9304-4568-b09c-e84e6bf6c37e.png#width=699">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zhos.me/2018/12/19/yuque/去除正则化会避免模型过拟合吗？/">





  <title>去除正则化会避免模型过拟合吗？ | 武德</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">武德</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2018/12/19/yuque/去除正则化会避免模型过拟合吗？/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="武德">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">去除正则化会避免模型过拟合吗？</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T12:28:16+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://medium.com/@diazagasatya/will-dropout-regularization-prevents-your-model-to-overfit-11afa10cd4e0" target="_blank" rel="noopener">medium</a><br><em>有人可能会争辩说，最好是过拟合你的模型，然后对其进行逆向工程而不是相反。</em><br>在这个项目中，我们可以看到将Dropout正则化实现到神经网络后的准确性和验证损失的差异。 我们将使用PyTorch库从头开始构建一个顺序神经网络，以便在fashion-MNIST数据集中对10个不同的类进行分类。 这个数据集是28x28灰度图像的衣服。 我们将深入研究dropout的方法，并证明它是否能防止过度拟合。<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545194059986-5d2850f6-9304-4568-b09c-e84e6bf6c37e.png#width=699" alt><br>该项目的灵感来自：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Facebook Udacity PyTorch Challenge.</span><br></pre></td></tr></table></figure>
</blockquote>
<p>首先，我们将创建一个没有正则化实现的神经网络，我们的假设是我们可以推断，随着时间的推移，我们的模型在验证集中表现不佳，因为我们用训练集训练我们的模型越多， 通过对测试数据的特定特征进行分类则越好，从而创建不良的泛化模型去推理。<br><a name="dhgofx"></a></p>
<h4 id="让我们导入Fashion-MNIST数据集"><a href="#让我们导入Fashion-MNIST数据集" class="headerlink" title="让我们导入Fashion-MNIST数据集"></a><a href="#dhgofx"></a>让我们导入Fashion-MNIST数据集</h4><p>让我们使用torchvision下载数据集，通常我们将20％的数据集分开用于验证集。 但在这种情况下，我们将直接从torchvision下载数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> helper</span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), </span><br><span class="line">                                transforms.Normalize((<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>),</span><br><span class="line">                                (<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>))])</span><br><span class="line">traindataset = datasets.FashionMNIST(<span class="string">'~/.pytorch/F_MNIST_data/'</span>, download=<span class="literal">True</span>,train=<span class="literal">True</span>,transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(dataset=traindataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">testdataset = datasets.FashionMNIST(<span class="string">'~/.pytorch/F_MNIST_data/'</span>, download=<span class="literal">True</span>,train=<span class="literal">False</span>,transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(dataset=testdataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<p>我们需要导入torchvision来下载数据集和转换。 然后我们使用变换库将图像转换为张量并进行标准化。 通常批量训练和验证集以提高训练速度并且改组数据也会增加训练和测试数据的学习差异。<br>定义神经网络<br><br>这个模型将有2个隐藏层，输入层将有784个单元，并且在最终层将有10个输出，因为我们有10个不同的类进行分类。 我们将使用交叉熵损失，因为它具有对数性质，可以将我们的输出归一化到接近零或一。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">from torch.functional import F</span><br><span class="line">class FashionNeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # Create layers here</span><br><span class="line">        self.layer_input = nn.Linear(784,256)</span><br><span class="line">        self.layer_hidden_one = nn.Linear(256,128)</span><br><span class="line">        self.layer_hidden_two = nn.Linear(128,64)</span><br><span class="line">        self.layer_output = nn.Linear(64,10)</span><br><span class="line">    </span><br><span class="line">   def forward(self, x):</span><br><span class="line">        # Flattened the input to make sure it fits the layer input</span><br><span class="line">        x = x.view(x.shape[0],-1)</span><br><span class="line">        # Pass in the input to the layer and do forward propagation</span><br><span class="line">        x = F.relu(self.layer_input(x))</span><br><span class="line">        x = F.relu(self.layer_hidden_one(x))</span><br><span class="line">        x = F.relu(self.layer_hidden_two(x))</span><br><span class="line">        # Dimension = 1</span><br><span class="line">        x = F.log_softmax(self.layer_output(x),dim=1)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure></p>
<p>该神经网络将使用ReLU作为隐藏层的非线性激活函数，并使用log-softmax激活输出和负对数似然函数用于我们的损失函数。 如果我们查看PyTorch库中的<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener">交叉熵损失</a>的文档，该标准将nn.LogSoftmax()和nn.NLLLoss()组合在一个单独的类中。 损失可以描述为：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545194731399-7541a6eb-2940-4587-9157-62c90ab37010.png#width=611" alt></p>
<p>请注意，转发传播结束时的线性函数的dim = 1，这意味着输出结果的每一行的概率总和必须等于1.给单个元素的概率最高图像的概率最高 被归类为相应的类索引。<br>我们必须确保模型的输出形状正确性，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Instantiate the model</span><br><span class="line">model = FashionNeuralNetwork()</span><br><span class="line"># Get the images and labels from the test loader</span><br><span class="line">images, labels = next(iter(testloader))</span><br><span class="line"># Get the log probability prediction from our model</span><br><span class="line">log_ps = model(images)</span><br><span class="line"># Normalize the probability by taking the exponent of the log-prob</span><br><span class="line">ps = torch.exp(log_ps)</span><br><span class="line"># Print out the size</span><br><span class="line">print(ps.shape)</span><br></pre></td></tr></table></figure></p>
<p>确保输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([64, 10])</span><br></pre></td></tr></table></figure></p>
<p><a name="o3h0uo"></a></p>
<h4 id="测量我们模型的准确性"><a href="#测量我们模型的准确性" class="headerlink" title="测量我们模型的准确性"></a><a href="#o3h0uo"></a>测量我们模型的准确性</h4><p>由于我们想要一个类的最高概率，我们将使用ps.topk来获得top-k值和top-k索引的元组，例如，如果在4个元素中最高为kth，我们将得到3作为指数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top_p, top_class = ps.topk(1,dim=1)</span><br><span class="line"># Print out the most likely classes for the first 10 examples</span><br><span class="line">print(top_class[:10,:])</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545195322539-23690b05-a8bf-47b0-92ab-a35a3245d1a2.png#width=122" alt><br>top_class是尺寸为64x1的2D张量，而我们的标签是尺寸为64的1D张量。为了测量标签和模型预测之间的准确度，我们必须确保张量的形状是相同的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># We have to reshape the labels to 64x1 using the view() method</span><br><span class="line">equals = top_class == labels.view(*top_class.shape)</span><br><span class="line">print(equals.shape)</span><br></pre></td></tr></table></figure></p>
<p>比较张量的输出将是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([64, 1])</span><br></pre></td></tr></table></figure></p>
<p>为了计算模型的准确性，我们只需计算模型正确预测的次数。 如果我们的预测与标签相同，则上面的==运算符将逐行检查。 最终结果将是二进制0不相同，1正确预测。 我们可以使用torch.mean计算平均值，但我们需要将equals转换为FloatTensor。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accuracy = torch.mean(equals.type(torch.FloatTensor))</span><br><span class="line"># Print the accuracy</span><br><span class="line">print(f&apos;Accuracy: &#123;accuracy.item()*100&#125;%&apos;)</span><br></pre></td></tr></table></figure></p>
<p><a name="go1ssu"></a></p>
<h4 id="训练我们的模型"><a href="#训练我们的模型" class="headerlink" title="训练我们的模型"></a><a href="#go1ssu"></a>训练我们的模型</h4><p>由于我们希望损失函数与Logarithm Softmax函数的行为相反，我们将使用负对数似然来计算我们的损失。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">from torch import optim</span><br><span class="line"># Instantiate the model</span><br><span class="line">model = FashionNeuralNetwork()</span><br><span class="line"># Use Negative Log Likelyhood as our loss function</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line"># Use ADAM optimizer to utilize momentum</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=0.003)</span><br><span class="line"># Train the model 30 cycles</span><br><span class="line">epochs = 30</span><br><span class="line"># Initialize two empty arrays to hold the train and test losses</span><br><span class="line">train_losses, test_losses = [],[]</span><br><span class="line"># Start the training</span><br><span class="line">for i in range(epochs):</span><br><span class="line">    running_loss = 0</span><br><span class="line">    # Loop through all of the train set forward and back propagate</span><br><span class="line">    for images,labels in trainloader:</span><br><span class="line">        optimizer.zero_grad()                      </span><br><span class="line">        log_ps = model(images)                     </span><br><span class="line">        loss = loss_function(log_ps, labels)       </span><br><span class="line">        loss.backward()                            # Backpropagate</span><br><span class="line">        optimizer.step()                           </span><br><span class="line">        running_loss += loss.item()                </span><br><span class="line">    </span><br><span class="line">    # Initialize test loss and accuracy to be 0 </span><br><span class="line">    test_loss = 0</span><br><span class="line">    accuracy = 0</span><br><span class="line">    </span><br><span class="line">    # Turn off the gradients</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # Loop through all of the validation set</span><br><span class="line">        for images, labels in testloader:</span><br><span class="line">            log_ps = model(images)                                 </span><br><span class="line">            ps = torch.exp(log_ps)                                 </span><br><span class="line">            test_loss += loss_function(log_ps, labels)             </span><br><span class="line">            top_p, top_class = ps.topk(1,dim=1)                    </span><br><span class="line">            equals = top_class == labels.view(*top_class.shape)   </span><br><span class="line">            accuracy += torch.mean(equals.type(torch.FloatTensor))</span><br><span class="line">    </span><br><span class="line">    # Append the average losses to the array for plotting       </span><br><span class="line">    train_losses.append(running_loss/len(trainloader))</span><br><span class="line">    test_losses.append(test_loss/len(testloader))</span><br></pre></td></tr></table></figure></p>
<p>打印我们的模型<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545195544515-f2c011c6-a9ad-4673-a266-508907165f04.png#width=684" alt><br>这证明了我们的假设，即完全说明我们的模型将训练的很好，但不能推广训练数据集之外的图像。 我们可以看到，30个周期的训练损失显著减少，但我们的验证损失在大约36-48％之间波动。 这是过度拟合的标志，这个情况说明，模型学习训练数据集的特定特征和模式，它无法正确分类数据集之外的图像。 这通常很糟糕，因为这意味着如果我们使用推理，模型就无法正确分类。<br><a name="r5sdzs"></a></p>
<h4 id="为了清楚的说明"><a href="#为了清楚的说明" class="headerlink" title="为了清楚的说明"></a><a href="#r5sdzs"></a>为了清楚的说明</h4><p>让我们画图看一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Plot the graph here</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = &apos;retina&apos;</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.plot(train_losses, label=&apos;Training Loss&apos;)</span><br><span class="line">plt.plot(test_losses, label=&apos;Validation Loss&apos;)</span><br><span class="line">plt.legend(frameon=True)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545195769635-b5ee1591-723d-4cb7-a10b-2b0a8740d404.png#width=554" alt><br><a name="4gv0rl"></a></p>
<h4 id="过度拟合"><a href="#过度拟合" class="headerlink" title="过度拟合"></a><a href="#4gv0rl"></a>过度拟合</h4><p>从上图中我们可以清楚地看到，我们的模型并没有很好地泛华。 这意味着该模型在对训练数据集之外的图像进行分类方面做得不好。 这真的很糟糕，这意味着我们的模型只学习我们的训练数据集的具体内容，它变得如此个别，以至于它只能识别来自训练集的图像。 如果我们从图表中看到，每个周期的训练损失都会显著减少，但是，我们可以看到验证损失却没发生什么变化。<br><a name="mm70mi"></a></p>
<h4 id="正则"><a href="#正则" class="headerlink" title="正则"></a><a href="#mm70mi"></a>正则</h4><p>这就是正则化的用武之地，其中一种方法是进行L2正则化，也称为<em>early-stopping</em>，这基本上意味着我们将在验证损失最低时停止训练我们的模型。 在这种情况下，我们的验证损失在3-5个时期后达到最佳。 这意味着超过5个周期，我们的模型泛化会变得更糟。<br><br>但是，还有另一种方法可以解决这个问题。 我们可以为我们的模型进行<em>dropout</em>，以进行更多的泛化。 基本上，我们的模型通过在大型重量上滚雪球并使其他要训练的重量不足而贪婪地行动。 通过具有随机丢失，具有较小权重的节点将有机会在循环期间被训练，从而在结束时给出更一般化的分数。 换句话说，它迫使网络在权重之间共享信息，从而提供更好的泛化能力。<br>注意：<br><br>在训练期间，我们希望实行dropout，但是，在验证过程中，我们需要我们模型的全部功能，因为那时我们可以完全测量模型对这些图像进行泛化其准确性。 如果我们使用model.eval()模式，我们将停止使用dropout，并且不要忘记在训练期间使用model.train()再次使用它。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">### Define our new Network with Dropouts</span><br><span class="line">class FashionNeuralNetworkDropout(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # Create layers here</span><br><span class="line">        self.layer_input = nn.Linear(784,256)</span><br><span class="line">        self.layer_hidden_one = nn.Linear(256,128)</span><br><span class="line">        self.layer_hidden_two = nn.Linear(128,64)</span><br><span class="line">        self.layer_output = nn.Linear(64,10)</span><br><span class="line">        </span><br><span class="line">        # 20% Dropout here</span><br><span class="line">        self.dropout = nn.Dropout(p=0.2)</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Flattened the input to make sure it fits the layer input</span><br><span class="line">        x = x.view(x.shape[0],-1)</span><br><span class="line">        # Pass in the input to the layer and do forward propagation</span><br><span class="line">        x = self.dropout(F.relu(self.layer_input(x)))</span><br><span class="line">        x = self.dropout(F.relu(self.layer_hidden_one(x)))</span><br><span class="line">        x = self.dropout(F.relu(self.layer_hidden_two(x)))</span><br><span class="line">        # Dimension = 1</span><br><span class="line">        x = F.log_softmax(self.layer_output(x),dim=1)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure></p>
<p>这个神经网络将与第一个模型非常相似，但是，我们将增加20％的丢失。 现在让我们训练这个模型吧！<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">from torch import optim</span><br><span class="line"># Instantiate the model</span><br><span class="line">model = FashionNeuralNetworkDropout()</span><br><span class="line"># Use Negative Log Likelyhood as our loss function</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line"># Use ADAM optimizer to utilize momentum</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=0.003)</span><br><span class="line"># Train the model 30 cycles</span><br><span class="line">epochs = 30</span><br><span class="line"># Initialize two empty arrays to hold the train and test losses</span><br><span class="line">train_losses, test_losses = [],[]</span><br><span class="line"># Start the training</span><br><span class="line">for i in range(epochs):</span><br><span class="line">    running_loss = 0</span><br><span class="line"># Loop through all of the train set forward and back propagate</span><br><span class="line">    for images,labels in trainloader:</span><br><span class="line">        optimizer.zero_grad()                      </span><br><span class="line">        log_ps = model(images)                     </span><br><span class="line">        loss = loss_function(log_ps, labels)       </span><br><span class="line">        loss.backward()                            # Backpropagate</span><br><span class="line">        optimizer.step()                           </span><br><span class="line">        running_loss += loss.item()                </span><br><span class="line">    </span><br><span class="line">    # Initialize test loss and accuracy to be 0 </span><br><span class="line">    test_loss = 0</span><br><span class="line">    accuracy = 0</span><br><span class="line">    </span><br><span class="line">    # Turn off the gradients</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # Turn on Evaluation mode</span><br><span class="line">        model.eval()</span><br><span class="line">        # Loop through all of the validation set</span><br><span class="line">        for images, labels in testloader:</span><br><span class="line">            log_ps = model(images)                                 </span><br><span class="line">            ps = torch.exp(log_ps)                                 </span><br><span class="line">            test_loss += loss_function(log_ps, labels)             </span><br><span class="line">            top_p, top_class = ps.topk(1,dim=1)                    </span><br><span class="line">            equals = top_class == labels.view(*top_class.shape)   </span><br><span class="line">            accuracy += torch.mean(equals.type(torch.FloatTensor))</span><br><span class="line"></span><br><span class="line">    # Turn on Training mode again</span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    # Append the average losses to the array for plotting       </span><br><span class="line">    train_losses.append(running_loss/len(trainloader))</span><br><span class="line">    test_losses.append(test_loss/len(testloader))</span><br></pre></td></tr></table></figure></p>
<p>输出结果：<br><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545196342869-4eae3b68-560b-4427-8e2f-34dec5ae5da0.png#width=687" alt><br>这里的目标是使验证损失与我们的训练损失一样低，这意味着我们的模型相当准确。 让我们再次绘制图表，看看正则化后的差异。 即使精度水平仅整体上升0.3％，该模型也没有过度拟合，因为它保持了在训练期间训练的所有节点的平衡。 让我们绘制图表并查看差异：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Plot the graph here</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = &apos;retina&apos;</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.plot(train_losses, label=&apos;Training Loss&apos;)</span><br><span class="line">plt.plot(test_losses, label=&apos;Validation Loss&apos;)</span><br><span class="line">plt.legend(frameon=True)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545196385024-ee9585c8-819c-4da5-aaa1-10592b806d57.png#width=555" alt><br><a name="hsnqvc"></a></p>
<h4 id="推理"><a href="#推理" class="headerlink" title="推理"></a><a href="#hsnqvc"></a>推理</h4><p>现在我们的模型可以更好地泛化，让我们用提供模型尝试用训练数据集之外的图像进行预测，并可视化模型的分类。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Make sure to make our model in the evaluation mode</span><br><span class="line">model.eval()</span><br><span class="line"># Get the next image and label</span><br><span class="line">images, labels = next(iter(testloader))</span><br><span class="line">img = images[0]</span><br><span class="line"># Convert 2D image to 1D vector</span><br><span class="line">img = img.view(1, 784)</span><br><span class="line"># Calculate the class probabilities (log-softmax) for img</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    output = model.forward(img)</span><br><span class="line"># Normalize the output</span><br><span class="line">ps = torch.exp(output)</span><br><span class="line"># Plot the image and probabilities</span><br><span class="line">helper.view_classify(img.view(1, 28, 28), ps, version=&apos;Fashion&apos;)</span><br></pre></td></tr></table></figure></p>
<p><img src="https://cdn.nlark.com/yuque/0/2018/png/219582/1545196482739-f2d874ee-fbb1-4c28-915c-d2c72a8db458.png#width=624" alt><br><a name="67g8gr"></a></p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a><a href="#67g8gr"></a>结论</h4><p>这很棒！ 我们可以看到培训损失和验证损失之间的显着平衡。 可以肯定地说，如果我们训练模型进行更多循环并微调我们的超参数，则验证损失将减少。 从上图中我们可以看出，我们的模型随着时间的推移更好地泛化，模型在6-8个时期之后可以获得更好的精度，并且可以肯定地说模型通过实现模型的丢失来防止过度拟合。<br><a name="bc98"></a></p>
<h3 id="Thank-you-so-much-for-your-time-and-please-check-out-this-repository-for-the-full-code"><a href="#Thank-you-so-much-for-your-time-and-please-check-out-this-repository-for-the-full-code" class="headerlink" title="Thank you so much for your time, and please check out this repository for the full code!"></a><a href="#bc98"></a>Thank you so much for your time, and please check out this <a href="https://github.com/diazagasatya/deep_learning_fastai/blob/master/main/Fashion%20MNIST%20-%20Classification%20Problem.ipynb" target="_blank" rel="noopener">repository</a> for the full code!</h3><p>This is my <a href="https://www.diazidabagus.com/" target="_blank" rel="noopener">Portfolio</a> and <a href="https://www.linkedin.com/in/diaz-agasatya-16487b109/" target="_blank" rel="noopener">Linked-In</a> profile :)</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/19/yuque/迁移学习：使用Fast.AI库对4种北极犬进行分类/" rel="next" title="迁移学习：使用Fast.AI库对4种北极犬进行分类">
                <i class="fa fa-chevron-left"></i> 迁移学习：使用Fast.AI库对4种北极犬进行分类
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/19/yuque/评论：U-Net（生物医学图像分割）/" rel="prev" title="评论：U-Net（生物医学图像分割）">
                评论：U-Net（生物医学图像分割） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#让我们导入Fashion-MNIST数据集"><span class="nav-number">1.</span> <span class="nav-text">让我们导入Fashion-MNIST数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测量我们模型的准确性"><span class="nav-number">2.</span> <span class="nav-text">测量我们模型的准确性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练我们的模型"><span class="nav-number">3.</span> <span class="nav-text">训练我们的模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为了清楚的说明"><span class="nav-number">4.</span> <span class="nav-text">为了清楚的说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#过度拟合"><span class="nav-number">5.</span> <span class="nav-text">过度拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正则"><span class="nav-number">6.</span> <span class="nav-text">正则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#推理"><span class="nav-number">7.</span> <span class="nav-text">推理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结论"><span class="nav-number">8.</span> <span class="nav-text">结论</span></a></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#Thank-you-so-much-for-your-time-and-please-check-out-this-repository-for-the-full-code"><span class="nav-number"></span> <span class="nav-text">Thank you so much for your time, and please check out this repository for the full code!</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhos</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
