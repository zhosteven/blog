{"meta":{"title":"A+","subtitle":"武德","description":null,"author":"Zhos","url":"http://zhos.me","root":"/"},"pages":[{"title":"about","date":"2019-04-04T13:32:06.000Z","updated":"2019-04-04T13:32:06.586Z","comments":true,"path":"about/index.html","permalink":"http://zhos.me/about/index.html","excerpt":"","text":""}],"posts":[{"title":"12产品管理的行为数据类型","slug":"yuque/12产品管理的行为数据类型","date":"2019-04-04T10:09:41.000Z","updated":"2019-04-04T13:49:28.602Z","comments":true,"path":"2019/04/04/yuque/12产品管理的行为数据类型/","link":"","permalink":"http://zhos.me/2019/04/04/yuque/12产品管理的行为数据类型/","excerpt":"","text":"行为数据，描述观察到的用户或客户行为的数据，可以让您真正了解人们如何或将来可能使用您的产品。听到人们说他们想要的东西是一回事，但看看他们的实际表现会更好。获得比您想象的行为数据更容易。无论您是在进行研究，运行实验来验证假设或功能，还是您刚刚发布了一项功能，行为数据都是您最好的，可以说是最有信息量的朋友。本文旨在通过向您提供产品团队可用的所有类型的行为数据列表来帮助您。希望它提供了一些新的选项，如果您尝试使用数据来推动团队的决策，您可以考虑这些选项。该列表大致从最常见到最不常见的顺序排列。_我们是一家澳大利亚的公司，所以我们使用“behavio __ü__ RAL”不是美国的“行为”。_ 1：网站分析数据您可能最熟悉此行为数据源：网页浏览量，点击次数，浏览器选择，设备选择等。网站分析是行为数据的一种形式，显示用户如何与您的网站，移动应用或用户进行互动（观看次数/点击次数）网络应用程序及其与网页浏览（设备/浏览器/分辨率）相关的选择。此类数据的常用工具是Google Analytics和更具企业价值的Adobe Experience Cloud。 2：App Analytics数据如果您正在构建产品，您也会熟悉这些行为数据：按钮点击，活动使用和应用程序中的其他用户事件。您甚至可以在此处包含应用程序和错误日志。这类数据的常用工具是Mixpanel，Amplitude和KISSmetrics。它们可帮助您定义自定义事件以跟踪用户执行的操作。有什么更好的方式来理解行为，然后看看你的用户实际与之交互以及他们如何与之交互（或不与之交互）？用户可能采取的行程以及何时停止使用也是有用的数据点。有很多关于您可以从用户分析中收集到的所有信息的书面文章，所以我不会在这里介绍它。 3：搜索数据使用来自Google，Microsoft和其他人的搜索量数据，通过查看他们搜索的内容来了解行为。搜索行为本身就是一个行为数据点，它还可以提供对用户或客户正在进行的另一种行为的洞察。您可以从Google的关键字规划师处获取搜索数据。 4：广告点击，竞争和展示数据在LinkedIn，Google，Facebook或其他任何其他地方在线投放广告可提供行为数据。您可以看到人们实际回复的消息与他们所说的可能对他们有吸引力的消息。您还可以测试哪些细分市场对哪些细分市场的反应最佳。你几乎可以立即做到这一切。例如，如果我想了解如何最好地将我的产品销售给银行，我可以在24-48小时内向银行经理定位facebook和linkedin广告。我可以看到广告/广告的哪些短语和变体效果最好。至少，我会根据观察到的实际行为与陈述的行为，得到一些初步的，数据支持的见解。这是非常具有革命性的。不再猜测在营销中使用哪些词。 5：产品评论/反馈了解人们实际体验产品的方式，他们所面临的挑战或他们最喜欢的事情是一种有用的见解。您无需将此限制为您的产品; 您可以通过查看网站查看竞争对手，补充和可比对象。亚马逊为不同的实体商品提供了大量信息。在分析产品评论时，请记住并非所有产品评论都是真实的。 6：客户支持查询用户和客户的反馈 - 请求，错误，问题 - 是另一种行为数据。事实上，有人已经不遗余力地与您互动，特别是在数字和软件即服务的世界中，必须承担一定的重量。如果你有足够的音量，你也可以通过分析来理解这一点。您需要牢记客户的偏见或背景。也就是说，您推销产品的方式，描述功能的方式，某人经历的旅程都会影响您收到的反馈类型。 7：社交媒体社交媒体的喜欢，评论，心灵和份额是另一种行为数据来源。您甚至可以进一步分析并分析评论本身的内容/文本或图像的内容。通过编译此信息，您可以了解人们实际上在说什么，分享或喜欢什么，以及他们可能实际与之交互的内容。这可以让您深入了解过去或未来的趋势。当您分析社交媒体以了解行为时，您需要考虑用户喜欢/分享的容易程度。您还想要考虑某人可能喜欢或分享的动机。在许多情况下，许多人（包括我自己）实际上没有阅读内容，但看到一个流行语或标题听起来很合适，所以他们分享了。 8光标跟踪监视某人的光标在您的网站或应用上移动的位置是另一种行为数据。您可以了解他们关注的内容。您可以使用Hotjar等工具来跟踪光标。请记住，光标所在的位置并不是他们所关注的内容的精确表示。也就是说，我现在的眼睛正在查看Google文档中的“分享”按钮，但我正在打字，因为某些原因我的光标位于屏幕的死角。 9：眼动追踪如果您可以使用正确的设施和技术，您可以将行为数据提升到一个新的水平，并通过跟踪他们正在寻找的位置来跟踪人们聚焦的位置。这对数字产品很有帮助 - 网络应用程序，移动应用程序，数字资料 - 广告，网站，其他内容 - 以及超市货架和广告牌等非数字产品 10：物理相互作用计算机视觉已经改进，可以跟踪人们触摸超市货架的内容。这可以扩展到他们在任何环境中接触的东西，比如他们在麦当劳使用的游乐场设备，调味品或座位。以下是AIPoly技术的视频，该技术可追踪人们在超市中从货架上使用的内容： 11：面部表情分析计算机视觉还可以通过在与您的产品或产品营销互动时阅读某人的表达来帮助理解行为。就行为数据而言，您需要仔细考虑您对面部表情识别的聚合分析的重量。例如，如果我们相信亚马逊的AI套件告诉我们我们的一个团队成员对某些事情的感受，那么我们就会认为它总是引起负面的，不愉快的反应。然而，亚马逊无法正确解释他的胡须和胡须，尤其是在他微笑的时候。你还需要考虑他们所表达的表达方式（它所推断的情感）是否重要 - 只是因为当我驾驶我的船时我看起来有点严厉，并不意味着我不喜欢它。 12：购买历史和交易数据看到有人买了什么是另一种形式的行为数据。购买通常是需要或想要的强烈迹象。有些组织以有限的形式提供此类产品 - 通过其产品或服务进行购买。其他组织可以访问个人进行的所有或许多购买。您还可以访问匿名购买历史记录。我经常发现这些匿名来源有点过于笼统，不适用于特定产品。或者，数据集存在太多问题，无法在任何有意义的时间范围内进行分析。例如，交易数据可能显示在超市花了124美元而不是我买的东西。124美元的美食奶酪与124美元的尿布完全不同。这篇文章最初出现在Terem的博客上_。_","categories":[],"tags":[]},{"title":"从Alexa的错误中学习","slug":"yuque/从Alexa的错误中学习","date":"2019-04-04T09:08:29.000Z","updated":"2019-04-04T13:49:28.602Z","comments":true,"path":"2019/04/04/yuque/从Alexa的错误中学习/","link":"","permalink":"http://zhos.me/2019/04/04/yuque/从Alexa的错误中学习/","excerpt":"","text":"虽然亚马逊的“唤醒词”功能可以改善隐私，但它提出了自己的挑战，例如Alexa录制私人谈话并未经同意将其发送出去。通过本迪克森Amazon Echo设备最近记录了用户的私人谈话，并在未经他们知情和同意的情况下将其发送给他们的一个联系人。这（再次）引起了对智能扬声器的安全性和隐私性的担忧。然而，后来变得明显，Alexa的奇怪行为并不是一个险恶的间谍情节的一部分 - 相反，它是由于智能扬声器的工作方式导致的一系列相关故障引起的。根据亚马逊提供的一个帐户：“由于背景对话中的一个词听起来像’Alexa’，Echo醒了。然后，随后的对话被听作是“发送消息”请求。在这一点上，Alexa大声说’给谁？’ 此时，后台会话被解释为客户联系人列表中的名称。然后Alexa大声问道，’[联系人姓名]，对吗？Alexa然后将背景对话解释为“正确”。尽管这一系列事件不太可能，我们正在评估使这种情况更不可能的选择。“该场景是一个边缘案例，很少发生的事件。但它也是人工智能技术极限的一项有趣研究，它为Echo和其他所谓的“智能”设备提供动力。 太多的云依赖为了理解语音命令，Echo和Google Home等智能扬声器依赖于深度学习算法，这需要大量的计算能力。由于他们没有在本地执行任务的计算资源，他们必须将数据发送到制造商的云服务器，其中AI算法将语音数据转换为文本并处理命令。但智能扬声器无法将他们听到的所有内容发送到他们的云服务器，因为这需要制造商在他们的服务器上存储过多的数据 - 其中大多数都是无用的。意外地记录和存储在用户家中发生的私人谈话也会带来隐私挑战，并可能使制造商陷入困境，尤其是新的数据隐私法规严格限制了科技公司存储和使用数据的方式。这就是为什么智能扬声器被设计为在用户发出诸如“Alexa”或“Hey Google”之类的唤醒词之后触发的原因。只有在听到唤醒词后，他们才开始将麦克风的音频输入发送到云进行分析和处理。虽然这项功能可以改善隐私，但它最近也出现了挑战，正如最近的Alexa事件所强调的那样。Conversocial首席执行官Joshua March说：“如果[唤醒]这个词 - 或听起来非常类似的话 - 在谈话中途发出，Alexa就不会有任何先前的背景。” “在这一点上，对于任何与你设置的技能相关的命令（如他们的消息应用程序），它都会非常难听。在大多数情况下，通过限制Alexa注意的环境（因为它不记录或听你的任何正常谈话）来大大增强隐私，尽管在这种情况下这种情况适得其反。“边缘计算的进步可能有助于缓解这个问题。随着人工智能和深度学习进入越来越多的设备和应用程序，一些硬件制造商已经创建了专门用于执行AI任务的处理器，而不必过多依赖云资源。Edge AI处理器可以帮助Echo等设备更好地理解和处理对话，而不会通过将所有数据发送到云来侵犯用户的隐私。 语境和意图除了收到不同的和零散的音频片段，亚马逊的AI还在努力理解人类对话的细微差别。“尽管过去几年在深度学习方面取得了巨大进步，使软件能够比以往更好地理解语音和图像，但仍有很多限制，”March说。“虽然语音助理可以识别您所说的单词，但他们并不一定能够真正理解其背后的含义或意图。世界是一个复杂的地方，但今天任何一个人工智能系统都只能处理非常具体，狭窄的用例。“例如，我们人类有很多方法可以确定一个句子是针对我们的，例如语调，还是跟随视觉提示 - 比如说话者正在看的方向。相反，Alexa假定它是任何包含“A”字的句子的接收者。这就是用户经常意外触发它的原因。 部分问题在于我们夸大了当前AI应用程序的功能，经常使它们与人类思维相提并论，并且过于信任它们。这就是为什么当他们失败时我们会感到惊讶。“这里的部分问题是”人工智能“这个术语如此积极地推向市场，以至于消费者对这个术语与他们的关系产生了不必要的信任，”神经科学家，Starmind的创始人帕斯卡尔考夫曼说。“这个故事说明Alexa有很多能力，对于如何以及何时适当应用它们的理解相对有限。”当深度学习算法面临偏离数据和他们训练的场景的设置时，它们很容易失败。“人类人工智能的一个明确特征是自给自足的能力和对内容的真正理解，”考夫曼说。“这是真正认为AI’聪明’的关键部分，对其发展至关重要。创造自我意识的数字助理，让他们充分了解人性，将标志着他们从有趣的新奇转变为真正有用的工具。“但是，创建人类级AI（也称为一般AI）说起来容易做起来难。几十年来，我们一直认为它即将到来，只是因为技术的进步表明人类的思想是多么复杂而变得沮丧。许多专家认为追逐一般人工智能是徒劳的。同时，狭窄的AI（作为当前的人工智能技术被描述）仍然提供了许多机会并且可以被修复以避免重复错误。需要明确的是，深度学习和机器学习仍处于初期阶段，像亚马逊这样的公司不断更新其AI算法，以便在每次发生时解决边缘情况。 我们需要做什么“这是一个年轻的新兴领域。自然语言理解尤其处于起步阶段，因此我们可以在这里做很多事情，“Atomic X首席技术官Eric Moller说。Moller认为可以调整语音分析AI算法以更好地理解语调和变形。“在更广泛的句子中使用’Alexa’这个词听起来与调用或命令不同。Alexa不应该醒来，因为你顺便说出了这个名字，“莫勒说。通过足够的训练，AI应该能够区分哪些特定音调指向智能扬声器。科技公司也可以训练他们的AI，以便能够区分何时接收背景噪音而不是直接说话。“背景喋喋不休具有独特的听觉’特征’，人类非常善于接受并有选择地调整。我们没有理由不能训练AI模型来做同样的事情，“莫勒说。作为预防措施，AI助理应评估他们所做出的决策的影响，并在他们想要做一些可能敏感的事情的情况下参与人为决策。制造商应在其技术中加入更多保护措施，以防止在未经用户明确和明确同意的情况下发送敏感信息。Tonkean首席执行官Sagi Eliyahi表示：“虽然亚马逊确实报告说Alexa试图确认其解释的行动，但有些行动需要更加谨慎地管理并保持更高标准的用户意图确认。” “人类有相同的语音识别问题，偶尔会听到请求。然而，与Alexa不同，人类更可能完全确认他们理解不明确的请求，更重要的是，衡量请求与过去请求相比的可能性。“ 同时…虽然科技公司微调他们的AI应用程序以减少错误，但用户必须做出最终决定他们希望暴露于他们的AI驱动设备可能产生的潜在错误的程度。“这些故事与人们愿意分享的数据量与新的人工智能技术的承诺相冲突，”数据科学专家道格罗斯和几本关于人工智能和软件的书的作者说。“你可能会因为速度缓慢而取笑Siri。但她获得更多智慧的最佳方式是入侵我们的私人谈话。因此，未来十年左右的一个关键问题是，我们将允许这些AI代理人查看我们的行为有多少？“Starmind的神经科学家Kaufmann说：“哪个家庭会把人类助理放在起居室里让那个人一直听到任何形式的谈话？” “在隐私，保密或可靠性方面，我们至少应该对所谓的’AI’设备（如果不是更高的话）应用相同的标准，我们也适用于人类智能生物。”阅读更多：“ 需要搁置的三大技术创意（现在） ”","categories":[],"tags":[]},{"title":"语音助理在手机上被浪费了","slug":"yuque/语音助理在手机上被浪费了","date":"2019-04-04T08:55:53.000Z","updated":"2019-04-04T13:49:28.602Z","comments":true,"path":"2019/04/04/yuque/语音助理在手机上被浪费了/","link":"","permalink":"http://zhos.me/2019/04/04/yuque/语音助理在手机上被浪费了/","excerpt":"","text":"智能手机和智能扬声器在解锁语音接口的全部潜力方面受到限制。数字助理的未来是增强和虚拟现实。通过本迪克森自2011年首次亮相以来，苹果公司的语音助手Siri已经激励众多公司开发竞争对手的数字助理，其中最受欢迎的是 亚马逊的Alexa和Google智能助理 - 早已超越Siri的质量。然而，语音助手仅用于我们使用计算设备完成的一小部分任务，因为它们目前所处的平台存在局限性。 智能手机和PC不是专为语音设计的调查将告诉您，越来越多的人在智能手机上使用语音助理。但是他们的使用占手机上花费的一小部分时间。例如，Verto Analytics在4月份进行的一项调查发现，52％的智能手机用户使用语音助手，但他们的使用量仅限于每天0.33次，与用户每天使用智能手机进行的数百次互动相比，这一点可以忽略不计。责备智能手机设计。用户界面旨在让您盯着并滚动手机。对于任何带屏幕的设备，语音始终是次要输入。语音助理最适合需要免提体验的环境。您可以让Siri在您准备工作时阅读您的电子邮件，但您最好坐下来通过手机屏幕阅读它们。因为我们的大脑没有被优化以同时专注于两项任务，所以当你试图在做其他事情时听Siri阅读你的电子邮件时，你会分心。因此，语音助理仅限于简单的任务，例如打开应用程序，拨打电话或简单查询，例如询问天气 - 大多数用户在与手机的触摸屏交互时更喜欢执行的所有任务。台式机和笔记本电脑对语音助理来说甚至是不太直观的体验。自2016年以来，Siri已经在macOS上可用，而且微软已将Cortana作为Windows 10不可或缺的一部分。我拥有一台Mac和一台PC，我喜欢修补新的技术，但我从来没有在其中任何一个上设置数字助理。什么时候是您最后一次需要在免提情况下使用PC？在大多数情况下，我们正盯着屏幕并使用鼠标和键盘，这使得数字助理成为可选的，非常有用的功能，仅此而已。 演讲者无法释放语音的全部潜力智能扬声器显然更有效地使用语音助手; Verto发现，智能音箱使用者每天平均使用语音助手2.79次，这远远超过智能手机，但仍然不多。在这种情况下，问题在于智能扬声器本身，因为使用没有显示器的设备几乎无法做到。尽管您可以在Amazon Echo和Google Home等智能扬声器上安装数以万计的技能，但大多数用户都会将它们用于有限数量的任务，包括播放音乐，设置定时器以及打开和关闭灯光。只要您想执行涉及多个步骤的复杂任务，智能扬声器的限制就会变得明显。例如，The Information报告称拥有Amazon Echo的人中只有2％使用它进行语音购物，这是智能扬声器最广告宣传的功能之一。在进行购物时，用户想要浏览项目，查看他们的选项并进行比较，这对于仅使用扬声器作为输出媒体的设备是不可能的。为此，亚马逊推出了Echo Show，这是一款带触摸屏的智能扬声器，可以显示您设置的定时器列表，而不是为您阅读。谷歌本月早些时候跟随家庭中心。 AR和VR是语音助手的真正家园语音助手的完美环境是用户希望执行各种任务同时将注意力集中在周围环境（如增强和虚拟现实应用程序）上的设置。与智能手机和计算机应用程序不同，AR和VR应用程序要求用户查看其真实和虚拟环境并与之交互，而不是专注于显示屏。与智能扬声器不同，AR耳机不仅限于您在厨房和起居室中执行的任务。由于基础技术的进步，AR在越来越多的专业领域（如制造和医疗保健）中显示出前景。但主要挑战之一是用户界面。耳机没有键盘，鼠标或触摸屏。用户通常通过耳机侧面的手势，控制器和触摸板与应用程序进行交互。某些设备还支持基本语音命令，但与AR环境进行交互通常非常具有挑战性。人工智能语音助手可以派上用场，特别是与眼动追踪等其他技术结合使用时。例如，用户可以盯着虚拟和物理元素而不是使用手势，并要求他们的语音助手执行不同的操作，例如查询信息或激活对象。这在用户已经完全无法使用手势或控制器的设置中尤其有用。Magic Leap正在为其Magic Leap One混合现实耳机制作两名AI助手，首席执行官Rony Abovitz 告诉The Verge。一个是“一个简单的机器人生物，用于执行低级任务”，另一个是“一个单独的类似人类的实体，你会被视为平等，如果你是粗鲁的话它会离开房间。 ”我不同意Abovitz，数字助理必须有人类行为。像钢铁侠的JARVIS这样的东西会更实用，更少分散注意力。尽管如此，Abovitz对此非常关注 - 数字助理将成为未来AR装备不可或缺的一部分。当增强现实充分发挥其潜力并且虚拟现实设备达到主流时，数字助理将不再是您休闲时使用的可选功能。它们将成为您的AR / VR体验中不可分割的一部分，帮助您完成传统的用户交互方式无法完成的任务。阅读更多：“ 哪个移动语音助理使用最多？”","categories":[],"tags":[]},{"title":"转移学习：重新启动Inception V3以进行自定义图像分类","slug":"yuque/转移学习：重新启动Inception V3以进行自定义图像分类","date":"2019-03-18T10:00:40.000Z","updated":"2019-04-04T13:49:28.570Z","comments":true,"path":"2019/03/18/yuque/转移学习：重新启动Inception V3以进行自定义图像分类/","link":"","permalink":"http://zhos.me/2019/03/18/yuque/转移学习：重新启动Inception V3以进行自定义图像分类/","excerpt":"","text":"让我们通过将现有的图像分类器（Inception V3）调整为自定义任务来体验转移学习的强大功能：对产品图像进行分类，以帮助食品和杂货零售商减少仓库和零售店库存管理过程中的人力。这项工作的源代码可以在我下面的GitHub存储库中找到。https://github.com/wisdal/Image-classification-transfer-learning使用的工具：TensorFlow v1.1，Python 3.4，Jupyter。 深度神经网络的应用实际上是在滚动。无论是医疗保健，运输还是零售，各行各业的公司都对投资构建智能解决方案感到兴奋。同时，让我们希望人类的智慧仍然无可争议:) 趋势AI文章： 1.神经网络如何工作&gt; 2. ResNets，HighwayNets和DenseNets，哦，我的！&gt; 3.机器学习傻瓜在实际情况下，强大的图像分类器等解决方案可以帮助公司跟踪货架库存，对产品进行分类，记录产品量，从专用设备（无人机？机器人？）实时捕获的原始产品图像。当然，能够识别产品并从给定的图片预测其类别是交易的一部分，这就是这个实验的全部内容：我们将培训机器人从图像中分类食品和杂货产品。 “这是一项轻松的任务！”，我们认为是人类，但计算机程序并不一定如此。无论如何，这个假设甚至都不适用于所有类别的人类; 一个5岁的孩子是一个完美的反例！当我们放大时，一切都是为了在您的生活中看到足够的产品图像，您可以从给定的图像轻松识别您之前看过的任何产品。当我们使用现有标记数据训练模型时，我们尝试将这种经验概念转移到模型，以便学习如何准确地区分训练数据集中存在的不同类别的数据。从这个意义上说，我们使用人工神经网络，它只不过是模仿人类大脑的实际运作方式。使用这些算法构建模型的知识稍后将在未标记的观察上进行测试。在我们的示例中，模型将基于其先前学习的内容标记输入图像，因此通常分配给该任务的名称监督学习 。谈到性能，已经注意到，在大多数监督学习的情况下，训练有素的模型往往比人类提供更好的准确性。在这个实际任务中，您会惊讶地发现即使在困难的条件下（模糊的图像，质量差的图像等），我们的算法也非常优于人类。我们有一个来自hackerearth的产品图像数据集可以在这里下载。我们应该如何进行，为什么我们使用转学习？ 为何转学？当我们考虑对图像进行分类时，我们常常选择从头开始构建我们的模型以获得最佳匹配。这是一个选项，但构建自定义深度学习模型需要大量的计算资源和大量的培训数据。此外，已经存在的模型在分类来自各种类别的图像时表现得相当好。您可能听说过ImageNet及其大视觉识别挑战。在这个计算机视觉挑战中，模型试图将大量图像分类为1000个类，如“斑马”，“达尔马提亚”和“洗碗机”。Inception V3是Google Brain Team为此而建立的模型。毋庸置疑，该模型表现非常出色。 那么，我们可以利用这个模型的存在来进行像现在这样的自定义图像分类任务吗？嗯，这个概念有一个名字：转移学习。它可能不如从头开始的完整培训那么高效，但对于许多应用来说都是惊人的有效。通过修改现有的丰富深度学习模型，它可以显着减少训练数据和时间。 为什么会这样在神经网络中，神经元被分层组织。不同的层可以对其输入执行不同类型的转换。信号可能在多次遍历各层之后从第一层（输入）传播到最后一层（输出）。作为最后一个隐藏层，“瓶颈”具有足够的汇总信息，以提供执行实际分类任务的下一层。在retrain.py脚本中，我们删除旧的顶层，并在我们下载的图片上训练一个新的顶层。我们的最后一层再训练可以用于新类的原因是，结果表明，区分ImageNet中所有1000个类所需的信息通常也可用于区分新类型的对象。我们现在弄脏手！ 第1步：预处理图像1234567label_counts = train.label.value_counts（）plt.figure（figsize =（12,6））sns.barplot（label_counts.index，label_counts.values，alpha = 0.9）plt.xticks（rotation =&apos;vertical&apos;）plt.xlabel （&apos;Image Labels&apos;，fontsize = 12）plt.ylabel（&apos;Counts&apos;，fontsize = 12）plt.show（） 假设您已经下载了数据集，您会发现它附带了一个我们需要正确设置的“train”文件夹。我们的目标是将每个图像放在代表其类别的子文件夹中。最后，我们应该有x个子文件夹，x是不同类别的数量。 为了这个预处理目的，我为您提供了pre_process.ipynb笔记本。 1234567891011121314for img in tqdm(train.values): filename=img[0] label=img[1] src=os.path.join(data_root,&apos;train_img&apos;,filename+&apos;.png&apos;) label_dir=os.path.join(data_root,&apos;train&apos;,label) dest=os.path.join(label_dir,filename+&apos;.jpg&apos;) im=Image.open(src) rgb_im=im.convert(&apos;RGB&apos;) if not os.path.exists(label_dir): os.makedirs(label_dir) rgb_im.save(dest) if not os.path.exists(os.path.join(data_root,&apos;train2&apos;,label)): os.makedirs(os.path.join(data_root,&apos;train2&apos;,label)) rgb_im.save(os.path.join(data_root,&apos;train2&apos;,label,filename+&apos;.jpg&apos;)) 笔记本不仅仅是配置图像子文件夹，所以一定要检查它。因为我们的数据集带有25个独特的标签，而我们只有3215个训练图像，我们需要增加数据以防止我们的模型过度拟合。 123456789101112131415161718192021222324252627282930313233datagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&apos;nearest&apos;)class_size=600src_train_dir=os.path.join(data_root,&apos;train&apos;)dest_train_dir=os.path.join(data_root,&apos;train2&apos;)it=0for count in label_counts.values: #nb of generations per image for this class label in order to make it size ~= class_size ratio=math.floor(class_size/count)-1 print(count,count*(ratio+1)) dest_lab_dir=os.path.join(dest_train_dir,label_counts.index[it]) src_lab_dir=os.path.join(src_train_dir,label_counts.index[it]) if not os.path.exists(dest_lab_dir): os.makedirs(dest_lab_dir) for file in os.listdir(src_lab_dir): img=load_img(os.path.join(src_lab_dir,file)) #img.save(os.path.join(dest_lab_dir,file)) x=img_to_array(img) x=x.reshape((1,) + x.shape) i=0 for batch in datagen.flow(x, batch_size=1,save_to_dir=dest_lab_dir, save_format=&apos;jpg&apos;): i+=1 if i &gt; ratio: break it=it+1 第2步：重新训练瓶颈并微调模型由谷歌提供，我们立即开始使用retrain.py脚本。该脚本默认下载Inception V3 预训练模型。重新训练脚本是我们算法的核心组件，也是使用从初始v3开始的转移学习的任何自定义图像分类任务的核心组件。它是由TensorFlow作者自己设计的，用于此特定目的（自定义图像分类）。 脚本的作用：它训练一个新的顶层（瓶颈），可以识别特定类别的图像。顶层接收每个图像的2048维向量作为输入。然后在该表示之上训练softmax层。假设softmax层包含N个标签，这对应于学习与学习的偏差和权重相对应的N + 2048 N（或1001 N）个模型参数。该脚本完全可以自定义，这里是可配置的参数列表： image_dir：标记图像文件夹的路径。幸运的是，我们在预处理步骤中正确设置了它。 output_graph，intermediate_output_graphs_dir，output_labels等：保存输出文件的位置。 失真功能：我的最爱。仅此功能就值得整整一段。您可能已经注意到我们的训练集中的图像是完美的（清晰，高质量，明确）但不幸的是，在生产中并非总是如此。该算法可能会在部署后遇到，模糊图像，昏暗的图像等。 我们的算法应该足够智能，以捕捉这些图像代表相同的事情，并不是那么明显（这只是一个小例子） […]这就是失真特征的全部内容。我们有意地在训练过程中随机变换图像（大小，颜色，方向等）以使机器人习惯于不良图像，以避免在这种情况下失去预测准确性。 how_many_training_steps：时代数。 学习率。 … 您可以随意使用这些参数。学习率，nb。时期等是确定性参数。使用它们来微调您的模型并记住您可以随时使用TensorBoard来可视化您的训练结果。您可以从一开始就获得约85％的准确度（零微调）。 第3步：在看不见的记录上测试模型这一步没什么可疯狂的。只是一个小脚本来测试在上一步中构建和保存的模型，在我们数据集的“test”文件夹中的图像上。查看测试笔记本，了解需要完成的工作。 12345678910111213141516171819202122def run_graph(src, labels, input_layer_name, output_layer_name, num_top_predictions): with tf.Session() as sess: i=0 #outfile=open(&apos;submit.txt&apos;,&apos;w&apos;) #outfile.write(&apos;image_id, label \\n&apos;) for f in os.listdir(dest): image_data=load_image(os.path.join(dest,test[i]+&apos;.jpg&apos;)) #image_data=load_image(os.path.join(src,f)) softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name) predictions, = sess.run(softmax_tensor, &#123;input_layer_name: image_data&#125;) # Sort to show labels in order of confidence top_k = predictions.argsort()[-num_top_predictions:][::-1] for node_id in top_k: human_string = labels[node_id] score = predictions[node_id] #print(&apos;%s (score = %.5f) %s , %s&apos; % (test[i], human_string)) print(&apos;%s, %s&apos; % (test[i], human_string)) #outfile.write(test[i]+&apos;, &apos;+human_string+&apos;\\n&apos;) i+=1 return 0 结论而已！希望这篇文章对你有用。随意评论并提出改进建议。我鼓励你试一试，并在评论中告诉我你能达到多少准确度。我很乐意收到你的来信。正如我之前所说，你肯定能够获得超过85％的基准精度。剩下的就是微调！在我的情况下，我的最终模型在测试装置上的准确性让我感到震惊，考虑到需要的工作量很少。很好地理解事情的工作方式有时候会有所帮助:)。我认为该项目是任何想要尝试图像失真或超参数调整的人的良好基础。这就是为我增加了更多的百分点。请随意查看我在GitHub上的代码。资源：tensorflow.org/tutorials/image_recognition","categories":[],"tags":[]},{"title":"Spark上的实际Python工作负载：独立群集","slug":"yuque/Spark上的实际Python工作负载：独立群集","date":"2019-03-18T09:45:16.000Z","updated":"2019-04-04T13:49:28.574Z","comments":true,"path":"2019/03/18/yuque/Spark上的实际Python工作负载：独立群集/","link":"","permalink":"http://zhos.me/2019/03/18/yuque/Spark上的实际Python工作负载：独立群集/","excerpt":"","text":"关于在Spark上运行Python有无数文章和论坛帖子，但大多数人认为要提交的工作包含在一个.py文件中：spark-submit wordcount.py - 完成！如果你的Python程序不仅仅是一个脚本怎么办？也许它为Spark生成动态SQL来执行，或使用Spark的输出刷新模型。随着您的Python代码变得更像一个应用程序（具有目录结构，配置文件和库依赖项），将其提交给Spark需要更多考虑。以下是我最近考虑使用Spark 2.3将一个这样的Python应用程序用于生产时的替代方案。第一篇文章重点介绍Spark独立集群。另一篇文章介绍了EMR Spark（YARN）。我远不是Spark的权威，更不用说Python了。我的决定试图平衡正确性和易于部署，以及应用程序对群集的限制。让我知道你的想法。 趋势AI文章： 1.使用机器学习预测购买行为&gt; 2.理解和构建生成对抗网络（GAN）&gt; 3.使用OpenCV和Haar Cascades构建Django POST面部检测API&gt; 4.通过Hindsight Experience Replay从错误中学习 示例Python应用程序为了模拟完整的应用程序，下面的场景假定Python 3应用程序具有以下结构： 1234project.py data / data_source.py data_source.ini _data_source.ini_包含各种配置参数： 123[spark] app_name =myPySpark App master_url = spark：// sparkmaster：7077 _data_source.py_是一个模块，负责在Spark中获取和处理数据，使用NumPy进行数学转换，并将Pandas数据帧返回给客户端。依赖关系： 123456from pyspark import SparkConf, SparkContextfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType, StructField, FloatTypeimport pandas as pdimport numpy as npimport configparser 它定义了一个创建和初始化的DataSource类……SparkContext`SparkSession` 123456789101112class DataSource: def __init__(self): config = configparser.ConfigParser() config.read(&apos;./data/data_source.ini&apos;) master_url = config[&apos;spark&apos;][&apos;master_url&apos;] app_name = config[&apos;spark&apos;][&apos;app_name&apos;] conf = SparkConf().setAppName(app_name) \\ .setMaster(master_url) self.sc = SparkContext(conf=conf) self.spark = SparkSession.builder \\ .config(conf=conf) \\ .getOrCreate() …和一个_get_data（）_方法： 从NumPy正态分布创建RDD。 应用函数将每个元素的值加倍。 将RDD转换为Spark数据帧并在顶部定义临时视图。 应用Python UDF，使用SQL对每个dataframe元素的内容进行平方。 将结果作为Pandas数据帧返回给客户端。 12345678910111213def get_data(self, num_elements=1000) -&gt; pd.DataFrame: mu, sigma = 2, 0.5 v = np.random.normal(mu, sigma, num_elements) rdd1 = self.sc.parallelize(v) def mult(x): return x * np.array([2]) rdd2 = rdd1.map(mult).map(lambda x: (float(x),)) schema = StructType([StructField(&quot;value&quot;, FloatType(), True)]) df1 = self.spark.createDataFrame(rdd2, schema) df1.registerTempTable(&quot;test&quot;) def square(x): return x ** 2 self.spark.udf.register(&quot;squared&quot;, square) df2 = self.spark.sql(&quot;SELECT squared(value) squared FROM test&quot;) return df2.toPandas() project.py是我们的主程序，充当上述模块的客户端： 1234567from data.data_source import DataSourcedef main(): src = DataSource() df = src.get_data(num_elements=100000) print(f&quot;Got Pandas dataframe with &#123;df.size&#125; elements&quot;) print(df.head(10))main() 克隆回购：https：//bitbucket.org/calidoteam/pyspark.git在开始之前，让我们回顾一下向Spark提交工作时可用的选项。 spark-submit，客户端和集群模式 Spark支持各种集群管理器：独立（即内置于Spark），Hadoop的YARN，Mesos，Kubernetes，所有这些都控制着工作负载在一组资源上的运行方式。 spark-submit是唯一与所有集群管理器一致的接口。对于Python应用程序，spark-submit可以在需要时上载和暂存您提供的所有依赖项，如.py，.zip或.egg文件。 在客户端模式下驱动程序_）将在运行的同一主机上spark-submit运行。确保此类主机靠近工作节点以减少网络延迟符合您的最佳利益。 在_集群_模式下驱动程序_某个_工作节点并从其运行。从远程主机提交作业时，这很有用。从Spark 2.4独立开始，Spark 2.4.0集群模式不是一个选项。 或者，可以spark-submit通过SparkSession在Python应用程序中配置连接到群集来绕过。这需要正确的配置和匹配的PySpark二进制文件。您的Python应用程序将在客户端模式下有效运行：它将从您启动它的主机上运行。 以下部分描述了几种部署方案，以及每种方案中需要的配置。 ＃1：直接连接到Spark（客户端模式，无spark-submit）针对Spark独立的客户端模式的Python应用程序 这是最简单的部署方案：Python应用程序通过指向Spark主URL直接建立spark上下文，并使用它来提交工作： 123456conf = SparkConf().setAppName(&quot;My PySpark App&quot;) \\ .setMaster(&quot;spark://192.168.1.10:7077&quot;)sc = SparkContext(conf=conf)spark = SparkSession.builder \\ .config(conf=conf) \\ .getOrCreate() 在独立群集中，资源在作业持续时间内分配，默认配置为客户端应用程序提供所有可用资源，因此需要对多租户环境进行微调。执行程序进程（JVM或python）由每个节点本地的_工作_进程启动。这类似于传统的客户端 - 服务器应用程序，因为客户端只是“连接”到“远程”集群。建议：确保驱动程序和群集之间有足够的带宽。大多数网络活动发生在驱动程序和它的执行程序之间，因此这个“远程”集群实际上必须在近距离（LAN）内。通过启用Apache Arrow改进Java-Python序列化： Python工作负载（NumPy，Pandas和其他应用于Spark RDD，数据帧和数据集的转换）默认需要大量的Java和Python进程的序列化和反序列化，并且会迅速降低性能。从Spark 2.3开始，启用Apache Arrow（包含在下面列出的步骤中）使这些传输更加高效。跨所有群集节点和驱动程序主机部署依赖关系。这包括下载和安装Python 3，pip安装PySpark（必须与目标集群的版本匹配），PyArrow以及其他库依赖项： 1234sudo yum install python36pip install pyspark==2.3.1 pip install pyspark[sql]pip install numpy pandas msgpack sklearn 注意：在安装像PySpark这样的大型库（~200MB）时，可能会遇到以“ MemoryError” 结尾的错误。如果是这样，请尝试： 1pip install --no-cache-dir pyspark==2.3.1 配置和环境变量：在客户端，$SPARK_HOME必须指向pip安装PySpark的位置： 1234567891011$ pip show pysparkName: pysparkVersion: 2.3.1Summary: Apache Spark Python APIHome-page: https://github.com/apache/spark/tree/master/pythonAuthor: Spark DevelopersAuthor-email: dev@spark.apache.orgLicense: http://www.apache.org/licenses/LICENSE-2.0Location: /opt/anaconda/lib/python3.6/site-packagesRequires: py4j$ export SPARK_HOME=/opt/anaconda/lib/python3.6/site-packages 在每个群集节点上，设置其他默认参数和环境变量。特别是对于Python应用程序：$SPARK_HOME/conf/spark-defaults.sh spark.sql.execution.arrow.enabled true $SPARK_HOME/conf/spark-env.sh export PYSPARK_PYTHON=/usr/bin/python3 ：Python可执行文件，所有节点。 export PYSPARK_DRIVER_PYTHON=/usr/bin/python3 ：驱动程序的Python可执行文件，如果与执行程序节点不同。 注：环境变量是从哪里读spark-submit的_推出_，不一定是从群集主机内。 运行它将工作负载提交到集群只需运行Python应用程序（例如spark-submit，不需要）： 12$ cd my-project-dir/$ python3 project.py 在运行时，可以看到运行多个python3进程的从属节点在作业上运行： ＃2：集装箱式应用（客户端模式，无火花提交）这是前一个场景的扩展，由于可移植性等原因，最好将Python应用程序作为Docker容器运行，作为CI / CD管道的一部分。除了针对上一个方案推荐的配置外，还需要以下内容：构建容器以包含所有依赖项：从包含Python 3和/或Java 8 OpenJDK的映像开始，然后pip-install PySpark，PyArrow以及应用程序所需的所有其他库。配置Spark驱动程序主机和端口，在容器中打开它们：这是执行程序到达容器内驱动程序所必需的。可以通过编程方式设置驱动程序的Spark属性（spark.conf.set(“property”, “value”)）： 1234spark.driver.host : host_ip_address (e.g. 192.168.1.10)spark.driver.port : static_port (e.g. 51400)spark.driver.bindAddress : container_internal_ip (e.g. 10.192.6.81)spark.driver.blockManagerPort : static_port (e.g. 51500) 在Docker中，端口可以使用-p选项从命令行公开到外部：-p 51400:51400 -p 51500:51500。其他文章建议只发布此端口范围：-p 5000–5010:5000–5010 运行它与前一个场景一样，运行容器将启动Python驱动程序： 1docker run -p 51400：51400 -p 51500：51500 &lt;docker_image_url&gt; ＃3：通过spark-submit提供Python应用程序（客户端模式）此方案实际上与方案＃1相同，仅为了清楚起见包含在此处。唯一的区别是Python应用程序是使用该spark-submit进程启动的。除了日志文件之外，还会将群集事件发送到stdout： 12345$ cd my-project-dir/$ ls -l rwxrwxr-x. 3 centos centos 70 Feb 25 02:11 data-rw-rw-r--. 1 centos centos 220 Feb 25 01:09 project.py$ spark-submit project.py 笔记： 根据我的经验，spark-submit只要从项目根目录（my-project-dir/）调用它就没有必要在调用时传递依赖的子目录/文件。 由于示例应用程序已指定主URL，因此无需将其传递给spark-submit。否则，更完整的命令将是： 1$ spark-submit --master spark://sparkcas1:7077 --deploy-mode client project.py 从Spark 2.3开始，无法将集群模式下的Python应用程序提交给独立的Spark集群。这样做会产生错误： 12$ spark-submit --master spark://sparkcas1:7077 --deploy-mode cluster project.pyError: Cluster deploy mode is currently not supported for python applications on standalone clusters. Takeaways- Python on Spark独立集群： 虽然独立群集在生产中不受欢迎（可能是因为商业支持的分发包括群集管理器），但只要不需要多租户和动态资源分配，它们的占用空间就会更小并且做得很好。 对于Python应用程序，部署选项仅限于客户端模式。 使用Docker来容纳Python应用程序具有所有预期的优势，非常适合客户端模式部署。","categories":[],"tags":[]},{"title":"如何（以及为什么）创建一个好的验证集","slug":"yuque/如何（以及为什么）创建一个好的验证集","date":"2019-03-18T04:19:06.000Z","updated":"2019-04-04T13:49:28.574Z","comments":true,"path":"2019/03/18/yuque/如何（以及为什么）创建一个好的验证集/","link":"","permalink":"http://zhos.me/2019/03/18/yuque/如何（以及为什么）创建一个好的验证集/","excerpt":"","text":"#撰稿：2017年11月13日由Rachel Thomas撰写一个非常常见的场景：看似令人印象深刻的机器学习模型在生产中实施时是完全失败的。这些影响包括那些现在对机器学习持怀疑态度且不愿意再次尝试的领导者。怎么会发生这种情况？对于开发结果与生产结果之间的这种脱节的最可能的罪魁祸首之一是选择不当的验证集（或者更糟糕的是，根本没有验证集）。根据数据的性质，选择验证集可能是最重要的一步。尽管sklearn提供了一种train_test_split方法，但该方法采用了数据的随机子集，这对于许多现实问题来说是一个糟糕的选择。培训，验证和测试集的定义可能相当细微，有时不一致地使用这些术语。在深度学习社区中，“测试时间推断”通常用于指对生产中的数据进行评估，这不是测试集的技术定义。如上所述，sklearn有一种train_test_split方法，但没有train_validation_test_split。Kaggle只提供培训和测试集，但要做得好，您需要将他们的训练集分成您自己的验证和训练集。而且，事实证明Kaggle的测试集实际上被细分为两组。很多初学者可能会感到困惑，这并不奇怪！我将在下面解决这些微妙之处。 首先，什么是“验证集”？在创建机器学习模型时，最终目标是使其准确处理新数据，而不仅仅是用于构建数据的数据。考虑下面一组数据的3种不同模型的例子：资料来源：Andrew Ng的机器学习课程对于最右侧的模型，图示数据点的误差最小（蓝色曲线几乎完美地穿过红点），但它不是最佳选择。这是为什么？如果你要收集一些新的数据点，它们很可能不在右边图表中的那条曲线上，而是更接近中间图形中的曲线。根本的想法是： 训练集用于训练给定的模型 验证集用于在模型之间进行选择（例如，随机森林或神经网络是否更适合您的问题？您想要一个有40棵树或50棵树的随机森林吗？） 测试集告诉你你是怎么做的。如果您已经尝试了很多不同的模型，那么您可能只是偶然得到一个在验证集上表现良好的模型，并且拥有测试集有助于确保不是这种情况。 验证和测试集的一个关键属性是它们必须代表您将来会看到的新数据。这可能听起来像一个不可能的命令！根据定义，您还没有看到这些数据。但是你仍然知道一些事情。 什么时候随机子集不够好？看一些例子是有益的。虽然这些例子中有很多来自Kaggle比赛，但它们代表了您在工作场所会遇到的问题。 时间序列如果您的数据是时间序列，那么选择数据的随机子集将非常简单（您可以在您尝试预测的日期之前和之后查看数据）并且不代表大多数业务用例（您在哪里）正在使用历史数据来构建将来使用的模型。如果您的数据包含日期并且您要构建将来使用的模型，则需要选择具有最新日期的连续部分作为验证集（例如，可用数据的最后两周或上个月） 。假设您要将下面的时间序列数据拆分为训练和验证集：时间序列数据随机子集是一个糟糕的选择（太容易填补空白，并不表示您在生产中需要什么）：训练集的选择不佳使用较早的数据作为训练集（以及验证集的后续数据）：为您的训练集提供更好的选择Kaggle目前正在竞争预测一系列厄瓜多尔杂货店的销售情况。Kaggle的“培训数据”从2013年1月1日至2017年8月15日运行，测试数据跨越2017年8月16日至2017年8月31日。一个好方法是使用2017年8月1日至8月15日作为验证集，以及所有早期数据作为你的训练集。 新人，新船，新…您还需要考虑在生产中进行预测的数据可能与您训练模型所需的数据有何不同。在Kaggle 分心驾驶员竞赛中，独立数据是汽车驾驶员的照片，因变量是一个类别，如发短信，吃饭或安全向前看。如果您是从这些数据构建模型的保险公司，请注意您最感兴趣的是模型在您之前没有见过的驱动程序上的表现（因为您可能只为一小部分人提供培训数据）。对于Kaggle比赛也是如此：测试数据由未在训练集中使用的人组成。同一个人驾驶时在电话上交谈的两个图像。如果您将上述图像之一放入训练集中，并将其中一个放在验证集中，那么您的模型似乎表现得比新人更好。另一个观点是，如果你使用所有人来训练你的模型，你的模型可能过分适应那些特定人的特殊性，而不仅仅是学习状态（发短信，吃饭等）。在Kaggle渔业竞赛中也有类似的动态，以确定渔船捕获的鱼类种类，以减少非法捕捞濒危种群。测试集由未出现在训练数据中的船组成。这意味着您希望验证集包含不在训练集中的船只。有时可能不清楚您的测试数据会有何不同。例如，对于使用卫星图像的问题，您需要收集有关训练集是否仅包含某些地理位置的更多信息，或者是否来自地理位置分散的数据。 交叉验证的危险sklearn没有train_validation_test拆分的原因是假设您经常使用交叉验证，其中训练集的不同子集用作验证集。例如，对于3倍交叉验证，数据被分为3组：A，B和C.首先训练模型A和B组合作为训练集，并在验证集C上进行评估。 ，将模型训练为A和C组合作为训练集，并在验证集B上进行评估。依此类推，最终将3倍的模型性能平均化。但是，交叉验证的问题在于，由于上述各节中描述的所有原因，它很少适用于现实世界的问题。交叉验证仅适用于您可以随机调整数据以选择验证集的相同情况。 Kaggle的“训练集”=你的训练+验证集Kaggle比赛的一个好处是它们会迫使你更严格地考虑验证集（为了做得好）。对于那些刚接触Kaggle的人来说，它是一个举办机器学习比赛的平台。Kaggle通常会将数据分为两组，您可以下载： 一个训练集，其中包括独立的变量，以及对因变量（你正在尝试预测）。对于试图预测销售的厄瓜多尔杂货店的例子，自变量包括商店ID，商品ID和日期; 因变量是销售数量。对于尝试确定驾驶员是否在车轮后面进行危险行为的示例，自变量可以是驾驶员的图片，并且因变量是类别（例如发短信，吃饭或安全地向前看）。 一个测试集，它只有自变量。您将对测试集进行预测，您可以将其提交给Kaggle，并获得您的评分。 这是开始机器学习所需的基本思想，但要做得好，理解起来要复杂一些。您将需要创建自己的培训和验证集（通过拆分Kaggle“培训”数据）。您将使用较小的训练集（Kaggle训练数据的子集）来构建模型，并且在提交给Kaggle之前，您可以在验证集（也是Kaggle的训练数据的子集）上对其进行评估。最重要的原因是Kaggle将测试数据分为两组：公共和私人排行榜。您在公共排行榜上看到的分数仅适用于您预测的一部分（您不知道哪个子集！）。您的预测在私人排行榜上的表现将不会在比赛结束前公布。这一点很重要的原因是你最终可能会过度适应公共排行榜，直到最后你在私人排行榜上做得不好时才会意识到这一点。使用良好的验证集可以防止这种情况。您可以通过查看您的模型是否具有与Kaggle测试集相比较的相似分数来检查您的验证集是否有用。创建自己的验证集很重要的另一个原因是Kaggle限制您每天提交两次，并且您可能希望尝试更多。第三，确切地看到你在验证集上出错的地方是有益的，而且Kaggle没有告诉你测试集的正确答案，甚至没有告诉你哪些数据点你的错误，只是你的整体得分。理解这些区别不仅对Kaggle有用。在任何预测性机器学习项目中，您希望模型能够在新数据上表现良好。","categories":[],"tags":[]},{"title":"如何使用良好的软件工程实践设置PySpark环境以进行开发","slug":"yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发","date":"2019-03-18T03:16:24.000Z","updated":"2019-04-04T13:49:28.578Z","comments":true,"path":"2019/03/18/yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发/","link":"","permalink":"http://zhos.me/2019/03/18/yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发/","excerpt":"","text":"在本文中，我们将讨论如何设置我们的开发环境以创建高质量的python代码以及如何自动执行一些繁琐的任务来加速部署。我们将介绍以下步骤： 使用pipenv在隔离的虚拟环境中设置我们的依赖项 如何为多个作业设置项目结构 如何运行pyspark工作 如何使用Makefile 自动执行开发步骤 如何使用flake8测试代码的质量 如何使用pytest-spark为PySpark应用程序运行单元测试 运行测试覆盖率，看看我们是否使用pytest-cov创建了足够的单元测试第1步：设置虚拟环境虚拟环境有助于我们将特定应用程序的依赖关系与系统的整体依赖关系隔离开来。这很好，因为我们不会涉及现有库的依赖性问题，并且在单独的系统（例如docker容器或服务器）上安装或卸载它们更容易。对于此任务，我们将使用pipenv。要在mac os系统上安装它，例如运行： 1brew install pipenv 要为应用程序声明我们的依赖项（库），我们需要在项目的路径路径中创建一个Pipfile： 1234567891011[[source]]url = &apos;https://pypi.python.org/simple&apos;verify_ssl = truename = &apos;pypi&apos;[requires]python_version = &quot;3.6&quot;[packages]flake8 = &quot;*&quot;pytest-spark = &quot;&gt;=0.4.4&quot;pyspark = &quot;&gt;=2.4.0&quot;pytest-cov = &quot;*&quot; 这里有三个组件。在[[source]]标签中，我们声明了下载所有软件包的url，在[requires]中我们定义了python版本，最后在[packages]中声明了我们需要的依赖项。我们可以将依赖项绑定到某个版本，或者使用“*”符号来获取最新版本。要创建虚拟环境并激活它，我们需要在终端中运行两个命令： 12pipenv --three installpipenv shell 一旦完成这一步，你应该看到你在一个新的venv中，让项目的名字出现在命令行的终端中（默认情况下，env采用项目的名称）： 1(pyspark-project-template) host:project$ 现在，您可以使用两个命令进出。停用env并返回标准环境： 1deactivate 再次激活虚拟环境（您需要位于项目的根目录中）：1source `pipenv --venv`/bin/activate 第2步：项目结构该项目可以具有以下结构： 12345678910111213141516171819pyspark-project-template src/ jobs/ pi/ __init__.py resources/ args.json word_count/ __init__.py resources/ args.json word_count.csv main.py test/ jobs/ pi/ test_pi.py word_count/ test_word_count.py 排除一些init.py文件以简化操作，但您可以在本教程末尾的github上找到完整项目的链接。我们基本上有源代码和测试。每个作业都分成一个文件夹，每个作业都有一个资源文件夹，我们在其中添加该作业所需的额外文件和配置。在本教程中，我使用了两个经典示例 - pi，生成最多小数的pi数和字数，以计算csv文件中的单词数。 第3步：使用spark-submit运行作业我们先来看看main.py文件的样子： 12345678910111213141516171819if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser(description=&apos;My pyspark job arguments&apos;) parser.add_argument(&apos;--job&apos;, type=str, required=True, dest=&apos;job_name&apos;, help=&apos;The name of the spark job you want to run&apos;) parser.add_argument(&apos;--res-path&apos;, type=str, required=True, dest=&apos;res_path&apos;, help=&apos;Path to the jobs resurces&apos;) args = parser.parse_args() spark = SparkSession\\ .builder\\ .appName(args.job_name)\\ .getOrCreate() job_module = importlib.import_module(&apos;jobs.%s&apos; % args.job_name) res = job_module.run(spark, get_config(args.res_path, args.job_name)) print(&apos;[JOB &#123;job&#125; RESULT]: &#123;result&#125;&apos;.format(job=args.job_name, result=res)) 当我们运行我们的工作时，我们需要两个命令行参数： - job，是我们想要运行的作业的名称（在例外pi或word_count中）和 - res-path，是作业的相对路径。我们需要第二个参数，因为spark需要知道我们资源的完整路径。在生产环境中，我们将代码部署在集群上，我们将资源转移到HDFS或S3，我们将使用该路径。在进一步解释代码之前，我们需要提一下，我们必须压缩作业文件夹并将其传递给spark-submit语句。假设我们在项目的根目录中： 12cd src/ zip -r ../jobs.zip jobs/ 这将使代码在我们的应用程序中作为模块提供。基本上在第16行的main.py中，我们以编程方式导入作业模块。我们的作业pi和word_count都有一个run函数，所以我们只需要运行这个函数来启动这个作业（main.py中的第17行）。我们还在那里传递了工作的配置。让我们看一下word_count作业，进一步了解这个例子： 1234567891011from operator import adddef get_keyval(row): words = filter(lambda r: r is not None, row) return [[w.strip().lower(), 1] for w in words]def run(spark, config): df = spark.read.csv(config[&apos;relative_path&apos;] + config[&apos;words_file_path&apos;]) mapped_rdd = df.rdd.flatMap(lambda row: get_keyval(row)) counts_rdd = mapped_rdd.reduceByKey(add) return counts_rdd.collect() 此代码在word_count文件夹的init.py文件中定义。我们在这里可以看到，我们使用两个配置参数来读取资源文件夹中的csv文件：相对路径和csv文件的位置。其余的代码只计算单词，所以我们不会在这里详细介绍。值得一提的是，每个作业在resources文件夹中都有一个args.json文件。这里我们实际定义了传递给作业的配置。这是word_count作业的配置文件： 123&#123; &quot;words_file_path&quot;: &quot;/word_count/resources/word_count.csv&quot;&#125; 所以我们现在有了所有细节来运行我们的spark-submit命令： 1spark-submit --py-files jobs.zip src/main.py --job word_count --res-path /your/path/pyspark-project-template/src/jobs 要运行另一个作业pi，我们只需要更改- job标志的参数 。 第4步：编写单元测试，并使用覆盖率运行它们要为pyspark应用程序编写测试，我们使用pytest-spark，一个非常易于使用的模块。该WORD_COUNT工作单元测试： 123456789101112131415161718from src.jobs.word_count import get_keyval, rundef test_get_keyval(): words=[&apos;this&apos;, &apos;are&apos;, &apos;words&apos;, &apos;words&apos;] expected_results=[[&apos;this&apos;, 1], [&apos;are&apos;, 1], [&apos;words&apos;, 1], [&apos;words&apos;, 1]] assert expected_results == get_keyval(words)def test_word_count_run(spark_session): expected_results = [(&apos;one&apos;, 1), (&apos;two&apos;, 1), (&apos;three&apos;, 2), (&apos;four&apos;, 2), (&apos;test&apos;, 1)] conf = &#123; &apos;relative_path&apos;: &apos;/your/path/pyspark-project-template/src/jobs&apos;, &apos;words_file_path&apos;: &apos;/word_count/resources/word_count.csv&apos; &#125; assert expected_results == run(spark_session, conf) 我们需要从src模块导入我们想要测试的函数。这里更有趣的部分是我们如何进行test_word_count_run。我们可以看到没有初始化的spark会话，我们只是在测试中将其作为参数接收。这要归功于pytest-spark模块，因此我们可以专注于编写测试，而不是编写样板代码。接下来让我们讨论一下代码覆盖率。我们怎么知道我们是否编写了足够的单元测试？很简单，我们运行测试覆盖工具，告诉我们尚未测试的代码。对于python，我们可以使用pytest-cov模块。要使用代码覆盖率运行所有测试，我们必须运行： 1pytest --cov=src test/jobs/ where - cov flag告诉pytest在哪里检查覆盖范围。测试覆盖率结果： 123456789---------- coverage: platform darwin, python 3.7.2-final-0 -----------Name Stmts Miss Cover-----------------------------------------------------src/__init__.py 0 0 100%src/jobs/__init__.py 0 0 100%src/jobs/pi/__init__.py 11 0 100%src/jobs/word_count/__init__.py 9 0 100%-----------------------------------------------------TOTAL 20 0 100% 我们的测试覆盖率是100％，但是等一下，缺少一个文件！为什么main.py没有在那里列出？如果我们认为我们有不需要测试的python代码，我们可以将它从报告中排除。为此，我们需要在项目的根目录中创建一个 .coveragerc文件。对于此示例，它看起来像这样： 12[run]omit = src/main.py 第5步：运行静态代码分析很好，我们有一些代码，我们可以运行它，我们有良好的覆盖率的单元测试。我们做对了吗？还没！我们还需要确保按照python最佳实践编写易于阅读的代码。为此，我们必须使用名为flake8的python模块检查我们的代码。要运行它： 1flake8 ./src 它将分析src文件夹。如果我们有干净的代码，我们就不应该收到任何警告。但不，我们有一些问题： 1234flake8 ./src./src/jobs/pi/__init__.py:13:1: E302 expected 2 blank lines, found 1./src/jobs/pi/__init__.py:15:73: E231 missing whitespace after &apos;,&apos;./src/jobs/pi/__init__.py:15:80: E501 line too long (113 &gt; 79 characters) 我们来看看代码： 12345678910111213141516from random import randomfrom operator import addNUMBER_OF_STEPS_FACTOR = 100000def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 &lt;= 1 else 0def run(spark, config): number_of_steps = config[&apos;partitions&apos;] * NUMBER_OF_STEPS_FACTOR count = spark.sparkContext.parallelize(range(1, number_of_steps + 1),config[&apos;partitions&apos;]).map(f).reduce(add) return 4.0 * count / number_of_steps 我们可以看到在第13行我们有一个E302警告。这意味着我们需要在两种方法之间增加一条线。然后是第15行的E231和E501。这一行的第一个警告告诉我们，我们需要在和之间留出一个额外的空间，第二个警告通知我们线路太长，而且很难读（我们可以’甚至在要点中完整地看到它！）。**range(1, number_of_steps +1),** **config[** 解决所有警告后，代码看起来更容易阅读： 12345678910111213141516171819from random import randomfrom operator import addNUMBER_OF_STEPS_FACTOR = 100000def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 &lt;= 1 else 0def run(spark, config): number_of_steps = config[&apos;partitions&apos;] * NUMBER_OF_STEPS_FACTOR count = spark.sparkContext\\ .parallelize(range(1, number_of_steps + 1), config[&apos;partitions&apos;]).map(f).reduce(add) return 4.0 * count / number_of_steps 第6步：将所有内容与Makefile放在一起因为我们在终端中运行了一堆命令，所以在最后一步中我们将研究如何简化和自动执行此任务。我们可以在项目的根目录中创建一个Makefile，如下所示：12345678910111213.DEFAULT_GOAL := runinit: pipenv --three install pipenv shellanalyze: flake8 ./srcrun_tests: pytest --cov=src test/jobs/run: find . -name &apos;__pycache__&apos; | xargs rm -rf rm -f jobs.zip cd src/ &amp;&amp; zip -r ../jobs.zip jobs/ spark-submit --py-files jobs.zip src/main.py --job $(JOB_NAME) --res-path $(CONF_PATH) 如果我们想要使用coverage运行测试，我们只需输入： 1make run_tests 如果我们想要运行pi工作： 1make run JOB_NAME=pi CONF_PATH=/your/path/pyspark-project-template/src/jobs 这就是所有人！希望这个对你有帮助。一如既往，代码存储在github上。","categories":[],"tags":[]},{"title":"使用Spark Structured Streaming，XGBoost和Scala进行实时预测","slug":"yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测","date":"2019-03-18T03:06:19.000Z","updated":"2019-04-04T13:49:28.578Z","comments":true,"path":"2019/03/18/yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测/","link":"","permalink":"http://zhos.me/2019/03/18/yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测/","excerpt":"","text":"在本文中，我们将讨论构建完整的机器学习管道。第一部分将侧重于在标准批处理模式下训练二元分类器，在第二部分中我们将进行一些实时预测。我们将使用来自泰坦尼克号的数据：机器学习灾难中的众多Kaggle比赛之一。在开始之前，请知道您应该熟悉Scala，Apache Spark和Xgboost。所有源代码也将在Github上提供。很酷，现在让我们开始吧！ 训练我们将使用Spark中的ML管道训练XGBoost分类器。分类器将保存为输出，并将在Spark Structured Streaming实时应用程序中用于预测新的测试数据。第1步：启动spark会话我们正在创建一个将在本地运行的spark应用程序，并将使用与使用的核心一样多的线程local[*] ： 1234val spark = SparkSession.builder() .appName(&quot;Spark XGBOOST Titanic Training&quot;) .master(&quot;local[*]&quot;) .getOrCreate() 第2步：定义架构接下来，我们定义从csv读取的数据的模式。这通常比让火花推断模式更好，因为它消耗的资源更少，我们可以完全控制字段。 1234567891011121314val schema = StructType( Array(StructField(\"PassengerId\", DoubleType), StructField(\"Survival\", DoubleType), StructField(\"Pclass\", DoubleType), StructField(\"Name\", StringType), StructField(\"Sex\", StringType), StructField(\"Age\", DoubleType), StructField(\"SibSp\", DoubleType), StructField(\"Parch\", DoubleType), StructField(\"Ticket\", StringType), StructField(\"Fare\", DoubleType), StructField(\"Cabin\", StringType), StructField(\"Embarked\", StringType) )) 第3步：读取数据我们把csv读成a DataFrame，确保我们提到我们有一个标题。 12345val df_raw = spark .read .option(&quot;header&quot;, &quot;true&quot;) .schema(schema) .csv(filePath) 第4步：删除空值所有空值都替换为0.这不是理想的，但是对于本教程的目的，它是可以的。 1val df = df_raw.na.fill(0) 第5步：将名义值转换为数字在浏览此步骤的代码之前，让我们简要介绍一些Spark ML概念。他们介绍了ML管道的概念，它是一组构建在其上的高级API DataFrames，可以更轻松地将多个算法组合到一个流程中。管道的主要元素是Transformer和Estimator。第一个可以表示可以将a DataFrame转换为另一个DataFrame的算法，而后者是可以适合a DataFrame来生成a 的算法Transformer 。为了将名义值转换为数字值，我们需要Transformer为每列定义一个： 1234567891011121314val sexIndexer = new StringIndexer() .setInputCol(\"Sex\") .setOutputCol(\"SexIndex\") .setHandleInvalid(\"keep\")val cabinIndexer = new StringIndexer() .setInputCol(\"Cabin\") .setOutputCol(\"CabinIndex\") .setHandleInvalid(\"keep\")val embarkedIndexer = new StringIndexer() .setInputCol(\"Embarked\") .setOutputCol(\"EmbarkedIndex\") .setHandleInvalid(\"keep\") 我们正在使用它StringIndexer来转换价值观。对于每个Transformer我们定义的输入列和输出列将包含修改后的值。步骤6：将列组合成特征向量我们将使用另一个Transformer将XGBoost分类中使用的列组合Estimator成一个向量： 123val vectorAssembler = new VectorAssembler() .setInputCols(Array(\"Pclass\", \"SexIndex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"CabinIndex\", \"EmbarkedIndex\")) .setOutputCol(\"features\") 第7步：添加XGBoost估算器定义Estimator它将产生模型。可以在地图中定义估计器的设置。我们还可以设置功能和标签列： 123val xgbEstimator = new XGBoostEstimator(Map[String, Any](&quot;num_rounds&quot; -&gt; 100)) .setFeaturesCol(&quot;features&quot;) .setLabelCol(&quot;Survival&quot;) 第8步：构建管道和分类器在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作的顺序： 1val pipeline = new Pipeline().setStages(Array(sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgbEstimator)) 输入DataFrame将被多次转换，最终将生成使用我们的数据训练的模型。我们将保存输出，以便在第二个实时应用程序中使用它。 12val cvModel = pipeline.fit(df)cvModel.write.overwrite.save(modelPath) 预测我们将使用Spark Structured Streaming来基本传输文件中的数据。在现实世界的应用程序中，我们从专用的分布式队列（例如Apache Kafka或AWS Kinesis）读取数据，但是对于此演示，我们将只使用一个简单的文件。简要描述Spark Structured Streaming是一个构建在Spark SQL之上的流处理引擎。它使用相同的概念，DataFrames数据存储在一个无限制的表中，当数据流入时，该表随着新行的增长而增长。第1步：创建输入读取流我们再次创建一个spark会话并定义数据的架构。请注意，测试csv不包含标签Survival 。最后我们可以创建输入流DataFrame, df。输入路径必须是我们存储csv文件的目录。它可以包含一个或多个具有相同模式的文件。 123456789101112131415161718192021222324val spark: SparkSession = SparkSession.builder() .appName(\"Spark Structured Streaming XGBOOST\") .master(\"local[*]\") .getOrCreate()val schema = StructType( Array(StructField(\"PassengerId\", DoubleType), StructField(\"Pclass\", DoubleType), StructField(\"Name\", StringType), StructField(\"Sex\", StringType), StructField(\"Age\", DoubleType), StructField(\"SibSp\", DoubleType), StructField(\"Parch\", DoubleType), StructField(\"Ticket\", StringType), StructField(\"Fare\", DoubleType), StructField(\"Cabin\", StringType), StructField(\"Embarked\", StringType) )) val df = spark .readStream .option(\"header\", \"true\") .schema(schema) .csv(fileDir) 第2步：加载XGBoost模型在对象中，XGBoostModel我们加载预先训练的模型，该模型将应用于我们在流中读取的每一批新行。 123456789101112131415161718object XGBoostModel &#123; private val modelPath = \"your_path\" private val model = PipelineModel.read.load(modelPath) def transform(df: DataFrame) = &#123; // replace nan values with 0 val df_clean = df.na.fill(0) // run the model on new data val result = model.transform(df_clean) // display the results result.show() &#125;&#125; 第3步：定义自定义ML接收器为了能够将我们的分类器应用于新数据，我们需要创建一个新的接收器（流和输出之间的接口，在我们的例子中是XGBoost模型）。为此，我们需要一个自定义接收器（MLSink），一个抽象接收器提供器（MLSinkProvider）和provider（）的实现XGBoostMLSinkProvider。 12345678910111213141516171819202122abstract class MLSinkProvider extends StreamSinkProvider &#123; def process(df: DataFrame): Unit def createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): MLSink = &#123; new MLSink(process) &#125;&#125;case class MLSink(process: DataFrame =&gt; Unit) extends Sink &#123; override def addBatch(batchId: Long, data: DataFrame): Unit = &#123; process(data) &#125;&#125;class XGBoostMLSinkProvider extends MLSinkProvider &#123; override def process(df: DataFrame) &#123; XGBoostModel.transform(df) &#125;&#125; 第4步：在我们的自定义接收器中写入数据最后一步是定义一个将数据写入自定义接收器的查询。还必须定义检查点位置，以便应用程序“记住”在发生故障时在流中读取的最新行。如果我们运行程序，每个新批次的数据将显示在控制台上，同时包含预测的标签。 12345df.writeStream .format(\"titanic.XGBoostMLSinkProvider\") .queryName(\"XGBoostQuery\") .option(\"checkpointLocation\", checkpoint_location) .start()","categories":[],"tags":[]},{"title":"PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试","slug":"yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试","date":"2019-03-18T02:32:11.000Z","updated":"2019-04-04T13:49:28.578Z","comments":true,"path":"2019/03/18/yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试/","link":"","permalink":"http://zhos.me/2019/03/18/yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试/","excerpt":"","text":"在本教程中，我们将讨论使用标准机器学习管道集成PySpark和XGBoost。我们将使用来自泰坦尼克号的数据：机器学习灾难中的众多Kaggle比赛之一。在开始之前，请知道您应该熟悉Apache Spark和Xgboost以及Python。本教程中使用的代码可以在github上的Jupyther笔记本中找到。 第1步：下载或构建XGBoost jarpython代码需要两个scala jar依赖项才能工作。您可以直接从maven下载它们： xgboost4j xgboost4j火花 如果您希望自己构建它们，可以从我之前的教程中找到如何进行构建。 第2步：下载XGBoost python包装器您可以从此处下载PySpark XGBoost代码。这是我们要编写的部分和XGBoost scala实现之间的接口。我们将在本教程后面的代码中看到它如何集成。 第3步：启动一个新的Jupyter笔记本我们将开始一个新的笔记本，以便能够编写我们的代码： 1jupyter notebook 第4步：将自定义XGBoost jar添加到Spark应用程序在开始Spark之前，我们需要添加我们之前下载的jar。我们可以使用--jars标志来做到这一点： 12import os os.environ [&apos;PYSPARK_SUBMIT_ARGS&apos;] =&apos; - jars xgboost4j-spark-0.72.jar，xgboost4j-0.72.jar pyspark-shell&apos; 第5步：将PySpark集成到Jupyther笔记本中使PySpark可用的最简单方法是使用该[findspark](https://github.com/minrk/findspark)软件包： 12import findspark findspark.init（） 第6步：启动spark会话我们现在准备开始火花会议。我们正在创建一个将在本地运行的spark应用程序，并将使用与使用的核心一样多的线程local[*] ： 12345spark = SparkSession\\ .builder\\ .appName(&quot;PySpark XGBOOST Titanic&quot;)\\ .master(&quot;local[*]&quot;)\\ .getOrCreate() 第7步：添加PySpark XGBoost包装器代码正如我们现在有了spark会话，我们可以添加先前下载的包装器代码： 1spark.sparkContext.addPyFile(&quot;YOUR_PATH/sparkxgb.zip&quot;) 第8步：定义架构接下来，我们定义从csv读取的数据的模式。这通常比让火花推断模式更好，因为它消耗的资源更少，我们可以完全控制字段。 1234567891011121314schema = StructType( [StructField(&quot;PassengerId&quot;, DoubleType()), StructField(&quot;Survival&quot;, DoubleType()), StructField(&quot;Pclass&quot;, DoubleType()), StructField(&quot;Name&quot;, StringType()), StructField(&quot;Sex&quot;, StringType()), StructField(&quot;Age&quot;, DoubleType()), StructField(&quot;SibSp&quot;, DoubleType()), StructField(&quot;Parch&quot;, DoubleType()), StructField(&quot;Ticket&quot;, StringType()), StructField(&quot;Fare&quot;, DoubleType()), StructField(&quot;Cabin&quot;, StringType()), StructField(&quot;Embarked&quot;, StringType()) ]) 步骤9：将csv数据读入数据帧我们将csv读入a DataFrame，确保我们提到我们有一个标题，我们也null用0 替换值： 123456df_raw = spark\\ .read\\ .option(&quot;header&quot;, &quot;true&quot;)\\ .schema(schema)\\ .csv(&quot;YOUR_PATH/train.csv&quot;)df = df_raw.na.fill(0) 步骤10：C 将标称值转换为数字在浏览此步骤的代码之前，让我们简要介绍一些Spark ML概念。他们介绍了ML管道的概念，它是一组构建在其上的高级API DataFrames，可以更轻松地将多个算法组合到一个流程中。管道的主要元素是Transformer和Estimator。第一个可以表示可以将a DataFrame转换为另一个DataFrame的算法，而后者是可以适合a DataFrame来生成a 的算法Transformer 。为了将名义值转换为数字值，我们需要Transformer为每列定义一个： 1234567891011121314sexIndexer = StringIndexer()\\ .setInputCol(&quot;Sex&quot;)\\ .setOutputCol(&quot;SexIndex&quot;)\\ .setHandleInvalid(&quot;keep&quot;) cabinIndexer = StringIndexer()\\ .setInputCol(&quot;Cabin&quot;)\\ .setOutputCol(&quot;CabinIndex&quot;)\\ .setHandleInvalid(&quot;keep&quot;) embarkedIndexer = StringIndexer()\\ .setInputCol(&quot;Embarked&quot;)\\ .setOutputCol(&quot;EmbarkedIndex&quot;)\\ .setHandleInvalid(&quot;keep&quot;) 我们正在使用它StringIndexer来转换价值观。对于每个Transformer我们定义的输入列和输出列将包含修改后的值。 步骤11：将列组合成特征向量我们将使用另一个Transformer将XGBoost分类中使用的列组合Estimator成一个向量： 123vectorAssembler = VectorAssembler()\\ .setInputCols([&quot;Pclass&quot;, &quot;SexIndex&quot;, &quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;, &quot;CabinIndex&quot;, &quot;EmbarkedIndex&quot;])\\ .setOutputCol(&quot;features&quot;) 第12步：定义XGBoostEstimator在这一步中，我们定义了Estimator将产生模型的东西。这里使用的大多数参数都是默认的： 12345xgboost = XGBoostEstimator( featuresCol=&quot;features&quot;, labelCol=&quot;Survival&quot;, predictionCol=&quot;prediction&quot;) 我们只定义feature, label（必须匹配来自的列DataFrame）和prediction包含分类器输出的新列。 步骤13：建立管道和分类器在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作的顺序： 1pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost]) 输入DataFrame将被多次转换，最终将生成使用我们的数据训练的模型。 步骤14：训练模型并预测新的测试数据我们首先将数据分成火车和测试，然后我们将模型与火车数据拟合，最后我们看到我们为每位乘客获得的预测： 123trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)model = pipeline.fit(trainDF)model.transform(testDF).select(col(&quot;PassengerId&quot;), col(&quot;prediction&quot;)).show()","categories":[],"tags":[]},{"title":"将ML投入生产：在Python中使用Apache Kafka","slug":"yuque/将ML投入生产：在Python中使用Apache Kafka","date":"2019-03-15T10:03:41.000Z","updated":"2019-04-04T13:49:28.578Z","comments":true,"path":"2019/03/15/yuque/将ML投入生产：在Python中使用Apache Kafka/","link":"","permalink":"http://zhos.me/2019/03/15/yuque/将ML投入生产：在Python中使用Apache Kafka/","excerpt":"","text":"我们将说明如何使用一系列工具（即Kafka，MLFlow和Sagemaker）来帮助生产ML。为此，我们将设置一个简单的场景，我们希望它类似于一些真实用例，然后描述一个潜在的解决方案。可以在此处找到包含所有代码的伴随仓库。 场景公司使用一系列服务收集数据，这些服务在用户/客户与公司的网站或应用程序交互时生成事件。当这些交互发生时，算法需要实时运行，并且需要根据算法的输出（或预测）采取一些立即行动。最重要的是，经过_ñ相互作用（或意见）的算法需要重新训练不停止的预测_ 服务，因为用户将保持互动。对于这里的练习，我们使用了成人数据集，其目标是根据年龄，原籍国等来预测个人是否获得高于/低于50k的收入。为了使该数据集适应前面描述的情景，可以假设通过在线问卷/表格收集该年龄，本国等，我们需要预测用户是否实时获得高/低收入。如果收入高，那么我们会立即给他们打电话/给他们发电子邮件。然后，在_N个_新观察之后，我们重新训练算法，同时我们继续预测新用户。解决方案图1是潜在解决方案的图示。为了实现这个解决方案，我们使用了Kafka-Python（可以在这里找到一个很好的教程），以及LightGBM和Hyperopt或HyperparameterHunter。图1. 实时预测ML管道。以下提供完整描述我们将在本练习中使用的唯一Python“局外人”是Apache-Kafka（我们将使用python API Kafka-Python，但仍然需要在您的系统中安装Kafka）。如果你在Mac上，只需使用Homebrew： 1brew install kafka 这也将安装zookeeper依赖项。如前所述，我们使用了Adult数据集。这是因为我们的目的是说明潜在的ML管道并提供有用的代码，同时保持相对简单。但请注意，此处描述的管道原则上与数据无关。当然，预处理将根据数据的性质而改变，但如果不相同，管道组件将保持相似。初始化实验您可以在我们的仓库中找到用于此帖子的代码（以及更多）。在那里，有一个名为的脚本initialize.py 。该脚本将下载数据集，设置目录结构，预处理数据，在训练数据集上训练初始模型并优化该模型的超参数。在现实世界中，这将对应于通常的实验阶段和离线训练初始算法的过程。在这篇文章中，我们希望主要关注管道和相应的组件而不是ML。尽管如此，让我们简单地提一下我们在本练习中使用的ML工具。鉴于我们正在使用的数据集，数据预处理非常简单。我们编写了一个名为的自定义类[FeatureTools](https://github.com/jrzaurin/ml_pipelines/blob/master/utils/feature_tools.py)，可以utils在repo 中的模块中找到。这个类有.fit和 .transform方法将标准化/缩放数字特征，编码分类特征并生成我们称之为“交叉列”，这是两个（或更多）分类特征之间的笛卡尔积的结果。处理完数据后，我们使用LightGBM将模型与Hyperopt或HyperparameterHunter相匹配，以执行超参数优化。可以在train模块中找到与此任务相关的代码，其中可以找到两个脚本[train_hyperop](https://github.com/jrzaurin/ml_pipelines/blob/master/train/train_hyperopt.py).py和[train_hyperparameterhunter](https://github.com/jrzaurin/ml_pipelines/blob/master/train/train_hyperparameterhunter.py).py 。我们可能会在python（Skopt，Hyperopt和HyperparameterHunder）中编写一个单独的帖子来比较超参数优化包，但是现在，请知道：如果你想要速度，那么使用Hyperopt。如果您不关心速度并且想要详细跟踪优化例程，请使用HyperparameterHunter。用Hunter McGushion 的话来说，包装的创造者： “长期以来，超参数优化一直是一个耗时的过程，只是指向了进一步优化的方向，然后你基本上不得不重新开始。” HyperparameterHunter就是为了解决这个问题，它做得非常好。目前，该软件包是建立在Skopt之上的，这就是为什么它比Hyperopt慢得多。但是，我知道有人努力将Hyperopt作为HyperparameterHunter的另一个后端包含在内。当发生这种情况时，不会有任何争议，HyperparameterHunter应该是您的首选工具。尽管如此，如果有人感兴趣，我在回购中包含了一个笔记本，比较了Skopt和Hyperopt的表现。让我们现在转到管道流程本身。App Messages Producer这意味着生产管道的哪个部分可能看起来相对简单。因此，我们直接使用Adult数据集生成消息（JSON对象）。在现实世界中，人们将拥有许多可以生成事件的服务。从那里，有一个选项。这些事件中的信息可能存储在数据库中，然后通过常规查询进行汇总。从那里，Kafka服务将消息发布到管道中。或者，这些事件中的所有信息可以直接发布到不同的主题中，“聚合服务”可以将所有信息存储在单个消息中，然后将其发布到管道中（当然，也可以组合使用他们俩）。例如，可能允许用户通过Facebook或Google注册，收集他们的姓名和电子邮件地址。然后他们可能会被要求填写调查问卷，我们会继续收集他们进展的事件。在此过程中的某个时刻，所有这些事件将在单个消息中聚合，然后通过Kafka生产者发布。这篇文章中的管道将从聚合了所有相关信息的点开始。我们这里的消息是Adult数据集中的单个观察。下面我们将包含消息内容的示例： 1’&#123;“age”:25,”workclass”:”Private”,”fnlwgt”:226802,”education”:”11th”,”marital_status”:”Never-married”,”occupation”:”Machine-op-inspct”,”relationship”:”Own-child”,”race”:”Black”,”gender”:”Male”,”capital_gain”:0,”capital_loss”:0,”hours_per_week”:40,”native_country”:”United-States”,”income_bracket”:”&lt;=50K.”&#125;’ App / Service的核心（图1中最灰色，最左边的框）是下面的代码段： 123456789101112131415161718192021222324df_test = pd.read_csv(PATH/'adult.test')df_test['json'] = df_test.apply(lambda x: x.to_json(), axis=1)messages = df_test.json.tolist()def start_producing(): producer = KafkaProducer(bootstrap_servers=KAFKA_HOST) for i in range(200): message_id = str(uuid.uuid4()) message = &#123;'request_id': message_id, 'data': json.loads(messages[i])&#125; producer.send('app_messages', json.dumps(message).encode('utf-8')) producer.flush() print(\"\\033[1;31;40m -- PRODUCER: Sent message with id &#123;&#125;\".format(message_id)) sleep(2)def start_consuming(): consumer = KafkaConsumer('app_messages', bootstrap_servers=KAFKA_HOST) for msg in consumer: message = json.loads(msg.value) if 'prediction' in message: request_id = message['request_id'] print(\"\\033[1;32;40m ** CONSUMER: Received prediction &#123;&#125; for request id &#123;&#125;\".format(message['prediction'], request_id)) 请注意，我们使用测试数据集来生成消息。这是因为我们设计了一个尽可能与真实世界相似的场景（在一定限度内）。考虑到这一点，我们使用训练数据集来构建初始model和dataprocessor对象。然后，我们使用测试数据集生成消息，目的是模拟随时间接收新信息的过程。关于上面的代码片段，简单地说，生产者会将消息发布到管道（start_producing()）中并使用带有最终预测（start_consuming()）的消息。与我们在此描述的管道不同的方式不包括流程的开始（事件收集和聚合），我们也跳过最后，即如何处理最终预测。尽管如此，我们还是简要讨论了一些用例，这些用例可能会在帖子结尾处有用，这将说明最后阶段。实际上，除了忽略过程的开始和结束之外，我们认为这条管道在现实世界中可以使用的管道相当好。因此，我们希望我们的仓库中包含的代码对您的某些项目有用。预测者和训练师该实现的主要目标是实时运行算法并在不停止预测服务的情况下每_N次_观察重新训练它。为此，我们实现了两个组件，Predictor（在repo中）和Trainer（）。[predictor](https://github.com/jrzaurin/ml_pipelines/blob/master/predictor.py).py`trainer.py&lt;br /&gt;现在让我们逐一描述图1中显示的数字，使用代码片段作为我们的指南。请注意，下面的过程假设有一个运行initialize.py脚本，因此初始文件model.p和dataprocessor.p`文件存在于相应的目录中。另外，请强调下面的代码包含Predictor和Trainer的核心。有关完整代码，请参阅回购。预报器Predictor代码的核心如下所示 1234567891011121314151617181920def start(model_id, messages_count, batch_id): for msg in consumer: message = json.loads(msg.value) if is_retraining_message(msg): model_fname = 'model_&#123;&#125;_.p'.format(model_id) model = reload_model(MODELS_PATH/model_fname) print(\"NEW MODEL RELOADED &#123;&#125;\".format(model_id)) elif is_application_message(msg): request_id = message['request_id'] pred = predict(message['data'], column_order) publish_prediction(pred, request_id) append_message(message['data'], MESSAGES_PATH, batch_id) messages_count += 1 if messages_count % RETRAIN_EVERY == 0: model_id = (model_id + 1) % (EXTRA_MODELS_TO_KEEP + 1) send_retrain_message(model_id, batch_id) batch_id += 1 （1a）predictor.py片段中的第12行。预测器将从应用程序/服务接收消息，它将进行数据处理并在接收消息时实时运行模型。所有这些都是使用函数中的现有对象dataprocessor和model对象发生的predict。（1b）predictor.py片段中的第13行。一旦我们运行预测，Predictor将发布publish_prediction()最终将由App / Service接收的result（）。（2）predictor.py片段中的第17-20行。每条RETRAIN_EVERY消息，Predictor都会发布一条“ 重新训练 ”消息（send_retrain_message()），由培训师阅读。训练者 12345678910111213def start(): consumer = KafkaConsumer(RETRAIN_TOPIC, bootstrap_servers=KAFKA_HOST) for msg in consumer: message = json.loads(msg.value) if 'retrain' in message and message['retrain']: model_id = message['model_id'] batch_id = message['batch_id'] message_fname = 'messages_&#123;&#125;_.txt'.format(batch_id) messages = MESSAGES_PATH/message_fname train(model_id, messages) publish_traininig_completed(model_id) （3）trainer.py片段中的第12行。培训师将阅读该消息并使用新的累积数据集（train()）触发重新训练过程。这是原始数据集加上RETRAIN_EVERY新的观察结果。列车功能将独立于1a和1b中描述的过程运行“初始化实验” 部分中描述的整个过程。换句话说，训练者将重新训练模型，而预测器在消息到达时保持预测。在这个阶段值得一提的是，在这里我们发现我们的实现与将在现实世界中使用的实现之间存在进一步的差异。在我们的实现中，一旦处理了RETRAIN_EVERY多个观察结果，就可以重新训练算法。这是因为我们使用Adult测试数据集来生成消息，其中包括目标列（“ _income_braket_ ”）。在真实的单词中，基于算法输出所采取的动作的真实结果通常在算法运行之后不容易访问，但是一段时间之后。在那种情况下，另一个过程应该是收集真实的结果，一旦收集的真实结果的数量等于RETRAIN_EVERY算法将被重新训练。例如，假设此管道实现了电子商务的实时推荐系统。我们已经离线训练了一个推荐算法，目标列是我们的用户喜欢我们建议的分类表示：0,1,2和3对于不喜欢或与项目交互的用户，喜欢该项目（例如点击像按钮），将项目添加到他们的篮子，并分别购买该项目。当系统提供建议时，我们仍然不知道用户最终会做什么。因此，随着用户信息在网站（或应用程序）中导航时收集和存储用户信息，第二个过程应该收集我们建议的最终结果。只有当两个进程都收集了RETRAIN_EVERY消息和结果时，才会对算法进行重新训练。（4）trainer.py片段中的第13行。重新训练完成后，将发布带有相应信息的消息（published_training_completed()）。（5）predictor.py片段中的第5-8行。Predictor的消费者订阅了两个主题：[‘app_messages’, ‘retrain_topic’]。一旦它通过“retrain_topic”接收到重新训练完成的信息，它将加载新模型并像往常一样保持过程，而不会在过程中的任何时间停止。 如何运行管道在配套仓库中，我们已经包含了如何运行管道（本地）的说明。其实很简单。 启动zookeper和kafka： 1234$ brew services start zookeeper==&gt; Successfully started `zookeeper` (label: homebrew.mxcl.zookeeper)$ brew services start kafka==&gt; Successfully started `kafka` (label: homebrew.mxcl.kafka) 2.运行initialize.py： 1python initialize.py 3.在终端＃1中运行预测器（或训练器）： 1python predictor.py 4.在终端＃2中运行训练器（或预测器）： 1python trainer.py 5.在终端＃3中运行示例应用程序 1python samplea_app.py 然后，一旦处理了N条消息，您应该看到如下内容： 右上方终端：我们重新训练了模型，Hyperopt已经进行了10次评估（在实际练习中，这些应该是几百次）。左上方终端：一旦对模型进行了重新训练和优化，我们就会看到预测器如何加载新模型（在新LightGBM版本的恼人警告消息之后）。底部终端：服务照常进行。 一些潜在的用例以下是（很多）其他一些潜在用例。实时调整在线旅程让我们考虑出售一些商品的电子商务。当用户浏览网站时，我们会收集有关其行为信息的活动。我们之前已经培训了一种算法，我们知道在10次交互之后，我们可以很好地了解客户是否最终会购买我们的产品。此外，我们也知道他们可能购买的产品可能会很昂贵。因此，我们希望“在旅途中”定制他们的旅程，以促进他们的购物体验。这里的定制可能意味着什么，从缩短行程到改变页面布局。2. 电子邮件/致电您的客户与之前的用例类似，我们现在假设客户决定停止旅程（无聊，缺乏时间，可能太复杂等）。如果算法预测该客户具有很大的潜力，我们可以立即使用像本文所述的管道，或者使用受控延迟，发送电子邮件或调用它们。下一步记录和监控：在即将发布的帖子中，我们将通过MLFlow在管道记录和监控功能中插入。与HyperparameterHunter一起，该解决方案将自动跟踪模型性能和超参数优化，同时提供可视化监控。流量管理：此处描述的解决方案以及相应的代码已经过设计，因此可以在笔记本电脑中轻松地在本地和手动运行。然而，人们会认为在现实生活中，这将需要大规模运行云，而不是手动（请）。在那个阶段，如果我们可以使用涵盖整个机器学习工作流程的完全托管服务，那将是理想的，因此我们不需要关心维护服务，版本控制等。为此目的，我们将使用Sagemaker，它是构建的正是为了这个目的。","categories":[],"tags":[]},{"title":"使用MLflow赋予Spark功能","slug":"yuque/使用MLflow赋予Spark功能","date":"2019-03-15T08:33:35.000Z","updated":"2019-04-04T13:49:28.582Z","comments":true,"path":"2019/03/15/yuque/使用MLflow赋予Spark功能/","link":"","permalink":"http://zhos.me/2019/03/15/yuque/使用MLflow赋予Spark功能/","excerpt":"","text":"这篇文章旨在介绍我们使用MLflow的初步经验。我们将通过记录所有探索性迭代，开始使用自己的跟踪服务器发现MLflow 。然后，我们将展示使用UDF将Spark与MLflow相关联的经验。 上下文我们利用机器学习和人工智能的力量，使人们能够控制自己的健康和福祉。机器学习模型因此是我们正在开发的数据产品的核心，这就是为什么MLFLow，一个涵盖ML生命周期所有方面的开源平台引起了我们的注意。 MLflowMLflow的主要目标是在ML之上提供额外的层，允许数据科学家与几乎任何机器学习库（h2o，keras，mleap，pytorch，sklearn和tensorflow）一起工作，同时，它将他们的工作带到另一个层次。MLflow提供三个组件： 跟踪 - 记录和查询实验：代码，数据，配置和结果。跟踪建模进度非常有用。 项目 - 在任何平台（_即_ Sagemaker）上可重复运行的包装格式。 模型 - 将模型发送到各种部署工具的通用格式。 MLflow（目前处于alpha版本）是一个管理ML生命周期的开源平台，包括实验，可重复性和部署。 设置MLflow为了使用MLflow，我们首先需要设置所有Python环境以使用MLflow，我们将使用PyEnv _（_在Mac中_设置__ → _Python）。这将提供一个虚拟环境，我们可以在其中安装运行它所需的所有库。 1234pyenv install 3.7.0pyenv global 3.7.0 # Use Python 3.7mkvirtualenv mlflow # Create a Virtual Env with Python 3.7workon mlflow 安装所需的库 12345pip install mlflow==0.7.0 \\ Cython==0.29 \\ numpy==1.14.5 \\ pandas==0.23.4 \\ pyarrow==0.11.0 注意：我们使用PyArrow将模型作为UDF启动。PyArrow和Numpy版本需要修复，因为最新的版本之间存在一些冲突。 启动跟踪UIMLflow Tracking允许我们使用Python和REST API 记录和查询实验。此外，还可以定义我们将存储模型工件的位置（Localhost，Amazon S3_，_Azure Blob存储_，_Google云存储_或_SFTP服务器）。由于我们使用AWS ，因此我们将尝试将S3作为工件存储。 123456# Running a Tracking Servermlflow server \\ --file-store /tmp/mlflow/fileStore \\ --default-artifact-root s3://&lt;bucket&gt;/mlflow/artifacts/ \\ --host localhost --port 5000 MLflow建议使用持久性文件存储。这file-store是服务器存储运行和实验元数据的位置。因此，在运行服务器时，请确保这指向持久文件系统位置。在这里，我们只是/tmp用于实验。请记住，如果我们想使用mlflow服务器运行旧实验，它们必须存在于文件存储中。但是，如果没有它们，我们仍然可以在UDF中使用它们，因为只需要模型路径。 注意：请记住跟踪UI，模型客户端必须能够访问工件位置。这意味着，无论跟踪UI是否在EC2实例上，如果我们在本地运行MLflow，我们的机器应该可以直接访问S3来编写工件模型。 运行模型跟踪服务器运行后，我们可以开始训练我们的模型。作为一个例子，我们将使用MLflow Sklearn示例中提供的wine示例的修改。 1234MLFLOW_TRACKING_URI=http://localhost:5000 python wine_quality.py \\ --alpha 0.9 --l1_ration 0.5 --wine_file ./data/winequality-red.csv 如前所述，MLflow允许记录模型的参数，度量和工件，因此我们可以跟踪这些在不同迭代中如何演变。此功能非常有用，因为我们可以通过检查Tracking Server来重现我们的最佳模型，或者验证哪些代码正在执行所需的迭代，因为它记录（免费）git哈希提交。 123456789101112131415with mlflow.start_run(): ... model ... mlflow.log_param(&quot;source&quot;, wine_path) mlflow.log_param(&quot;alpha&quot;, alpha) mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio) mlflow.log_metric(&quot;rmse&quot;, rmse) mlflow.log_metric(&quot;r2&quot;, r2) mlflow.log_metric(&quot;mae&quot;, mae) mlflow.set_tag(&apos;domain&apos;, &apos;wine&apos;) mlflow.set_tag(&apos;predict&apos;, &apos;quality&apos;) mlflow.sklearn.log_model(lr, &quot;model&quot;) 服务模型 使用“ &gt; mlflow服务器&gt; _ ”_启动的&gt; MLflow跟踪服务器还托管REST API，用于跟踪运行并将数据写入本地文件系统。您可以使用“ &gt; _MLFLOW_TRACKING_URI_&gt; _”环境变量指定跟踪服务器URI，MLflow跟踪API会自动与该URI处的跟踪服务器通信，以创建/获取运行信息，记录指标等。**参考：**[文档//运行跟踪服务器](https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server)为了提供模型，我们只需要运行一个跟踪服务器（**参见_启动UI）和一个模型运行ID**。 123456# Serve a sklearn model through 127.0.0.0:5005MLFLOW_TRACKING_URI=http://0.0.0.0:5000 mlflow sklearn serve \\ --port 5005 \\ --run_id 0f8691808e914d1087cf097a08730f17 \\ --model-path model 要使用MLflow服务功能为模型提供服务，我们需要访问跟踪UI，因此只需指定即可检索模型信息--run_id 。一旦跟踪服务器为模型提供服务，我们就可以查询新的模型端点。 123456789101112131415161718192021# Query Tracking Server Endpointcurl -X POST \\ http://127.0.0.1:5005/invocations \\ -H &apos;Content-Type: application/json&apos; \\ -d &apos;[ &#123; &quot;fixed acidity&quot;: 3.42, &quot;volatile acidity&quot;: 1.66, &quot;citric acid&quot;: 0.48, &quot;residual sugar&quot;: 4.2, &quot;chloridessssss&quot;: 0.229, &quot;free sulfur dsioxide&quot;: 19, &quot;total sulfur dioxide&quot;: 25, &quot;density&quot;: 1.98, &quot;pH&quot;: 5.33, &quot;sulphates&quot;: 4.39, &quot;alcohol&quot;: 10.8 &#125;]&apos;&gt; &#123;&quot;predictions&quot;: [5.825055635303461]&#125; 从Spark运行模型虽然通过训练模型和使用服务功能（ref：mlflow // docs // models #local）实时为模型提供服务非常强大，但使用Spark（批量或流式传输）应用模型更是如此强大，因为它加入了分配力量。想象一下，进行离线训练，然后以更简单的方式将输出模型应用于所有数据。这就是Spark和MLflow共同完美的地方。 安装PySpark + Jupyter + Spark参考：PySpark开始 - Jupyter展示我们如何将MLflow模型应用于Spark数据帧。我们需要使用PySpark设置Jupyter笔记本。 123456首先下载最新的稳定Apache Spark（当前版本2.3.2）。cd ~/Downloads/tar -xzf spark-2.3.2-bin-hadoop2.7.tgzmv ~/Downloads/spark-2.3.2-bin-hadoop2.7 ~/ln -s ~/spark-2.3.2-bin-hadoop2.7 ~/spark̀ 123在我们的virtualEnv中安装PySpark和Jupyterpip install pyspark jupyter 1234567设置Environmnet变量export SPARK_HOME=~/sparkexport PATH=$SPARK_HOME/bin:$PATHexport PYSPARK_DRIVER_PYTHON=jupyterexport PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook --notebook-dir=$&#123;HOME&#125;/Projects/notebooks&quot;通过定义notebook-dir，我们将能够将我们的笔记本存储和保存在所需的文件夹中。 从PySpark启动Jupyter由于我们将Jupyter配置为PySpark驱动程序，现在我们可以使用附加到我们笔记本的PySpark上下文启动Jupyter。 1234567891011(mlflow) afranzi:~$ pyspark[I 19:05:01.572 NotebookApp] sparkmagic extension enabled![I 19:05:01.573 NotebookApp] Serving notebooks from local directory: /Users/afranzi/Projects/notebooks[I 19:05:01.573 NotebookApp] The Jupyter Notebook is running at:[I 19:05:01.573 NotebookApp] http://localhost:8888/?token=c06252daa6a12cfdd33c1d2e96c8d3b19d90e9f6fc171745[I 19:05:01.573 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 19:05:01.574 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=c06252daa6a12cfdd33c1d2e96c8d3b19d90e9f6fc171745 如上所示，MLflow提供了将模型工件记录到S3的功能。因此，一旦我们选择了一个模型，我们就可以使用该mlflow.pyfunc模块将其作为UDF导入。 1234567891011121314import mlflow.pyfuncmodel_path = &apos;s3://&lt;bucket&gt;/mlflow/artifacts/1/0f8691808e914d1087cf097a08730f17/artifacts/model&apos;wine_path = &apos;/Users/afranzi/Projects/data/winequality-red.csv&apos;wine_udf = mlflow.pyfunc.spark_udf(spark, model_path)df = spark.read.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;true&quot;).option(&apos;delimiter&apos;, &apos;;&apos;).load(wine_path)columns = [ &quot;fixed acidity&quot;, &quot;volatile acidity&quot;, &quot;citric acid&quot;, &quot;residual sugar&quot;, &quot;chlorides&quot;, &quot;free sulfur dioxide&quot;, &quot;total sulfur dioxide&quot;, &quot;density&quot;, &quot;pH&quot;, &quot;sulphates&quot;, &quot;alcohol&quot; ] df.withColumn(&apos;prediction&apos;, wine_udf(*columns)).show(100, False) 到目前为止，我们已经展示了如何通过在我们所有的葡萄酒数据集中运行葡萄酒质量预测来将PySpark与MLflow结合使用。但是当您需要使用Scala Spark的Python MLflow模块时会发生什么？我们还设法通过在Scala和Python之间共享Spark Context来测试它。这意味着我们注册的MLflow UDF在Python，然后从斯卡拉用它（叶氏，不是一个很好的解决方案，但至少它的东西🍭）。 Scala Spark + MLflow对于此示例，我们将Toree 内核添加到现有的Jupyter中。安装Spark + Toree + Jupyter 123pip install toreejupyter toree install --spark_home=/Users/afranzi/spark --sys-prefixjupyter kernelspec list 123Available kernels: apache_toree_scala /Users/afranzi/.virtualenvs/mlflow/share/jupyter/kernels/apache_toree_scala python3 /Users/afranzi/.virtualenvs/mlflow/share/jupyter/kernels/python3 正如您在附带的笔记本中看到的，UDF在Spark和PySpark之间共享。我们希望这最后一部分对那些喜欢Scala并且必须将ML模型投入生产的团队有所帮助。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103In [1]:import org.apache.spark.sql.functions.colimport org.apache.spark.sql.types.StructTypeimport org.apache.spark.sql.&#123;Column, DataFrame&#125;import scala.util.matching.Regexval FirstAtRe: Regex = \"^_\".rval AliasRe: Regex = \"[\\\\s_.:@]+\".rdef getFieldAlias(field_name: String): String = &#123; FirstAtRe.replaceAllIn(AliasRe.replaceAllIn(field_name, \"_\"), \"\")&#125;def selectFieldsNormalized(columns: List[String])(df: DataFrame): DataFrame = &#123; val fieldsToSelect: List[Column] = columns.map(field =&gt; col(field).as(getFieldAlias(field)) ) df.select(fieldsToSelect: _*)&#125;def normalizeSchema(df: DataFrame): DataFrame = &#123; val schema = df.columns.toList df.transform(selectFieldsNormalized(schema))&#125;FirstAtRe = ^_AliasRe = [\\s_.:@]+getFieldAlias: (field_name: String)StringselectFieldsNormalized: (columns: List[String])(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFramenormalizeSchema: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrameOut[1]:[\\s_.:@]+In [2]:val winePath = \"~/Research/mlflow-workshop/examples/wine_quality/data/winequality-red.csv\"val modelPath = \"/tmp/mlflow/artifactStore/0/96cba14c6e4b452e937eb5072467bf79/artifacts/model\"winePath = ~/Research/mlflow-workshop/examples/wine_quality/data/winequality-red.csvmodelPath = /tmp/mlflow/artifactStore/0/96cba14c6e4b452e937eb5072467bf79/artifacts/modelOut[2]:/tmp/mlflow/artifactStore/0/96cba14c6e4b452e937eb5072467bf79/artifacts/modelIn [3]:val df = spark.read .format(\"csv\") .option(\"header\", \"true\") .option(\"delimiter\", \";\") .load(winePath) .transform(normalizeSchema)df = [fixed_acidity: string, volatile_acidity: string ... 10 more fields]Out[3]:[fixed_acidity: string, volatile_acidity: string ... 10 more fields]In [4]:%%PySparkimport mlflowfrom mlflow import pyfuncmodel_path = \"/tmp/mlflow/artifactStore/0/96cba14c6e4b452e937eb5072467bf79/artifacts/model\"wine_quality_udf = mlflow.pyfunc.spark_udf(spark, model_path)spark.udf.register(\"wineQuality\", wine_quality_udf)Out[4]:&lt;function spark_udf.&lt;locals&gt;.predict at 0x1116a98c8&gt;In [6]:df.createOrReplaceTempView(\"wines\")In [10]:%%SQLSELECT quality, wineQuality( fixed_acidity, volatile_acidity, citric_acid, residual_sugar, chlorides, free_sulfur_dioxide, total_sulfur_dioxide, density, pH, sulphates, alcohol ) AS predictionFROM winesLIMIT 10Out[10]:+-------+------------------+|quality| prediction|+-------+------------------+| 5| 5.576883967129615|| 5| 5.50664776916154|| 5| 5.525504822954496|| 6| 5.504311247097457|| 5| 5.576883967129615|| 5|5.5556903912725755|| 5| 5.467882654744997|| 7| 5.710602976324739|| 7| 5.657319539336507|| 5| 5.345098606538708|+-------+------------------+In [17]:spark.catalog.listFunctions.filter('name like \"%wineQuality%\").show(20, false)+-----------+--------+-----------+---------+-----------+|name |database|description|className|isTemporary|+-----------+--------+-----------+---------+-----------+|wineQuality|null |null |null |true |+-----------+--------+-----------+---------+-----------+ 下一步尽管MLflow目前处于Alpha（🥁），但它看起来很有希望。只需能够运行多个机器学习框架并从同一端点使用它们，它就能将所有推荐系统提升到新的水平。此外，MLflow 通过在它们之间建立公共层，使&gt; 数据工程师和数据科学家&gt; 更加接近 。&gt;在对MLflow进行这项研究之后，我们确信我们将进一步研究它并开始在Spark管道和我们的推荐系统中使用它。让文件存储与数据库同步而不是使用FS会很好。这应该允许多个端点使用相同的文件存储。就像使用相同的Glue Metastore有多个Presto和Athena实例一样。最后，感谢MLflow背后的所有社区，使其成为可能，让我们的数据更有趣。如果您正在玩MLflow，请随时联系我们并提供有关您如何使用它的一些反馈！更重要的是，如果您在生产中使用MLflow。","categories":[],"tags":[]},{"title":"MLflow模型","slug":"yuque/MLflow模型","date":"2019-03-15T07:28:46.000Z","updated":"2019-04-04T13:49:28.582Z","comments":true,"path":"2019/03/15/yuque/MLflow模型/","link":"","permalink":"http://zhos.me/2019/03/15/yuque/MLflow模型/","excerpt":"","text":"MLflow模型MLflow模型是用于打包机器学习模型的标准格式，可用于各种下游工具 - 例如，通过REST API实时提供服务或在Apache Spark上进行批量推理。该格式定义了一种约定，允许您以不同的“风格”保存模型，这些“风味”可以被不同的下游工具理解。 目录 存储格式 模型API 内置型号口味 自定义口味 内置部署工具存储格式每个MLflow模型都是一个包含任意文件MLmodel 的目录，以及目录根目录中的一个文件，该文件可以定义可以查看模型的多种_风格_。使MLflow模型功能强大的关键概念：它们是部署工具可用于理解模型的约定，这使得编写可与任何ML库中的模型一起使用的工具成为可能，而无需将每个工具与每个库集成。MLflow定义了所有内置部署工具支持的几种“标准”风格，例如描述如何将模型作为Python函数运行的“Python函数”风格。但是，库也可以定义和使用其他类型。例如，MLflow的mlflow.sklearn库允许将模型加载为scikit-learn Pipeline对象，以便在知道scikit-learn的代码中使用，或者作为通用Python函数用于仅需要应用模型的工具（例如，工具）用于将模型部署到Amazon SageMaker）。特定模型支持的所有风格都MLmodel以YAML格式在其文件中定义。例如，mlflow.sklearn输出模型如下： 1234# Directory written by mlflow.sklearn.save_model(model, &quot;my_model&quot;)my_model/├── MLmodel└── model.pkl 它的MLmodel文件描述了两种风格： 12345678time_created: 2018-05-25T17:28:53.35flavors: sklearn: sklearn_version: 0.19.1 pickled_model: model.pkl python_function: loader_module: mlflow.sklearn 该模型然后可以与任何支持工具中使用_任一_的sklearn或 python_function模型的味道。例如，该命令可以为具有flavor 的模型提供服务：mlflow sklearn`sklearn` 1mlflow sklearn serve my_model 此外，命令行工具可以将模型打包并部署到AWS SageMaker，只要它们支持这种风格：mlflow sagemaker`python_function` 1mlflow sagemaker deploy -m my_model [other options] MLmodel格式的字段除了从口味场列出了模型的口味，在MLmodel YAML格式可以包含以下字段： TIME_CREATED创建模型的日期和时间，采用UTC ISO 8601格式。 run_id如果使用MLflow Tracking保存模型，则创建模型的运行的ID 。 模型API您可以通过多种方式保存和加载MLflow模型。首先，MLflow包括与几个公共库的集成。例如，scikit-learn模型的[mlflow.sklearn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#module-mlflow.sklearn)包括save_model，log_model和load_model函数。其次，您可以使用mlflow.models.Model该类来创建和编写模型。这个类有四个关键功能： add_flavor为模型添加味道。每个flavor都有一个字符串名称和一个键值属性字典，其中值可以是任何可以序列化为YAML的对象。 save 将模型保存到本地目录。 log 使用MLflow Tracking将模型记录为当前运行中的工件。 load 从本地目录或先前运行中的工件加载模型。内置型号口味MLflow提供了几种可能在您的应用程序中有用的标准风格。具体来说，它的许多部署工具都支持这些风格，因此您可以将这些风格导出自己的模型，以便从所有这些工具中受益。Python函数（python_function）该python_function模型的味道定义了一个通用的文件系统格式为Python模型和保存和加载模型，并从该格式提供了工具。该格式是自包含的，因为它包含加载和使用模型所需的所有信息。依赖关系直接存储在模型中或通过Conda环境引用。python_function模型的约定是具有predict以下签名的方法或函数： 1predict(data: pandas.DataFrame) -&gt; [pandas.DataFrame | numpy.array] 其他MLflow组件期望python_function模型遵循此约定。该python_function模型格式定义为包含所有所需的数据的目录结构，代码和配置： 12345./dst-path/ ./MLmodel: configuration &lt;code&gt;: code packaged with the model (specified in the MLmodel file) &lt;data&gt;: data packaged with the model (specified in the MLmodel file) &lt;env&gt;: Conda environment definition (specified in the MLmodel file) 一个python_function模式目录必须包含MLmodel在其与“python_function”格式和下列参数的根文件： loader_module [required]:可以加载模型的Python模块。期望是mlflow.sklearn可导入的模块标识符（例如）importlib.import_module。导入的模块必须包含具有以下签名的函数：&gt; _load_pyfunc（path：string） - &gt; path参数由data参数指定，可以引用文件或目录。 code [optional]:包含此模型打包的代码的目录的相对路径。在导入模型加载器之前，此目录中的所有文件和目录都将添加到Python路径中。 data [optional]:包含模型数据的文件或目录的相对路径。路径传递给模型加载器。 env [可选]：导出的Conda环境的相对路径。如果存在，则在运行模型之前激活此环境。 案例1tree example/sklearn_iris/mlruns/run1/outputs/linear-lr 1234567├── MLmodel├── code│ ├── sklearn_iris.py│├── data│ └── model.pkl└── mlflow_env.yml 1cat example/sklearn_iris/mlruns/run1/outputs/linear-lr/MLmodel 123456python_function: code: code data: data/model.pkl loader_module: mlflow.sklearn env: mlflow_env.yml main: sklearn_iris 有关更多信息，请参阅mlflow.pyfunc。 H O（h2o）H2O模型风味可以记录和加载H2O模型。这些模型将通过使用保存mlflow.h2o.save_model。使用mlflow.h2o.log_model也将启用有效的味道。Python Function将H2O模型作为PyFunc模型加载时，h2o.init(...)将调用。因此，正确版本的h2o（-py）必须在环境中。给出的参数h2o.init(...)可以model.h2o/h2o.yaml在键下定制init。有关更多信息，请参阅mlflow.h2o。 Keras（keras）该keras模型味道启用日志记录和装载Keras模型。该模型将通过Keras提供的model_save功能以HDF5文件格式保存。此外，模型可以加载为。有关更多信息，请参阅。Python Functionmlflow.keras MLeap（mleap）该mleap模型风味支持使用MLeap持久性机制保存模型。该mlflow/java软件包中提供了一个用于加载具有MLeap风格格式的MLflow模型的配套模块。有关更多信息，请参阅mlflow.mleap。 PyTorch（pytorch）该pytorch模型味道启用日志记录和装载PyTorch模型。模型使用torch.save（模型）方法以.pth格式完全存储。给定包含已保存模型的目录，您可以将模型记录到MLflow via 。然后可以加载保存的模型以进行推理。有关更多信息，请参阅。log_saved_model`mlflow.pyfunc.load_pyfunc()[mlflow.pytorch`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#module-mlflow.pytorch) Scikit-learn（sklearn）该sklearn模型的味道提供了一个简单易用的界面来处理scikit学习模式，没有外部的依赖关系。它使用Python的pickle模块保存和加载模型，并生成有效的 python_functionflavor模型。有关更多信息，请参阅mlflow.sklearn。 Spark MLlib（spark）该spark模型的味道能使出口星火MLlib模型作为MLflow模型。导出的模型使用Spark MLLib的本机序列化进行保存，然后可以作为MLlib模型加载回来或作为python_function模型进行部署。当部署为a时python_function，模型会创建自己的SparkContext，并在评分之前将pandas DataFrame输入转换为Spark DataFrame。虽然这不是最有效的解决方案，尤其是对于实时评分，但它使您能够轻松地将任何MLlib PipelineModel（只要PipelineModel没有外部JAR依赖性）部署到MLflow支持的任何端点。有关更多信息，请参阅mlflow.spark。 TensorFlow（tensorflow）该tensorflow模型的味道使记录TensorFlow 并加载它们早在模型上大熊猫DataFrames推断。给定包含已保存模型的目录，您可以将模型记录到MLflow ，然后使用加载保存的模型进行推理。有关更多信息，请参阅。Saved Models`Python Functionlog_saved_modelmlflow.pyfunc.load_pyfunc[mlflow.tensorflow`](https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#module-mlflow.tensorflow) 自定义口味您可以在MLmodel文件中添加一种风格，可以通过直接编写或使用mlflow.models.Model类构建它。为您的风味选择一个任意的字符串名称。MLflow工具忽略他们不理解的MLmodel文件中的风格。 内置部署工具MLflow提供了在本地计算机和多个生产环境中部署模型的工具。并非所有部署方法都适用于所有型号的风格。Python函数格式和所有兼容格式支持部署。 将python_function模型部署为本地REST API端点 python_function在Microsoft Azure ML上部署模型 python_function在Amazon SageMaker上部署模型 将python_function模型导出为Apache Spark UDF将python_function模型部署为本地REST API端点MLflow可以在本地部署模型作为本地REST API端点，或直接对CSV文件进行评分。在部署到远程模型服务器之前，此功能是测试模型的便捷方式。您可以使用CLI界面在本地部署Python函数flavor到mlflow.pyfunc模块。本地REST API服务器接受以下数据格式作为输入： JSON序列化的pandas DataFrames的split方向。例如， 。使用 请求标头值指定此格式。从MLflow 0.9.0开始，如果是（即没有格式规范），这将是默认格式。data = pandas_df.to_json(orient=&#39;split&#39;)`Content-Typeapplication/json; format=pandas-splitContent-Typeapplication/json` JSON序列化的pandas DataFrames的records方向。我们不建议使用此格式，因为无法保证保留列顺序。目前，使用 或的Content-Type请求标头值 指定此格式。从MLflow 0.9.0开始，将参考 格式。为了向前兼容，我们建议使用格式或指定内容类型。application/json; format=pandas-records`application/jsonapplication/jsonsplitsplitapplication/json; format=pandas-records` CSV序列化的pandas DataFrames。例如，。使用请求标头值指定此格式。data = pandas_df.to_csv()`Content-Typetext/csv` 有关序列化pandas DataFrames的更多信息，请参阅 pandas.DataFrame.to_json。 命令 serve 将模型部署为本地REST API服务器。 predict 使用该模型生成本地CSV文件的预测。 有关详细信息，请参阅： 123mlflow pyfunc - help mlflow pyfunc serve - help mlflow pyfunc predict - help python_function在Microsoft Azure ML上部署模型该mlflow.azureml模块可以将python_function模型打包到Azure ML容器映像中。这些映像可以部署到Azure Kubernetes服务（AKS）和Azure容器实例（ACI）平台，以实现实时服务。生成的Azure ML ContainerImage包含一个Web服务器，它接受以下数据格式作为输入： JSON序列化的pandas DataFrames的split方向。例如，。使用请求标头值指定此格式。data = pandas_df.to_json(orient=&#39;split&#39;)`Content-Typeapplication/json` build_image向现有Azure ML工作区注册MLflow模型，并构建Azure ML容器映像以部署到AKS和ACI。在Azure的ML SDK要求才能使用此功能。Azure ML SDK需要Python 3.它不能与早期版本的Python一起安装。 使用Python API的示例工作流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import mlflow.azuremlfrom azureml.core import Workspacefrom azureml.core.webservice import AciWebservice, Webservice# Create or load an existing Azure ML workspace. You can also load an existing workspace using# Workspace.get(name=\"&lt;workspace_name&gt;\")workspace_name = \"&lt;Name of your Azure ML workspace&gt;\"subscription_id = \"&lt;Your Azure subscription ID&gt;\"resource_group = \"&lt;Name of the Azure resource group in which to create Azure ML resources&gt;\"location = \"&lt;Name of the Azure location (region) in which to create Azure ML resources&gt;\"azure_workspace = Workspace.create(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group, location=location, create_resource_group=True, exist_okay=True)# Build an Azure ML container image for deploymentazure_image, azure_model = mlflow.azureml.build_image(model_path=\"&lt;path-to-model&gt;\", workspace=azure_workspace, description=\"Wine regression model 1\", synchronous=True)# If your image build failed, you can access build logs at the following URI:print(\"Access the following URI for build logs: &#123;&#125;\".format(azure_image.image_build_log_uri))# Deploy the container image to ACIwebservice_deployment_config = AciWebservice.deploy_configuration()webservice = Webservice.deploy_from_image( image=azure_image, workspace=azure_workspace, name=\"&lt;deployment-name&gt;\")webservice.wait_for_deployment()# After the image deployment completes, requests can be posted via HTTP to the new ACI# webservice's scoring URI. The following example posts a sample input from the wine dataset# used in the MLflow ElasticNet example:# https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wineprint(\"Scoring URI is: %s\", webservice.scoring_uri)import requestsimport json# `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientationsample_input = &#123; \"columns\": [ \"alcohol\", \"chlorides\", \"citric acid\", \"density\", \"fixed acidity\", \"free sulfur dioxide\", \"pH\", \"residual sugar\", \"sulphates\", \"total sulfur dioxide\", \"volatile acidity\" ], \"data\": [ [8.8, 0.045, 0.36, 1.001, 7, 45, 3, 20.7, 0.45, 170, 0.27] ]&#125;response = requests.post( url=webservice.scoring_uri, data=json.dumps(sample_input), headers=&#123;\"Content-type\": \"application/json\"&#125;)response_json = json.loads(response.text)print(response_json) Example workflow using the MLflow CLI123456789101112131415161718192021222324252627282930313233343536mlflow azureml build-image -w &lt;workspace-name&gt; -m &lt;model-path&gt; -d \"Wine regression model 1\"az ml service create aci -n &lt;deployment-name&gt; --image-id &lt;image-name&gt;:&lt;image-version&gt;# After the image deployment completes, requests can be posted via HTTP to the new ACI# webservice's scoring URI. The following example posts a sample input from the wine dataset# used in the MLflow ElasticNet example:# https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_winescoring_uri=$(az ml service show --name &lt;deployment-name&gt; -v | jq -r \".scoringUri\")# `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientationsample_input='&#123; \"columns\": [ \"alcohol\", \"chlorides\", \"citric acid\", \"density\", \"fixed acidity\", \"free sulfur dioxide\", \"pH\", \"residual sugar\", \"sulphates\", \"total sulfur dioxide\", \"volatile acidity\" ], \"data\": [ [8.8, 0.045, 0.36, 1.001, 7, 45, 3, 20.7, 0.45, 170, 0.27] ]&#125;'echo $sample_input | curl -s -X POST $scoring_uri\\-H 'Cache-Control: no-cache'\\-H 'Content-Type: application/json'\\-d @- For more info, see: 12mlflow azureml --helpmlflow azureml build-image --help python_function在Amazon SageMaker上部署模型该mlflow.sagemaker模块可以python_function在具有SageMaker兼容环境的Docker容器中本地部署模型，并在SageMaker上远程部署模型。要远程部署到SageMaker，您需要设置环境和用户帐户。要将自定义模型导出到SageMaker，您需要在Amazon ECR上提供与MLflow兼容的Docker镜像。MLflow提供默认的Docker镜像定义; 但是，由您来构建映像并将其上载到ECR。MLflow包括build_and_push_container执行此步骤的效用函数。构建和上传后，您可以将MLflow容器用于所有MLflow模型。使用该mlflow.sagemaker 模块部署的模型Web服务器接受以下数据格式作为输入，具体取决于部署风格： python_function：对于此部署风格，端点接受与pyfunc服务器相同的格式。pyfunc部署文档中描述了这些格式 。 mleap：对于此部署风格，端点仅接受split方向上的 JSON序列化pandas DataFrame 。例如， 。使用 请求标头值指定此格式。data = pandas_df.to_json(orient=&#39;split&#39;)`Content-Typeapplication/json` 命令 run-local在Docker容器中本地部署模型。图像和环境应与模型远程运行的方式相同，因此在部署之前测试模型非常有用。 该build-and-push-containerCLI命令构建一个MLfLow多克尔图像并上传到ECR。调用者必须设置正确的权限。图像是本地构建的，并且需要Docker存在于执行此步骤的计算机上。 deploy在Amazon SageMaker上部署模型。MLflow将Python Function模型上传到S3并启动为该模型提供服务的Amazon SageMaker端点。 使用MLflow CLI的示例工作流程 123mlflow sagemaker build-and-push-container - build the container (only needs to be called once)mlflow sagemaker run-local -m &lt;path-to-model&gt; - remotely For more info, see: 1234mlflow sagemaker --helpmlflow sagemaker build-and-push-container --helpmlflow sagemaker run-local --helpmlflow sagemaker deploy --help 将python_function模型导出为Apache Spark UDF您可以将python_function模型输出为Apache Spark UDF，可以将其上载到Spark群集并用于对模型进行评分。例 12pyfunc_udf = mlflow.pyfunc.spark_udf(&lt;path-to-model&gt;)df = spark_df.withColumn(&quot;prediction&quot;, pyfunc_udf(&lt;features&gt;)) 生成的UDF基于Spark的Pandas UDF，目前仅限于为每个观察生成单个值或相同类型的值数组。默认情况下，我们将第一个数字列作为double返回。您可以通过提供result_type 参数来控制返回的结果。支持以下值： &#39;int&#39;或IntegerType：int32返回可以适合结果的最左边的整数， 如果没有，则引发异常。 &#39;long&#39;或LongType：int64 返回可以适合结果的最左边的长整数，如果没有则引发异常。 数组类型（IntegerType | LongType）：返回一个可以放入所需尺寸的所有整数列。 &#39;float&#39;或FloatType：float32如果没有数字列，则返回最左边的数字结果转换 为或引发异常。 &#39;double&#39;或DoubleType：double如果没有数字列，则返回最左边的数字结果转换 为或引发异常。 ArrayType（FloatType | DoubleType）：返回强制转换为请求的所有数字列。类型。如果存在数字列，则会引发异常。 &#39;string&#39;或StringType：Result是最左边的列转换为字符串。 ArrayType（StringType）：返回转换为字符串的所有列。 1234from pyspark.sql.types import ArrayType, FloatTypepyfunc_udf = mlflow.pyfunc.spark_udf(&lt;path-to-model&gt;, result_type=ArrayType(FloatType()))# The prediction column will contain all the numeric columns returned by the model as floatsdf = spark_df.withColumn(&quot;prediction&quot;, pyfunc_udf(&lt;features&gt;))","categories":[],"tags":[]},{"title":"MLflow项目","slug":"yuque/MLflow项目","date":"2019-03-15T06:49:30.000Z","updated":"2019-04-04T13:49:28.582Z","comments":true,"path":"2019/03/15/yuque/MLflow项目/","link":"","permalink":"http://zhos.me/2019/03/15/yuque/MLflow项目/","excerpt":"","text":"MLflow项目MLflow项目是一种以可重用和可重复的方式打包数据科学代码的格式，主要基于约定。此外，Projects组件包括用于运行项目的API和命令行工具，可以将项目链接到工作流中。目录 概观 指定项目 运行项目 快速迭代 构建多步骤工作流程概观MLflow Projects的核心只是组织和描述代码的惯例，让其他数据科学家（或自动化工具）运行它。每个项目都只是一个包含代码的文件目录或Git存储库。MLflow可以根据将文件放在此目录中的约定来运行某些项目（例如，conda.yaml文件将被视为 Conda环境），但您可以通过添加MLproject文件来更详细地描述您的项目，该文件是YAML格式的文本文件。每个项目都可以指定几个属性： 名称项目的可读名称。 依赖运行项目所需的库。MLflow目前使用 Conda包管理器，它支持Python包和本机库（例如，CuDNN或Intel MKL），以指定依赖关系。MLFLOW_CONDA_HOME如果指定，MLflow将使用环境变量给出的Conda安装（例如，通过调用运行Conda命令$MLFLOW_CONDA_HOME/bin/conda），conda否则默认运行。 入口点可以在项目中执行的命令，以及有关其参数的信息。大多数项目至少包含一个您希望其他用户调用的入口点。某些项目还可以包含多个入口点：例如，您可能拥有一个包含多个特征化算法的Git存储库。您还可以将项目中的任何文件.py或.sh文件作为入口点调用。MLproject但是，如果在文件中列出入口点，则还可以为它们指定_参数_，包括数据类型和默认值。您可以使用 命令行工具或Python API 从Git URI或本地目录运行任何项目。这些API还允许在Databricks上提交项目以进行远程执行。mlflow runmlflow.projects.run() 警告默认情况下，MLflow将为Git项目使用一个新的临时工作目录。这意味着您通常应该使用绝对路径而不是相对路径将任何文件参数传递给MLflow项目。如果您的项目声明了其参数，MLflow将自动为类型参数创建绝对路径path。 指定项目默认情况下，任何Git存储库或本地目录都被视为项目，MLflow使用以下约定来确定其参数： 项目的名称是目录的名称。 该conda中指定的conda.yaml，如果存在的话。如果没有conda.yaml文件，MLflow将在运行项目时使用仅包含Python的Conda环境（特别是Conda可用的最新Python）。 项目中的任何.py和.sh文件都可以是一个入口点，没有显式声明参数。当您使用一组参数执行此类命令时，MLflow将使用语法传递命令行上的每个参数。--key value 1234567891011121314name: My Projectconda_env: my_env.yamlentry_points: main: parameters: data_file: path regularization: &#123;type: float, default: 0.1&#125; command: \"python train.py -r &#123;regularization&#125; &#123;data_file&#125;\" validate: parameters: data_file: path command: \"python validate.py &#123;data_file&#125;\" 如您所见，该文件可以指定名称和不同的环境文件，以及有关每个入口点的更多详细信息。具体来说，每个入口点都有一个运行_命令_和_参数_（包括数据类型）。接下来我们将描述这两个部分。 命令语法在文件中指定入口点时MLproject，该命令可以是Python 格式字符串语法中的任何字符串 。在入口点的parameters字段中声明的所有参数都将传递到此字符串中以进行替换。如果使用字段中_未_列出的 其他参数调用项目parameters，MLflow将使用语法传递它们，因此您可以使用该文件仅为参数的子集声明类型和默认值。--key value`MLproject`在替换命令中的参数之前，MLflow使用Python的shlex.quote函数转义它们 ，因此您无需担心在命令字段中添加引号。 指定参数MLflow允许为每个参数指定数据类型和默认值。您可以通过编写以下内容来指定数据类型： 1parameter_name: data_type 在您的YAML文件中，或者使用以下语法之一（在YAML中等效）添加默认值： 12345parameter_name: &#123;type: data_type, default: value&#125; # Short syntaxparameter_name: # Long syntax type: data_type default: value MLflow支持四种参数类型，其中一些特殊处理（例如，将数据下载到本地文件）。任何未声明的参数都被视为string。参数类型是： string任何文字字符串。 float一个真实的数字。MLflow验证参数是否为数字。 path本地文件系统上的路径。MLflow会将为此类参数传递的任何相对路径转换为绝对路径，并且还会将作为分布式存储URI（s3://和dbfs://）传递的任何路径下载到本地文件。将此类型用于只能读取本地文件的程序。 URI本地或分布式存储系统中的数据URI。MLflow会将任何相对路径转换为绝对路径，如path类型中所示。对于知道如何从分布式存储中读取的程序（例如使用Spark），请使用此类型。 运行项目MLflow提供了两种简单的方法来运行项目：命令行工具或Python API。这两个工具都采用以下参数：mlflow run mlflow.projects.run() 项目URI可以是本地文件系统上的目录，也可以是Git存储库路径，指定为表单的URI https://&lt;repo&gt;（使用HTTPS）或user@host:path （通过SSH使用Git）。要针对位于项目子目录中的MLproject文件运行，请在URI参数的末尾添加“＃”，然后是从项目根目录到包含所需项目的子目录的相对路径。 项目版本对于基于Git的项目，要运行的Git存储库中的提交哈希或分支名称。 入口点要使用的入口点的名称，默认为main。您可以使用MLproject文件中指定的任何入口点，或项目中的任何.py或.sh文件，作为项目根目录中的路径（例如，src/test.py）。 参数键值参数。如果需要，将验证并转换具有声明类型的任何参数 。 部署模式如果您有Databricks帐户，命令行和API都允许您在Databricks环境中远程启动项目。这包括设置群集参数，例如VM类型。当然，您还可以使用本地版本的 命令在您选择的任何其他计算基础架构上运行项目（例如，提交对标准作业排队系统执行的脚本）。mlflow run例如，本教程创建并发布一个训练线性模型的ML项目。该项目也在GitHub上发布，网址为https://github.com/mlflow/mlflow-example。要执行此项目，请运行： 1mlflow run git@github.com:mlflow/mlflow-example.git -P alpha=0.5 还有其他选项可用于禁用Conda环境的创建，如果您希望在现有shell环境中快速测试项目，这将非常有用。 Databricks上的远程执行在Databricks上远程运行项目的支持处于beta预览阶段，需要Databricks帐户。要接收有关该功能的未来更新，请在此处注册。 在Databricks上启动远程执行要使用此功能，您需要拥有Databricks帐户（尚不支持Community Edition），并且您必须设置Databricks命令行实用程序。在Databricks文档中找到更详细的说明（此处为Azure Databricks，此处为AWS上的Databricks）。有关如何使用该功能的简要概述如下：首先，创建一个包含 运行的集群规范的JSON文件 。然后，通过运行您的项目 1mlflow run &lt;uri&gt; -m databricks --cluster-spec &lt;json-cluster-spec&gt; 必须是Git存储库URI。您还可以通过git-username和git-password参数（或通过MLFLOW_GIT_USERNAME和 MLFLOW_GIT_PASSWORD环境变量）传递Git凭据 。 快速迭代如果要快速开发项目，我们建议创建一个MLproject文件，将主程序指定为main入口点，并运行它。为避免重复写入，您可以在文件中添加默认参数。mlflow run . 构建多步骤工作流程mlflow.projects.run()API，结合mlflow.tracking，使得可以构建具有不同的项目（或入口点在同一个项目）作为单独的步骤的多步骤的工作流程。每次调用都会mlflow.projects.run()返回一个运行对象，您可以使用它 mlflow.tracking来确定运行何时结束并获取其输出工件。然后，这些伪像可以被传递到另一个步骤，该步骤需要path或uri参数。您可以在单个Python程序中协调所有工作流，该程序查看每个步骤的结果，并使用自定义代码决定接下来要提交的内容。一些示例用于多步骤工作流的案例包括： 模块化您的数据科学代码不同的用户可以发布可重用的步骤，用于数据特征化，培训，验证等，其他用户或团队可以在他们的工作流程中运行。由于MLflow支持Git版本控制，因此另一个团队可以将其工作流程锁定到项目的特定版本，或者按照自己的计划升级到新版本。 超参数调整使用mlflow.projects.run()您可以在本地计算机上或在像Databricks这样的云平台上并行启动多个运行。然后，您的驱动程序可以实时检查每次运行的指标，以取消运行，启动新运行或选择目标指标上运行最佳的运行。 交叉验证有时您希望在培训和验证数据的不同随机分组上运行相同的培训代码。使用MLflow项目，您可以以允许此方式打包项目，例如，通过将列车/验证拆分的随机种子作为参数，或者首先调用可以拆分输入数据的另一个项目。有关如何构建此类多步骤工作流的示例，请参阅MLflow Multistep 工作流示例项目。","categories":[],"tags":[]},{"title":"MLFLOW概念","slug":"yuque/MLFLOW概念","date":"2019-03-15T03:36:04.000Z","updated":"2019-04-04T13:49:28.582Z","comments":true,"path":"2019/03/15/yuque/MLFLOW概念/","link":"","permalink":"http://zhos.me/2019/03/15/yuque/MLFLOW概念/","excerpt":"","text":"概念MLflow分为三个部分：跟踪，项目和 模型。您可以自己使用这些组件中的每一个 - 例如，您可能希望以MLflow的模型格式导出模型而不使用跟踪或项目 - 但它们也可以很好地协同工作。MLflow的核心理念是在您的工作流程中尽可能少地设置约束：它可以与任何机器学习库一起使用，按惯例确定代码的大部分内容，并且只需要很少的更改即可集成到现有的代码库中。与此同时，MLflow旨在采用以其格式编写的任何代码库，并使其可由多个数据科学家重现和重用。在这个页面上，我们描述了典型的ML工作流程以及MLflow适合的位置。 机器学习工作流程机器学习需要尝试各种数据集，数据准备步骤和算法，以构建最大化某些目标指标的模型。构建模型后，还需要将其部署到生产系统，监控其性能，并不断对新数据进行重新训练，并与其他模型进行比较。因此，通过机器学习提高效率可能具有挑战性，原因如下： 跟踪实验很困难。当您只是处理笔记本电脑上的文件或交互式笔记本时，如何判断哪些数据，代码和参数可以获得特定结果？ 重现代码很困难。即使您仔细跟踪代码版本和参数，也需要捕获整个环境（例如，库依赖项）以再次获得相同的结果。如果您希望其他数据科学家使用您的代码，或者您希望在另一个平台（例如，在云中）大规模运行相同的代码，则这尤其具有挑战性。 没有标准的方法来打包和部署模型。每个数据科学团队为其使用的每个ML库提出自己的方法，并且模型与产生它的代码和参数之间的链接经常丢失。 此外，虽然各个ML库提供了一些问题的解决方案（例如，模型服务），但为了获得最佳结果，您通常需要尝试多个ML库。MLflow允许您使用任何库来训练，重用和部署模型，并将它们打包成可重复的步骤，其他数据科学家可以将其用作“黑匣子”，甚至无需知道您正在使用哪个库。 MLflow组件MLflow提供三个组件来帮助管理ML工作流程：MLflow Tracking是一个API和UI，用于在运行机器学习代码时记录参数，代码版本，度量和输出文件，以及以后可视化结果。您可以在任何环境（例如，独立脚本或笔记本）中使用MLflow Tracking将结果记录到本地文件或服务器，然后比较多次运行。团队还可以使用它来比较来自不同用户的结果。MLflow Projects是打包可重用数据科学代码的标准格式。每个项目只是一个包含代码或Git存储库的目录，并使用描述符文件或简单约定来指定其依赖关系以及如何运行代码。例如，项目可以包含conda.yaml用于指定Python Conda环境的文件。在项目中使用MLflow Tracking API时，MLflow会自动记住执行的项目版本（例如，Git commit）和任何参数。您可以从GitHub或您自己的Git存储库轻松运行现有的MLflow项目，并将它们链接到多步骤工作流程。MLflow模型提供多种包装机器学习模型的约定，以及各种帮助您部署它们的工具。每个模型都保存为包含任意文件的目录和描述文件，该文件列出了可以使用模型的几种“风格”。例如，TensorFlow模型可以作为TensorFlow DAG加载，也可以作为Python函数加载到输入数据。MLflow提供了将许多常见模型类型部署到不同平台的工具：例如，任何支持“Python函数”风格的模型都可以部署到基于Docker的REST服务器，云平台（如Azure ML和AWS SageMaker），以及Apache Spark中的用户定义函数，用于批处理和流式推理。如果您使用Tracking API输出MLflow模型，MLflow还会自动记住哪个项目并运行它们。 可扩展性和大数据数据是在机器学习中获得良好结果的关键，因此MLflow旨在扩展到大型数据集，大型输出文件（例如，模型）和大量实验。具体来说，MLflow支持三维扩展： 单个MLflow运行可以在分布式集群上执行，例如，使用 Apache Spark。您可以在所选的分布式基础架构上启动运行，并将结果报告给Tracking Server以进行比较。MLflow包含一个内置API，可在Databricks上启动运行。 MLflow支持与不同参数并行启动多个运行，例如，用于超参数调整。您只需使用Projects API启动多次运行，使用Tracking API跟踪它们。 MLflow Projects可以从分布式存储系统（如AWS S3和DBFS）获取输入和写入输出。MLflow可以在本地自动下载此类文件，以用于只能在本地文件上运行的项目，或者如果支持，则为项目提供分布式存储URI。这意味着您可以编写构建大型数据集的项目，例如创建100 TB文件。示例用例无论您是单独工作的数据科学家还是大型组织的一员，您都可以通过多种方式使用MLflow：个人数据科学家可以使用MLflow跟踪在其机器上本地跟踪实验，在项目中组织代码以供将来重用，以及生产工程师随后可以使用MLflow的部署工具部署的输出模型。MLflow Tracking默认情况下只读取和写入文件到本地文件系统，因此无需部署服务器。数据科学团队可以部署MLflow跟踪服务器来记录和比较处理同一问题的多个用户的结果。通过设置用于命名其参数和度量的约定，他们可以尝试不同的算法来解决相同的问题，然后再次对新数据运行相同的算法以比较未来的模型。此外，任何人都可以下载并运行另一个模型。大型组织可以使用MLflow共享项目，模型和结果。任何团队都可以使用MLflow项目运行另一个团队的代码，因此组织可以打包其他团队可以使用的有用培训和数据准备步骤，或者比较同一任务中许多团队的结果。此外，工程团队可以轻松地将工作流程从研发转移到生产阶段。生产工程师可以以相同的方式部署来自不同ML库的模型，将模型作为文件存储在他们选择的管理系统中，并跟踪模型的运行来源。研究人员和开源开发人员可以使用MLflow项目格式向GitHub发布代码，使任何人都可以使用该命令轻松运行代码 。mlflow run github.com/...ML Library Developers可以输出MLflow Model格式的模型，让它们使用MLflow的内置工具自动支持部署。此外，部署工具开发人员（例如，构建服务平台的云供应商）可以自动支持各种模型。","categories":[],"tags":[]},{"title":"Mlflow介绍","slug":"yuque/Mlflow介绍","date":"2019-03-15T01:40:23.000Z","updated":"2019-04-04T13:49:28.586Z","comments":true,"path":"2019/03/15/yuque/Mlflow介绍/","link":"","permalink":"http://zhos.me/2019/03/15/yuque/Mlflow介绍/","excerpt":"","text":"介绍MLflow是一个完整的机器学习生命周期的开源平台。这意味着它具有在训练和运行期间监控模型的组件，存储模型的能力，在生产代码中加载模型以及创建管道。MLflow的主要目标是在ML之上提供额外的层，允许数据科学家与几乎任何机器学习库（h2o，keras，mleap，pytorch，sklearn和tensorflow）一起工作，同时，它将他们的工作带到另一个层次。MLflow提供三个组件： 跟踪 - 记录和查询实验：代码，数据，配置和结果。跟踪建模进度非常有用。 项目 - 在任何平台（_即_ Sagemaker）上可重复运行的包装格式。 模型 - 将模型发送到各种部署工具的通用格式。 MLflow是一个管理ML生命周期的开源平台，包括实验，可重复性和部署。 安装MLflow的一个特别之处是需要conda的支持，因此为保证后期可用，需要首先安装conda环境。 12wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.shbash Anaconda3-2018.12-Linux-x86_64.sh 安装完后，添加到环境变量： 1vim /etc/profile 添加git权限 1ssh-keygen -t rsa -b 4096 -C \"ujujzhao@gmail.com\" 安装mlflow 1./pip install mlflow 快速开始 使用Tracking API该MLflow跟踪API可以让你从你的数据的科学代码登录度量和工件（文件），看看您运行的历史。您可以通过编写如下的简单Python脚本来尝试它（此示例也包括在内quickstart/mlflow_tracking.py）： 12345678910111213141516import osfrom mlflow import log_metric, log_param, log_artifactif __name__ == \"__main__\": # Log a parameter (key-value pair) log_param(\"param1\", 5) # Log a metric; metrics can be updated throughout the run log_metric(\"foo\", 1) log_metric(\"foo\", 2) log_metric(\"foo\", 3) # Log an artifact (output file) with open(\"output.txt\", \"w\") as f: f.write(\"Hello world!\") log_artifact(\"output.txt\") 查看跟踪UI默认情况下，无论您在何处运行程序，跟踪API都会将数据写入文件到mlruns目录中。然后，您可以运行MLflow的跟踪UI：1mlflow ui 为保证外网能够访问，需添加host命令，即： 1mlflow ui -h 0.0.0.0 其他的命令功能请直接执行： 1mlflow ui --help 运行MLflow项目MLflow允许您将代码及其依赖项打包为一个可以在其他数据上以可重现的方式运行的_项目_。每个项目都包含其代码和MLproject定义其依赖项的文件（例如，Python环境），以及可以在项目中运行的命令以及它们采用的参数。您可以使用命令轻松运行现有项目，该命令从本地目录或GitHub URI运行项目：mlflow run 123mlflow run tutorial -P alpha=0.5mlflow run git@github.com:mlflow/mlflow-example.git -P alpha=5 有一个示例项目tutorial，包括一个MLproject指定其依赖项的文件。如果您尚未配置跟踪服务器，则项目会将其Tracking API数据记录在本地mlruns目录中，以便您可以使用这些运行来查看这些运行。mlflow ui 默认情况下，使用conda安装所有依赖项。要在不使用的情况下运行项目，可以提供选项 。在这种情况下，您必须确保已在Python环境中安装必要的依赖项。mlflow run`–no-conda` 教程本教程展示了如何使用MLflow端到端： 训练线性回归模型 打包以可重用且可重现的模型格式训练模型的代码 将模型部署到一个简单的HTTP服务器中，使您能够对预测进行评分 本教程使用数据集根据葡萄酒的“固定酸度”，“pH值”，“残糖”等定量特征预测葡萄酒的质量。该数据集来自UCI的机器学习库。 [1]目录 你需要什么 培训模型 比较模型 打包培训代码 服务模型 更多资源你需要什么要运行本教程，您需要： 安装MLflow（通过）pip install mlflow 安装conda 克隆（下载）MLflow存储库 git clone https://github.com/mlflow/mlflow cd进入examples你的MLflow克隆目录 - 我们将使用这个工作目录来运行教程。我们避免直接从我们的MLflow克隆中运行，因为这样做会导致教程从源代码中使用MLflow，而不是使用MLflow的PyPI安装。训练模型首先，训练一个带有两个超参数的线性回归模型：alpha和l1_ratio。 代码位于以下examples/sklearn_elasticnet_wine/train.py并在下面复制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.import osimport warningsimport sysimport pandas as pdimport numpy as npfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_scorefrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import ElasticNetimport mlflowimport mlflow.sklearndef eval_metrics(actual, pred): rmse = np.sqrt(mean_squared_error(actual, pred)) mae = mean_absolute_error(actual, pred) r2 = r2_score(actual, pred) return rmse, mae, r2if __name__ == \"__main__\": warnings.filterwarnings(\"ignore\") np.random.seed(40) # Read the wine-quality csv file (make sure you're running this from the root of MLflow!) wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"wine-quality.csv\") data = pd.read_csv(wine_path) # Split the data into training and test sets. (0.75, 0.25) split. train, test = train_test_split(data) # The predicted column is \"quality\" which is a scalar from [3, 9] train_x = train.drop([\"quality\"], axis=1) test_x = test.drop([\"quality\"], axis=1) train_y = train[[\"quality\"]] test_y = test[[\"quality\"]] alpha = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 0.5 l1_ratio = float(sys.argv[2]) if len(sys.argv) &gt; 2 else 0.5 with mlflow.start_run(): lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42) lr.fit(train_x, train_y) predicted_qualities = lr.predict(test_x) (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities) print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio)) print(\" RMSE: %s\" % rmse) print(\" MAE: %s\" % mae) print(\" R2: %s\" % r2) mlflow.log_param(\"alpha\", alpha) mlflow.log_param(\"l1_ratio\", l1_ratio) mlflow.log_metric(\"rmse\", rmse) mlflow.log_metric(\"r2\", r2) mlflow.log_metric(\"mae\", mae) mlflow.sklearn.log_model(lr, \"model\") 此示例使用熟悉的pandas，numpy和sklearn API来创建简单的机器学习模型。该MLflow跟踪的API记录有关每次训练运行信息，如超参数alpha和l1_ratio，用于训练模型和指标，如均方根误差，用来评估模型。该示例还以MLflow知道如何部署的格式序列化模型。您可以使用默认超参数运行示例，如下所示： 1python examples/sklearn_elasticnet_wine/train.PY 尝试一些其他的值alpha，并l1_ratio通过将它们作为参数传入train.py： 1python examples/sklearn_elasticnet_wine/train.py &lt;alpha&gt; &lt;l1_ratio&gt; 每次运行该示例时，MLflow都会在目录中记录有关实验运行的信息mlruns。 注意如果您想使用Jupyter笔记本版本train.py，请尝试使用教程笔记本examples/sklearn_elasticnet_wine/train.ipynb。 比较模型接下来，使用MLflow UI比较您生成的模型。在与包含mlruns运行的目录相同的当前工作目录中： 1mlflow ui -h 0.0.0.0 在此页面上，您可以看到实验运行列表，其中包含可用于比较模型的指标。 打包培训代码现在您已经拥有了培训代码，您可以对其进行打包，以便其他数据科学家可以轻松地重复使用该模型，或者您可以远程运行培训，例如在Databricks上。 您可以使用MLflow Projects约定来指定代码的依赖关系和入口点。该tutorial/MLproject文件指定项目具有位于 被调用的Conda环境文件中的依赖conda.yaml项，并且具有一个带有两个参数的入口点：alpha和l1_ratio。 123456789101112# tutorial/MLprojectname: tutorialconda_env: conda.yamlentry_points: main: parameters: alpha: float l1_ratio: &#123;type: float, default: 0.1&#125; command: \"python train.py &#123;alpha&#125; &#123;l1_ratio&#125;\" Conda文件依赖项 1234567891011# tutorial/conda.yamlname: tutorialchannels: - defaultsdependencies: - numpy=1.14.3 - pandas=0.22.0 - scikit-learn=0.19.1 - pip: - mlflow 要运行此项目，请调用mlflow run tutorial -P alpha=0.42如果存储库MLproject在根目录中有文件，您也可以直接从GitHub运行项目。本教程在您可以运行的。运行此命令后，MLflow将在具有指定依赖关系的新Conda环境中运行您的训练代码。https://github.com/mlflow/mlflow-example存储库中重复。mlflow run git@github.com:mlflow/mlflow-example.git -P alpha=0.42 服务模型现在您已使用MLproject约定打包模型并确定了最佳模型，现在是时候使用MLflow模型部署模型了。MLflow模型是用于打包机器学习模型的标准格式，可用于各种下游工具 - 例如，通过REST API实时提供服务或在Apache Spark上进行批量推断。在示例训练代码中，在训练线性回归模型之后，MLflow中的函数将模型保存为运行中的工件。 1mlflow.sklearn.log_model(lr, \"model\") 要查看此工件，您可以再次使用UI。当您单击实验运行列表中的日期时，您将看到此页面。 在底部，您可以看到调用mlflow.sklearn.log_model生成两个文件/Users/mlflow/mlflow-prototype/mlruns/0/7c1a0d5c42844dcdb8f5191146925174/artifacts/model。第一个文件MLmodel是一个元数据文件，告诉MLflow如何加载模型。第二个文件model.pkl是您训练的线性回归模型的序列化版本。在此示例中，您可以将此MLmodel格式与MLflow一起使用，以部署可以提供预测的本地REST服务器。要部署服务器，请运行： 1mlflow pyfunc serve /Users/mlflow/mlflow-prototype/mlruns/0/7c1a0d5c42844dcdb8f5191146925174/artifacts/model -p 1234 注意用于创建模型的Python版本必须与运行的版本相同。如果不是这种情况，您可能会看到错误 或。mlflow sklearn`UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0x9f in position 1: ordinal not in range(128)raise ValueError, “unsupported pickle protocol: %d”` 部署服务器后，您可以传递一些示例数据并查看预测。以下示例用于curl向splitpyfunc服务器发送带有方向的JSON序列化pandas DataFrame 。有关pyfunc模型服务器接受的输入数据格式的更多信息，请参阅 MLflow部署工具文档。 1curl -X POST -H &quot;Content-Type:application/json; format=pandas-split&quot; --data &apos;&#123;&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, &quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, &quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]&#125;&apos; http://127.0.0.1:1234/invocations 服务器应该响应输出类似于：1&#123;&quot;predictions&quot;: [6.379428821398614]&#125;","categories":[],"tags":[]},{"title":"我们从基于区域的物体探测器（更快的R-CNN，R-FCN，FPN）中学到了什么？","slug":"yuque/我们从基于区域的物体探测器（更快的R-CNN，R-FCN，FPN）中学到了什么？","date":"2019-01-31T10:12:24.000Z","updated":"2019-04-04T13:49:28.586Z","comments":true,"path":"2019/01/31/yuque/我们从基于区域的物体探测器（更快的R-CNN，R-FCN，FPN）中学到了什么？/","link":"","permalink":"http://zhos.me/2019/01/31/yuque/我们从基于区域的物体探测器（更快的R-CNN，R-FCN，FPN）中学到了什么？/","excerpt":"","text":"我们从基于区域的物体探测器（更快的R-CNN，R-FCN，FPN）中学到了什么？乔纳森惠以下2018年3月28日在本系列中，我们将全面介绍物体检测。在这里的第1部分中，我们介绍了基于区域的物体探测器，包括快速R-CNN，更快的R-CNN，R-FCN和FPN。在第2部分中，我们将研究单射击探测器。在第3部分中，我们将介绍性能和一些实现问题。通过在一个环境中研究它们，我们研究什么是有效的，什么是重要的，哪些可以改进。希望通过研究我们如何到达这里，它将为我们提供更多关于我们前进方向的见解。第1部分：我们从基于区域的物体探测器（更快的R-CNN，R-FCN，FPN）中学到了什么？第2部分：我们从单发物体探测器（SSD，YOLO），FPN和焦点损耗中学到了什么？第3部分：设计选择，经验教训和对象检测趋势？ 滑动窗口探测器自AlexNet赢得2012年ILSVRC挑战以来，使用CNN进行分类已经占据了该领域的主导地位。用于物体检测的一种蛮力方法是从左侧和右侧以及从上到下滑动窗口以使用分类来识别物体。为了在不同的观察距离检测不同的物体类型，我们使用不同尺寸和纵横比的窗户。 滑动窗（从右到左，上下）我们根据滑动窗口从图片中剪切出补丁。由于许多分类器仅采用固定大小的图像，因此修补程序会发生扭曲。然而，这不应该影响分类准确性，因为分类器被训练以处理变形图像。 将图像变形为固定大小的图像。将扭曲的图像块馈送到CNN分类器中以提取4096个特征。然后我们应用SVM分类器来识别类和边界框的另一个线性回归量。 滑动窗检测器的系统流程。下面是伪代码。我们创建了许多窗口来检测不同位置的不同对象形状。为了提高性能，一个明显的解决方案是减少_窗口_数量。窗口中的窗口 patch = get_patch（图像，窗口） 结果=检测器（补丁） 选择性搜索我们使用区域提议方法来创建用于对象检测的感兴趣区域（ROI），而不是强力方法。在选择性搜索（SS）中，我们从每个单独的像素开始作为其自己的组。接下来，我们计算每个组的纹理，并组合两个最接近的组。但是为了避免单个区域吞噬其他区域，我们更喜欢先将较小的组分组。我们继续合并区域，直到所有内容组合在一起。在下面的第一行中，我们展示了如何增长区域，第二行中的蓝色矩形显示了我们在合并期间可能实现的ROI。 （图片来源：van de Sande等，ICCV’11） R-CNNR-CNN利用区域提议方法来创建大约2000个ROI（感兴趣的区域）。区域被扭曲成固定大小的图像并单独馈送到CNN网络。然后是完全连接的层，以对对象进行分类并细化边界框。 使用区域提议，CNN，仿射层来定位对象。这是系统流程。 R-CNN的系统流程R-CNN的投资回报率低得多但质量更高，比滑动窗更快，更准确。投资回报率= ROI中投资回报率的区域提示（图像） 补丁= get_patch（图像，投资回报率） 结果=检测器（补丁） 边界框回归量区域提案方法计算密集。为了加快这一过程，我们经常选择一种较便宜的区域建议方法来创建ROI，然后使用线性回归量（使用完全连接的层）进一步细化边界框。 使用回归将原始ROI从蓝色细化为红色。 快速R-CNNR-CNN需要许多提议准确并且许多区域彼此重叠。R-CNN的训练和推理速度很慢。如果我们有2,000个提案，则每个提案都由CNN单独处理，即我们针对不同的投资回报率重复提取2000次特征。我们不是从头开始为每个图像补丁提取特征，而是使用特征提取器（CNN）首先提取整个图像的特征。我们还使用外部区域提议方法（如选择性搜索）来创建ROI，后者与相应的要素图组合以形成用于对象检测的补丁。我们使用ROI池将贴片扭曲到固定大小，并将它们馈送到完全连接的层以进行分类和定位（检测对象的位置）。通过不重复特征提取，Fast R-CNN显着缩短了处理时间。 在要素图上应用区域建议，并使用ROI池形成固定大小的修补程序。这是网络流程： 在下面的伪代码中，昂贵的特征提取正在逐渐退出for循环，这是因为它针对所有2000个ROI执行而显着提高了速度。快速R-CNN在训练中比R-CNN快10倍，在推理中快150倍。feature_maps = process（image）ROI = ROI中ROI的region_proposal（image） 补丁= roi_pooling（feature_maps，ROI） 结果= detector2（补丁）Fast R-CNN的一个主要特点是整个网络（特征提取器，分类器和边界框回归器）可以通过多任务损失（分类丢失和本地化丢失）进行端到端训练。这提高了准确性。投资回报率由于Fast R-CNN使用完全连接的层，因此我们应用ROI池来将可变大小的ROI扭曲成预定义的大小形状。让我们通过将8×8特征映射转换为预定义的2×2形状来简化讨论。 左下方：我们的功能图。 右上角：我们将ROI（蓝色）与要素图重叠。 左下图：我们将ROI分成目标维度。例如，对于我们的2×2目标，我们将ROI分成4个具有相似或相同大小的部分。 右下角：找到每个部分的最大值，结果是我们的变形特征映射。 输入要素图（左上），输出要素图（右下），蓝框是ROI（右上）。因此，我们得到一个2×2特征补丁，我们可以将其输入分类器和盒子回归器。 更快的R-CNN快速R-CNN依赖于外部区域提议方法，如选择性搜索。但是，这些算法在CPU上运行并且速度很慢。在测试中，快速R-CNN花费2.3秒进行预测，其中2秒用于生成2000个ROI。feature_maps = process（image）ROIs = region_proposal（image）＃贵！投资回报率中的ROI 补丁= roi_pooling（feature_maps，ROI） 结果= detector2（补丁）更快的R-CNN采用与快速R-CNN类似的设计，除了它用内部深度网络替换区域提议方法，而ROI则来自特征映射。新区域提案网络（RPN）更高效，并且在生成ROI时每个映像运行10毫秒。 网络流量与Fast R-CNN相同。网络流程类似，但区域提案现在由卷积网络（RPN）取代。 外部区域提议由内部深层网络取代。区域提案网络区域提议网络（RPN）将来自第一个卷积网络的输出特征映射作为输入。它在特征映射上滑动3×3过滤器，以使用像ZF网络这样的卷积网络（下面）进行类别无关区域提议。其他深度网络如VGG或ResNet可用于以速度为代价进行更全面的特征提取。ZF网络输出256个值，这些值被馈送到2个单独的完全连接的层中以预测边界框和2个对象性得分。该对象性测量框是否包含对象。我们可以使用回归量来计算单个对象度得分但是为了简单起见，更快的R-CNN使用具有2个可能类的分类器：一个用于“具有对象”类别而一个用于没有（即背景类别）。 对于要素图中的每个位置，RPN进行k猜测。因此，RPN每个位置输出4×k坐标和2×k分数。下图显示了具有3×3滤波器的8×8特征图，并且它输出总共8×8×3个ROI（对于k = 3）。右侧图表显示了单个位置提出的3个提案。 在这里，我们得到3个猜测，我们稍后会改进我们的猜测。由于我们只需要一个正确的，如果我们的初步猜测有不同的形状和大小，我们会更好。因此，更快的R-CNN不会提出随机边界框提议。相反，它预测像δx，δy这样的偏移相对于一些称为锚点的参考框的左上角。我们限制了那些偏移的值，所以我们的猜测仍然类似于锚点。 为了对每个位置进行k个预测，我们需要以每个位置为中心的k个锚点。每个预测与特定锚相关联，但不同位置共享相同的锚形状。 这些锚是经过精心预先选择的，因此它们是多样的，能够很好地覆盖不同尺度和纵横比的真实物体。这可以通过更好的猜测指导初始训练，并允许每个预测专注于某种形状。这种策略使早期训练更加稳定和轻松。更快的R-CNN使用更多的锚点。它配置了9个锚箱：3种不同的比例，3种不同的宽高比。每个位置使用9个锚点，每个位置生成2×9个对象度分数和4×9个坐标。 资源 锚在不同的论文中也称为&gt; 先验或&gt; 默认边界框。 R-CNN方法的性能如下图所示，更快的R-CNN甚至更快。 基于区域的完全卷积网络（R-FCN）假设我们只有一个特征图检测到脸部的右眼。我们可以用它来定位一张脸吗？这应该。由于右眼应位于面部图片的左上角，我们可以使用它来定位面部。 如果我们有专门检测左眼，鼻子或嘴巴的其他特征图，我们可以将结果组合在一起以更好地定位脸部。那么我们为什么要经历所有麻烦呢。在更快的R-CNN中，探测器应用多个完全连接的层来进行预测。拥有2,000个投资回报率，价格昂贵。feature_maps =处理（图像）的ROI = region_proposal（feature_maps）用于在感兴趣区ROI 补丁= roi_pooling（feature_maps，ROI） class_scores，盒=检测器（贴片）＃昂贵！ class_probabilities = softmax（class_scores）R-FCN通过减少每个ROI所需的工作量来提高速度。上面的基于区域的特征图与ROI无关，并且可以在每个ROI之外计算。剩下的工作要简单得多，因此R-FCN比快速R-CNN更快。feature_maps = process（image）ROIs = region_proposal（feature_maps） score_maps = compute_score_map（feature_maps）ROI中的ROI V = region_roi_pool（score_maps，ROI） class_scores，box = average（V）＃更简单！ class_probabilities = softmax（class_scores）让我们考虑一个5×5的特征映射M，里面有一个蓝色方形对象。我们将方形对象平均分成3×3个区域。现在，我们从M创建一个新的特征图，仅检测正方形的左上角（TL）。新功能图看起来像右下方的那个。仅激活黄色网格单元 [2,2]。 从左侧创建新的要素图以检测对象的左上角。由于我们将正方形划分为9个部分，我们可以创建9个特征映射，每个特征映射检测对象的相应区域。这些特征图称为位置敏感分数图，因为每个图检测（分数）对象的子区域。 生成9个得分图假设下面的虚线红色矩形是建议的ROI。我们将其划分为3×3个区域，并询问每个区域包含对象的相应部分的可能性。例如，左上角ROI区域包含左眼的可能性有多大。我们将结果存储在右图中的3×3投票数组中。例如，vote_array [0] [0]包含关于我们是否找到方形对象的左上角区域的分数。 将ROI应用于要素图以输出3 x 3阵列。将得分图和ROI映射到投票数组的过程称为位置敏感的 ROI池。这个过程非常接近我们之前讨论过的ROI池。我们不会进一步介绍它，但您可以参考未来的阅读部分以获取更多信息。 将ROI的一部分叠加到相应的得分图上以计算V [i] [j]计算位置敏感ROI池的所有值后，类别分数是其所有元素的平均值。 投资回报率池假设我们有C类要检测。我们将它扩展为C + 1类，因此我们为背景（非对象）添加了一个新类。每个班级都有自己的3×3分数图，因此总共有（C + 1）×3×3分数图。使用自己的一组得分图，我们预测每个班级的班级得分。然后我们在这些分数上应用softmax来计算每个类的概率。以下是数据流。对于我们的例子，我们下面有k = 3。 我们的旅程到目前为止我们从基本的滑动窗口算法开始。窗口中的窗口 patch = get_patch（图像，窗口） 结果=检测器（补丁）然后我们尝试减少窗口的数量，并在for循环之外移动尽可能多的工作。投资回报率= ROI中投资回报率的区域提示（图像） 补丁= get_patch（图像，投资回报率） 结果=检测器（补丁）在第2部分中，我们进一步完全删除了for循环。单发探测器可在单次射击中进行物体探测，无需单独的区域建议步骤。 进一步阅读FPN，R-FCN和Mask R-CNNFPN和R-FCN都比我们在此描述的更复杂。如需进一步研究，请参阅： 用于对象检测的特征金字塔网络（FPN）。 基于区域的完全卷积网络（R-FCN）。","categories":[],"tags":[]},{"title":"Kaggle＃1获胜图像分类挑战方法","slug":"yuque/Kaggle＃1获胜图像分类挑战方法","date":"2018-12-21T09:03:34.000Z","updated":"2019-04-04T13:49:28.586Z","comments":true,"path":"2018/12/21/yuque/Kaggle＃1获胜图像分类挑战方法/","link":"","permalink":"http://zhos.me/2018/12/21/yuque/Kaggle＃1获胜图像分类挑战方法/","excerpt":"","text":"这篇文章是关于我用于Kaggle竞赛的方法：植物幼苗分类。我在排名中排名第一，持续了几个月，最终在最终评估结束时以＃5结束。该方法非常通用，也可用于其他图像识别任务。 Kaggle是一个&gt; 预测建模和&gt; 分析竞赛的平台，统计人员和数据挖掘者在竞争中生成预测和描述公司和用户上传的数据集的最佳模型。这种&gt; 众包方法依赖于这样一个事实：无数的策略可以应用于任何预测建模任务，并且不可能事先知道哪种技术或分析师最有效。[1]另外，请查看在NLP上获得Intent Classification任务的最新成果的博客： 了解您的意图：SoTA结果意图分类此博客文章显示了在三个语料库上获得的最新最新结果：medium.com 任务概述你可以区分杂草和农作物幼苗吗？有效地这样做的能力可以意味着更好的作物产量和更好的环境管理。 所述奥尔胡斯大学信号处理组，在具有协作南丹麦大学，释放含有属于12种约960独特的植物的图像数据集在几个生长阶段。[1] [2]其中一个样本植物：鹅肠菜样本[3] 公开可获得在几个生长阶段属于12种物种的约960种独特植物的图像数据库。它包括带注释的RGB图像，物理分辨率约为每毫米10个像素。 为了使用数据库获得的分类结果的评估标准化，提出了基于F1分数的基准。该URL提供了数据集[13] 下图是描述数据集中所有12个类的示例： 将图像分类为各个类的任务，任务分为5个步骤： 步骤1：机器学习中的第一个也是最重要的任务是在继续任何算法之前分析数据集。这对于理解数据集的复杂性非常重要，这最终将有助于设计算法。 图像和类的分布如下：如前所述，共有12个类，总共有4750个图像。然而，如上所述，分布不均匀，并且类别分布从最大654个图像变化到最小221个图像。这清楚地表明数据不平衡，数据需要平衡才能获得最佳结果。我们将在第3步中讨论这个问题。 每班的图像分发 现在，将图像可视化以便更好地理解数据非常重要。因此，显示来自每个类的一些样本图像以便查看图像彼此之间的差异。从上面的图像中可以理解的是，所有图像看起来都非常相似。所以，我决定使用称为t-Distributed随机邻居嵌入（t-SNE）的可视化技术来看图像的分布。t分布式随机邻域嵌入（t-SNE）是一种降维的技术，特别适用于高维数据集的可视化。该技术可以通过Barnes-Hut近似实现，允许它应用于大型真实世界的数据集。[14]数据集的t-SNE可视化 仔细观察之后，我们很难看出课程的差异。因此，重要的是要了解数据是否很难仅仅为人类区分，或者机器学习模型也很难。所以，我们将为它做一个基本的基准测试。 培训和验证集在开始使用模型基准测试之前，我们需要将数据划分为训练和验证数据集。在原始测试集上测试模型之前，验证集将扮演测试数据集的角色。因此，基本上在训练数据集上训练模型并在验证集上进行测试，然后随着时间的推移可以在验证集上改进模型。一旦我们对验证集的结果感到满意，我们就可以在真实的测试数据集上应用该模型。通过这种方式，我们可以看到模型是否过度拟合或欠拟合我们的验证集，这可以帮助我们更好地拟合模型。 因此，我们通过将80％的图像保留为训练数据集并将20％的图像保持为验证集来划分4750个图像的数据集。 培训和验证数据分开 第2步：一旦我们获得了培训和验证集，我们将从数据集的基准测试开始。我们可以看到这是一个分类问题，在给出测试数据集时，我们需要将其分类为12个类中的一个。所以我们将使用卷积神经网络来完成任务。如果您是初学者并且需要更好地理解深度学习术语，请访问以下&gt; 博客： 有几种方法可以创建CNN模型，但对于第一个基准测试，我们将使用Keras深度学习库。我们还将使用Keras中可用的预训练模型，通过ImageNet数据集进行训练，我们将根据我们的任务对其进行微调。 从头开始训练卷积神经网络几乎实际上是低效的。因此，我们在ImageNet上使用预先训练的CNN模型的权重，使用1000个类，并通过保持一些层冻结并解冻其中一些并对其进行训练来对其进行微调。这是因为顶层学习简单的基本功能，我们不需要训练这些层，它可以直接应用于我们的任务。需要注意的一件重要事情是我们需要检查我们的数据集是否与ImageNet类似，以及我们的数据集有多大。这两个功能将决定我们如何执行微调。要了解更多详情，请阅读Andrej Karpathy的博客： 用于视觉识别的CS231n卷积神经网络用于斯坦福类CS231n的课程材料和注释：用于视觉识别的卷积神经网络。cs231n.github.io 在我们的例子中，数据集很小但有点类似于ImageNet。因此，我们可以直接使用ImageNet的权重，只需添加一个包含12个类的最终输出层即可查看第一个基准测试。然后我们将解除一些底层的解冻，然后训练这些层。 我们将使用Keras作为初始基准，因为Keras提供了许多预训练模型，我们将使用ResNet50和InceptionResNetV2来完成我们的任务。使用一个简单模型和一个非常高端模型对数据集进行基准测试非常重要，以了解我们是否过度拟合/不适合给定模型上的数据集。","categories":[],"tags":[]},{"title":"Capsule Networks：一种全新且极具吸引力的AI架构","slug":"yuque/Capsule Networks：一种全新且极具吸引力的AI架构","date":"2018-12-21T07:02:37.000Z","updated":"2019-04-04T13:49:28.586Z","comments":true,"path":"2018/12/21/yuque/Capsule Networks：一种全新且极具吸引力的AI架构/","link":"","permalink":"http://zhos.me/2018/12/21/yuque/Capsule Networks：一种全新且极具吸引力的AI架构/","excerpt":"","text":"链接 本文已被翻译成英文并首次发表于Deep Learning Turkey_。_卷积神经网络（CNN）在计算机视觉应用中经常是首选，因为它们在对象识别和分类任务上取得了成功。CNN由堆叠在一起的许多神经元组成。计算跨神经元的卷积需要大量计算，因此池化过程通常用于减小网络层的大小。卷积方法可以通过简单的计算学习数据的许多复杂特征。通过对我们的输入执行许多矩阵乘法和求和，我们可以得出我们问题的答案。 我总是听到CNN有多棒。什么时候失败？ CNN在解决对象识别和分类问题方面取得了巨大成功。但是，它们并不完美。如果CNN显示的方向对象不熟悉，或者对象出现在不习惯的位置，则预测任务可能会失败。 例如，如果您将一张脸倒置，网络将无法再识别眼睛，鼻子，嘴巴以及两者之间的空间关系。同样，如果你改变了脸部的特定区域（即切换眼睛和鼻子的位置），网络将能够识别脸部，但它不再是真实的脸部。CNN学习图像中的统计模式，但是他们没有学习关于什么使事物看起来像脸的基本概念。关于为什么CNN无法学习概念的理论，_AI_之_父_Geoffrey Hinton专注于用于缩小网络规模和计算要求的池化操作。他感叹道： “在卷积神经网络中使用的池化操作是一个很大的错误，它运作良好的事实是一场灾难！”汇集层正在破坏信息，使网络无法学习更高级别的概念。所以他开始着手开发一种新的架构，这种架构并没有过多地依赖于这种操作。 结果：胶囊网络 什么是胶囊网络？Hinton和Sabour借鉴了神经科学的观点，认为大脑被组织成称为胶囊的模块。这些胶囊特别擅长处理物体的特征，如姿势（位置，大小，方向），变形，速度，反照率，色调，纹理等。 理论上，大脑必须有一种机制，用于_将_低级视觉信息_路由_到它认为最适合处理它的胶囊。已经提出了胶囊网络和动态路由算法作为卷积神经网络模型不充分的问题的解决方案。 Portrait de femme au col d`hermine（奥尔加） 胶囊表示图像中存在的特定实体的各种特征。一个非常特殊的特征是图像中存在实例化的实体。实例化的实体是一个参数，如位置，大小，方向，变形，速度，反照率，色调，纹理等。表示其存在的一种显而易见的方法是使用一个单独的逻辑单元，其输出是实体存在的概率[ 1 ]。为了获得比CNN更好的结果，我们应该使用迭代路由协议机制。这些功能称为实例化参数。在经典CNN模型中，不能获得图像中对象的这种属性。平均/最大池化层减小了一组信息的大小，同时减小了大小。 好吧，某处有一个嘴唇，鼻子和眼睛，但卷积神经网络无法决定它应该在哪里以及它在哪里。 &gt; 对于传统的网络，错位的功能不会让它失望！ 壁球功能 壁球功能在深度神经网络中，激活函数是应用于层输出的简单数学运算。它们用于近似存在于数据中的非线性关系。激活层通常作用于标量值 - 例如，对向量中的每个元素进行标准化，使其落在0和1之间。在Capsule Networks中，一种称为squash函数的特殊类型的激活函数用于归一化向量的大小，而不是标量元素本身。 协议路由算法这些壁球功能的输出告诉我们如何通过训练学习不同概念的各种胶囊来路由数据。图像中每个对象的属性在路由它们的向量中表示。例如，脸部的激活可以将图像的不同部分路由到理解眼睛，鼻子，嘴巴和耳朵的胶囊。 胶囊网络在MNIST数据集中的应用由胶囊网络执行的角度估计 现在，下一步至关重要： 就像不同层次的深层CNN中的层学习图像的不同语义属性（内容，纹理，样式等）一样，胶囊也可以组织成不同的层次。在一个级别的胶囊进行预测，了解物体的形状，并将它们传递给更高级别的胶囊，这些胶囊可以了解方向。当多个预测一致时，更高级别的预测变得活跃。此过程被描述为动态路由，我现在将更详细地讨论。 那么，让我们创建一个逐步的胶囊架构来分类MNIST数据集： 第一层有一个经典的卷积层。在第二层中，在称为主要胶囊的层中执行卷积处理，其中应用该squash函数。每个主要胶囊接收图像的一个小区域作为输入（称为其感受野），并且它试图检测特定图案的存在和姿势 - 例如，圆圈。 更高层中的胶囊（称为路由胶囊）检测更大和更复杂的对象，例如数字8，由两个圆圈组成。然后他们使用一种新颖的挤压功能来保证这些矢量的长度在0到1之间。 在主胶囊层之前施加标准卷积层，并获得9×9×256的输出。在主胶囊层中应用具有32个通道的新卷积过程，步幅为2.然而，将其与其他卷积过程分开的这个特征是压缩的功能。最后，这给出了主要胶囊的输出。 初级胶囊的卷积过程 这提供了6x6输出。然而，在胶囊层中，实现动态路由算法，使得这些8长度输出DigitCaps向量的32个输出由于具有第三层动态路由的胶囊层而获得（逐协议路由算法） 。逐协议算法包括协议（检测和路由）更新的几次迭代。 Capulel层12345678910111213141516171819202122232425def CapsNet（input_shape，n_class，num_routing）： “”” 胶囊网络的MNIST数据集。 ：“input_shape”参数：vdata形状，3d，[w，h，c] ：“n_class”参数：类的数量 ：“num_routing”参数：动态路由迭代次数 ：功能输出：两个Keras模型，第一个用于训练，第二个用于evalaution。 `eval_model`用于同时进行训练。 “”” X = layers.Input（形状= input_shape） ＃ 1层：卷积层（Conv2D） 器CONV1 = layers.Conv2D（过滤器= 256，kernel_size = 9，步幅= 1，填充= '有效'，活化= ' RELU '，名字= ' CONV1 '）（x）的 ＃ 2层：Conv2D壁球活化，[无，num_capsule，dim_capsule]来回整形。 primarycaps = PrimaryCap（CONV1，dim_capsule = 8，n_channels = 32，kernel_size = 9，步幅= 2，填充= '有效'） ＃ 3层：胶囊层。运行：动态路由算法。 digitcaps = CapsuleLayer（num_capsule = n_class，dim_capsule = 16，num_routing = num_routing， name = ' digitcaps '）（primarycaps） ＃ 4层： ＃如果你使用Tensorflow，你可以跳过这个会话:) out_caps =长度（名称= ' capsnet '）（digitcaps） 动态路由在capsulelayers.py类CapsuleLayer (layers.Layer)函数中定义。由于该计算步骤，在图像中不存在对象的区域中矢量值较小，而检测区域中的矢量尺寸根据属性而变化。1234567891011121314151617181920212223242526272829303132类 CapsuleLayer（层。层）： “”” 胶囊层。它类似于密集层。密集层有`in_num`输入，每个都是标量，输出 来自前一层的神经元，它有'out_num`输出神经元。CapsuleLayer只是扩展了神经元的输出 从标量到矢量。所以它的输入形状= [None，input_num_capsule，input_dim_capsule]和输出形状= \\ [None，num_capsule，dim_capsule]。对于密集层，input_dim_capsule = dim_capsule = 1。 ：param num_capsule：此图层中的胶囊数 ：param dim_capsule：此层中胶囊的输出向量的维度 ：param routings：路由算法的迭代次数 “”” DEF __init__（自，num_capsule，dim_capsule，路线= 3， kernel_initializer = ' glorot_uniform '， ** kwargs）： 超级（CapsuleLayer，自我）。__init __（** kwargs） self .nu​​m_capsule = num_capsule self .dim_capsule = dim_capsule 自我 .routings =路线 self .kernel_initializer = initializers.get（kernel_initializer） def build（self，input_shape）： assert len（input_shape）&gt; = 3，“输入Tensor应该有shape = [None，input_num_capsule，input_dim_capsule] ” self .input_num_capsule = input_shape [ 1 ] self .input_dim_capsule = input_shape [ 2 ] ＃变换矩阵 self .W = self .add_weight（shape = [ self .nu​​m_capsule，self .input_num_capsule， self .dim_capsule，self .input_dim_capsule]， initializer = self .kernel_initializer， name = ' W '） self .built = 真的 你也可以在这里找到所有的工作。 测试性能🏅当使用10,000图像测试数据集进行测试时，我们获得了MNIST数据集的99.61％准确度，并且获得了FASHION MNIST数据集的92.22％准确度。是啊！😎对于具有80％重叠手写数字的MultiMNIST数据集，当数据重叠时，胶囊网络的性能似乎非常好，特别是与CNN模型相比时。MultiMNIST数据集的胶囊网络输出 MNIST的50纪元工作时间⏳与CNN相比，由于其计算复杂性，胶囊网络的训练时间较慢。以下是各种硬件和云服务器上的50纪元培训时间：当然 ：）要使用Google Colab支持，最吸引人的选项，请阅读&gt; Google Colab免费GPU教程！ ✔️Pros和❌Consof Capsule Networks与其他最先进的技术相比，✔️Capsule网络在MNIST数据集中取得了最大的成功。 ✔️使用较小的数据集成功。（通过强制模型学习胶囊中的特征变体，它可以用更少的训练数据更有效地推断可能的变体。） ✔️逐协议算法允许我们区分重叠图像中的对象。 ✔️使用激活矢量更容易解释图像。 ✔️Capsule网络维护对象的等效性，色调，姿势，反照率，纹理，变形，速度和位置等信息。 与现有技术模型相比，❌CIFAR10在数据集方面没有成功。 ❌尚未在非常大的数据集上进行测试。 ❌由于协议路由算法，训练模型需要更多时间。 具有不同路由算法的胶囊网络模型的应用表明它是一个需要更多实验并且仍在开发的主题。 “卷积神经网络注定失败” Geoffrey Hinton 另一个例子卷积神经网络的重要安全问题举例 在上面示出的示例中，当仅改变Kim Kardashian的图像的方向时，预测准确度显着下降。在右边的图像中，我们可以很容易地判断出一只眼睛和她的嘴是不正确放置的，这不是一个人的假设。然而，我们看到了0.90的预测分数。 受过良好训练的CNN在这种方法上存在一些障碍。除了容易被具有不正确位置的特征的图像欺骗之外，当以不同方向观看图像时CNN也容易混淆。 毫无疑问，CNN可能受到对抗性攻击的影响。这是一个可能导致安全问题的重要约束，特别是当我们将潜在模式嵌入到对象中以使其看起来像其他东西时。但是，正如我们所知，我们可以通过Capsule Networks解决这个问题！ ✨胶囊网络的模型是值得和有前途的！ 📝 我们关于胶囊网络的学术论文：使用胶囊网络识别手语听力和语言障碍者继续通过唇读或手和脸的运动（即手语）进行交流。胶囊网络可以帮助确保残疾人不仅可以充分参与生活，还可以通过与他人的健康和有效沟通来提高生活质量。 在这项工作中; Capsule Networks 以94.2％的验证准确度识别手语的数字。 手语数字数据集，Arda Mavi ve Zeynep Dikle 参考[1] Sabour，S.，Frosst，N. ve Hinton，GE，“ 胶囊之间的动态路由 ”，arXiv preprint arXiv：1710.09829,2017。[2] Hinton，GE，Krizhevsky A. ve Wang，SD“ 转换自动编码器。”国际人工神经网络会议。斯普林格，柏林，海德堡，2011年。[3] CSC2535：2013高级机器学习认真对待逆向图形，Geoffrey Hinton多伦多大学计算机科学系，2013年。[4] MNIST数据集的胶囊网络实现（土耳其语解释，深度学习Türkiye，kapsul-agi-capsule-network。[5] 使用胶囊网络识别手语，FuatBeşer，MerveAyyüceKIZRAK，BülentBOLAT，TülayYILDIRIM，https：//github.com/ayyucekizrak/Kapsul-Aglari-ile-Isaret-Dili-Tanima","categories":[],"tags":[]},{"title":"F-35是1.4万亿美元的国家灾难","slug":"yuque/F-35是1.4万亿美元的国家灾难","date":"2018-12-21T05:34:55.000Z","updated":"2019-04-04T13:49:28.590Z","comments":true,"path":"2018/12/21/yuque/F-35是1.4万亿美元的国家灾难/","link":"","permalink":"http://zhos.me/2018/12/21/yuque/F-35是1.4万亿美元的国家灾难/","excerpt":"","text":"链接 JSF是一个可怕的战斗机，轰炸机和攻击者 - 并且不适合航空母舰。F-35在战斗准备就绪之前还有很长的路要走。 这是现任退休运营测试与评估总监迈克尔吉尔摩在上一份年度报告中的离职信息。联合攻击战斗机计划已经消耗了超过1000亿美元和近25年。 要完成基本开发阶段，至少需要10亿美元和两年多。 吉尔莫尔告诉国会，五角大楼和公众，即使有大量的时间和金钱投入，“所有变种的运作适用性仍然低于服务部门的预期。”Gilmore详细介绍了该计划存在的一系列遗留问题，有时甚至是恶化问题，包括数百个关键性能缺陷和维护问题。 他还提出了一个严肃的问题，即空军的F-35A能否在空对空或空对地任务中取得成功，海军陆战队的F-35B是否可以进行基本的近距离空中支援，以及海军是否能够 F-35C适用于航空母舰。事实上，他发现“如果在战斗中使用，F-35飞机将需要支持来定位和避开现代威胁地面雷达，获取目标，并且由于未解决的性能缺陷和有限的武器运输可用性而使敌方战斗机编队“。在一份公开声明中，F-35联合计划办公室试图驳回吉尔摩的报告，声称“所有这些问题都是日本知识产权组织，美国服务机构，我们的国际合作伙伴和我们的行业所熟知的。”JPO对众多问题的承认是可以接受的，但没有迹象表明该办公室有任何计划 - 包括成本和进度重新估算 - 以解决目前已知问题而不偷工减料。显然，他们还没有计划应对并为将来四年即将进行的更加严格，发展和运营测试中发现的无数未知问题提供补救。这样的计划是必不可少的，应该由实际解决问题的速度而不是不切实际的现有时间表来推动。如何解决Gilmore发现的众多问题，以及我们如何才能最好地推进历史上最昂贵的武器计划，这个计划一直无法达到自己非常适度的承诺？ 电子化用于证明成本合理 - 而不是提供能力F-35正在向美国人民出售，其中很大一部分就是其任务系统，喷气式飞机上的大量精密电子设备。仔细阅读有关F-35的任何关于W-35的正式文章将会发现它们几乎总是指出它能够收集大量信息。这些信息应该通过其板载传感器和数据链接到外部网络源，然后由F-35的计算机系统合并，以便为飞行员识别和显示特定威胁，目标和伴随力图片 - 即“情境”意识。”这个过程旨在让飞行员主宰战场。然而，基于这些系统在开发测试期间的实际测试性能，电子设备实际上会干扰飞行员的生存和普及能力。总的来说，F-35的传感器，计算机和软件的问题，包括制造虚假目标和报告不准确的位置，已经非常严重，以至于爱德华兹空军基地的测试团队将他们评为“红色”，这意味着他们无法进行战斗。他们期待的任务。一个系统，即光电目标系统（EOTS），被飞行员挑选出来，其分辨率和范围都低于目前在传统飞机上使用的系统。 EOTS是旨在帮助F-35从足够远的地方探测和摧毁敌方战斗机以使斗狗成为过去的系统之一。它安装在靠近飞机机头的地方，包括一台电视摄像机，一个红外搜索和跟踪系统，以及一个激光测距仪和指示器。这些传感器在计算机控制下旋转，以在广泛的视野范围内跟踪目标，并在飞行员的头盔遮阳板显示器上显示图像。但是EOTS的局限性，包括图像随着湿度的降低，迫使飞行员飞行的距离比使用早期系统时更接近目标只是为了获得足够清晰的图像来发射导弹或射击。该报告说，问题非常严重，以至于F-35飞行员可能需要飞得如此接近才能获得他们必须机动的目标才能获得导弹射击所需的距离。因此，该系统的局限性可以迫使攻击性的F-35妥协意外，让敌人机动到第一次机会。投降惊喜的元素并让对手先射击是我们想要迫使敌人做的事情，而不是我们自己。另一个经常被吹捧的功能是分布式孔径系统（DAS），该功能应该赋予F-35卓越的态势感知能力。 DAS是将显示器供给臭名昭着的600,000美元头盔系统的主要传感器之一，它也未能实现炒作。DAS传感器是分布在F-35机身周围的六个摄像机或“眼睛”，它们向头盔遮阳板投射到飞行员想要观察的任何方向的外部视图，包括向下或向后。同时，头盔遮阳板显示飞行仪表以及从传感器和任务系统得出的目标和威胁符号。但由于过多的错误目标，不稳定的“抖动”图像​​和信息过载等问题，飞行员正在关闭一些传感器和计算机输入，而是依靠简化的显示器或更传统的仪表板。在这里，系统再次比它应该取代的系统好一点。在一些重要的武器交付准确性测试中，试飞员也对头盔有困难。一些飞行员将头盔中的显示描述为“操作上无法使用且可能不安全”，因为“符号杂乱”使地面目标模糊不清。在试图对目标射击短程AIM-9X空对空导弹时，飞行员报告说，他们对目标的看法被头盔护目镜上显示的符号所阻挡。飞行员还报告说，这些符号在试图追踪目标时不稳定。然后是由于“虚假轨道”导致飞行员实际上看到双重的问题。将各种车载仪器产生的所有信息并将其合并为飞行员的连贯图像存在问题，该过程称为传感器融合。飞行员报告说，不同的仪器，如飞机的雷达和EOTS，正在检测相同的目标，但编制信息的计算机正在将单个目标显示为两个。飞行员试图通过关闭一些传感器来解决这个问题，使多余的目标消失。 DOT＆E表示，这是“不可接受的战斗，违反了将多个传感器的贡献融合到一个准确的轨道和清晰的显示中以获得态势感知以及识别和接触敌方目标的基本原则。”虽然问题出在一个平面上，但是当几架飞机试图通过网络共享数据时，情况会更糟。 F-35具有多功能高级数据链路（MADL），旨在使飞机能够与其他F-35共享信息，以便为所有飞行员提供战斗空间的共同图像。它通过获取每个平面生成的所有数据并将其组合到一个共享的世界视图中来实现这一点。 但是，这个系统也会产生错误或分裂的目标图像。 使问题更加复杂的是，系统有时也会完全丢弃目标图像，导致驾驶舱内部存在混淆。所有这些意味着系统意味着让飞行员更好地了解周围的世界可以完全相反。 根据该报告，这些系统“继续降低战斗空间意识并增加飞行员的工作量。 这些缺陷的解决方法对于飞行员来说是耗时的，并且会减少高效和有效的任务执行。“F-35助推器表示这是重要的网络 - 实际上重要的是网络无法正常工作。 作为战士无效F-35从一开始就打算成为一架多用途飞机。这份最新的报告清楚地描述了迄今为止它在各种角色中如何叠加，包括与它应该取代的每架飞机相比。这个消息并不令人鼓舞。F-35作为空对空战斗机的缺点已经有了很好的记录。它在视觉范围（WVR）中的模拟空战中失去了名称，它的雷达隐身没有任何优势，在2015年初的F-16中，其中一架F-35应该取代作为空中战斗机。 F-35在空对空机动中反复丢失，尽管该测试被操纵，因为所使用的F-16是较重的双座版本并且进一步装载了重型拖曳外部燃料坦克阻碍其机动性。F-35助推器认为飞机的低雷达标志将使其远离WVR情况，但空战的历史是无法完全避免WVR的攻击。导弹故障，雷达干扰的影响以及其他难以预测的因素往往会一次又一次地迫使WVR参与。这份最新报告证实F-35并不像传统战斗机那样机动。所有三种变型“在跨音速下都表现出令人反感或不可接受的飞行质量，其中飞机上的空气动力正在迅速变化。”一个这样的问题被称为机翼下降，其中喷气机的翼尖在急转弯时突然下降，这可能导致飞机旋转并可能发生碰撞。在声屏障正下方的跨音速是战斗机飞行包线的最关键点。这些是历史上大多数空战发生的速度。正是在这些速度下，F-35需要最灵活才能成为有效的战斗机。 该计划试图通过改变F-35的飞行软件而不是通过重新设计导致问题的实际飞行表面来解决机动性能问题。该软件称为控制法则，将飞行员的操纵杆命令转换为飞机的行为。人们可以预期飞行员对飞机的某些力量会导致飞机的等效响应。由于软件的变化，有时并非如此。 例如，如果飞行员使用尖锐的杆移动以转动飞机，控制法软件现在可以更温和地转向以防止诸如 - 包括 - 挖掘等问题。 F-35辩护人试图通过声称F-35从未打算用于近距离空中斗狗来解雇这些问题，空军强烈要求飞机配备短距离空对空炮。 作为空对空战斗机，F-35的作​​战能力非常有限，因为目前软件版本只能使用两枚导弹，而且它们必须是雷达引导的先进中程空中飞行器。空中导弹（AMRAAMs）;如果它想要保持其隐形特征，它将来不会超过四个。 F-35作为空对空战斗机的能力目前进一步受到限制，因为AMRAAM并未针对近距​​离视距作战进行优化。最终，升级的软件版本将允许飞机携带除AMRAAM之外的导弹，但不会很快。这意味着F-35进入的任何战斗最好都是短暂的，因为它很快就会耗尽弹药。它的枪也可用于近距离战斗，但它目前还没有工作，因为在战斗中有效使用它的软件还没有完成。 F-35A中的大炮位于飞机侧面的一扇小门后面，在大炮发射前瞬间快速打开 - 这一特性旨在使飞机保持隐身状态。测试飞行表明，这扇门能够捕捉到飞过飞机表面的空气，将F-35的机头从瞄准点拉开，导致“超出精度规格”的误差。 工程师们正在对F-35的控制法进行更多修改，以纠正门引起的误差。进行这些更改并执行随后的“回归”重新测试以确认更改的有效性会延迟实际的枪支准确度测试。在这些测试发生之前，没有人能够知道F-35A的大炮是否能够真正击中目标。 F-35B和F-35C都将使用外置式枪架，而不是像空军型号那样的内部版本。由于两种型号机身形状的差异，海军陆战队和海军将使用不同型号的枪荚。两者都在地面上进行了试验，但飞行测试看看吊舱对喷气式飞机空气动力学的影响才刚刚开始。 DOT＆E警告说，就像F-35A上的枪门一样，可能会发现意外的飞行控制问题。必须设计对这些的修复，然后进行测试。只有这样，程序才能开始更全面的飞行中精度测试，这对于确定枪荚是否准确是必要的。 开发测试延迟以及解决测试可能发现的问题的过程非常严重，以至于该计划可能没有有效的初始操作测试和评估枪支。这不仅可以进一步延迟预定的测试，而且更重要的是，可以防止飞机很快到达战士。 作为拦截轰炸机无效F-35将具有极其有限的拦截有用性的几个主要原因 - 空军和海军陆战队的“初始作战能力”声明尽管如此。例如，欧洲，俄罗斯，中国甚至伊朗的国防公司多年来一直在努力开发和生产打败隐形飞机的系统。他们取得了一些成功。我们在1999年清楚地看到了这一点，当时一支塞尔维亚导弹部队击落了一架F-117隐形战斗机，该战斗机配备了过时的苏联时代SA-3地对空导弹，这是1961年首次部署的系统。塞尔维亚防空人员发现他们可以通过使用导弹电池的长波搜索雷达探测隐形飞机。然后，利用观察员和导弹自己的制导雷达，塞尔维亚部队能够追踪，瞄准并杀死一架隐身的F-117。为了表明这不是侥幸，塞尔维亚地空导弹击中并损坏了另一架F-117，以至于它再也没有在科索沃空战中飞行过。这些搜索雷达不受现代隐形飞机的特殊形状和涂层的影响，可以轻松检测到今天的隐形飞机，包括F-35。自第二次世界大战以来，俄罗斯人从未停止过制造这种雷达，并且现在在公开市场上以低至1000万美元的价格销售现代化，高度移动的卡车式数字长波雷达。中国和伊朗人也开始采用类似的雷达系统。比长波探测雷达更难对付的更简单的系统是无源探测系统（PDS），用于探测和跟踪飞机发射的射频（RF）信号 - 雷达信号，UHF和VHF无线电信号，识别-friend-or-foe（IFF）信号，Link-16等数据链路信号和TACAN等导航转发器信号。现代PDS的一个很好的例子是VERA-NG，这是一种捷克系统，在国际上销售，使用三个或更多的间隔良好的接收天线来检测和跟踪和识别战斗机和轰炸机发出的射频信号。系统的中央分析模块计算到达接收器的信号的时间差，以识别，定位和跟踪多达200架飞机发射雷达信号。VERA-NG只是世界上使用的众多PDS中的一种 - 俄罗斯人，中国人和其他人也生产PDS，这些PDS已经广泛使用了好几年。从对手采用PDS的角度来看，PDS的优点在于雷达隐身与其探测和跟踪飞机的能力无关。如果飞机必须使用其雷达，无线电，数据链路或导航系统来完成其任务，PDS很有可能通过这些发射来检测，跟踪和识别它。世界上每架飞机都容易受到PDS，隐身和非隐身的影响，而F-35也不例外。F-35的主要空对空武器AIM-120是超视距雷达导弹 - 因此，F-35必须使用大型雷达发射高功率信号才能探测到空中目标然后引导导弹到他们身边。同样，飞机必须采用高功率地面测绘雷达信号来远距离寻找地面目标。此外，如果飞机的系统必须与地层中的其他飞机通信或与AWACS等非机载支持飞机通信，则必须使用其无线电和数据链路。因此，F-35可能易受被动跟踪系统的检测。这些无源探测系统中的一些比搜索雷达便宜得多 - 而且它们在电子方面几乎检测不到。DOT＆E报告还列出了限制拦截有用性的几个主要原因。其中一个原因是F-35的Block 2B（USMC）和Block 3i（USAF）软件阻止它检测到许多威胁和目标，同时严重限制它可携带的武器种类。例如，F-35目前只能携带几种型号的大型制导直接攻击炸弹。这些都不能像电力导弹那样从远处发射。相反，它们落在从飞机到目标的弹道轨道上，这意味着它们只能在目标视野中以相对较短的距离释放。目前，F-35飞行员“将被迫飞得更近，以接近地面目标，并且根据敌方防空系统的威胁程度和可接受的任务风险，它可能仅限于接触仅由短程防御的地面目标防空，或根本没有防空。“F-35可携带的少量武器类型也限制了其在战斗中的灵活性。目前的软件一次只能支持一种炸弹，DOT＆E表示只有在攻击一个或两个类似目标时才有用。因此，例如，当一架F-35飞机装载的炸弹装载用于摧毁地面目标的炸弹时，它们将无法摧毁任何硬化或沙坑目标，因为它们不会需要更重的炸弹。预计F-35将携带更多种类的武器，因为更多的软件，炸弹架和测试验证这些武器已经开发出来 - 但我们直到2021年才知道哪些武器实际上是适合作战的。此外，为了携带除两个大型制导炸弹之外的东西，它将不得不使用外部武器和机架，大大降低了飞机本已令人失望的射程和机动性 - 当然，或多或少地消除了隐身。能够穿透严密防御的空域以摧毁敌方领土深处的固定目标，这是F-35经常被引用的理由。当然，F-35的有限射程 - 低于传统的F-16战斗机 - 意味着它不可能在俄罗斯和中国等大国的家乡内完成空军所谓的“深度打击”。 2016年的DOT＆E报告描述了一些官方的拖延行动，推迟了F-35的穿透力测试。例如，该计划现在才开始接收模拟敌方雷达系统的关键地面雷达模拟器设备，这些设备是在高度竞争的近邻情景中对F-35的有效性进行有力测试所需的。它只接收该设备，因为它是由DOT＆E寻求和采购的，当时很明显服务和JSF计划办公室不会寻求足以复制F-35预计能够复制近端威胁的测试基础设施反击。这种设备的交付工作已经开始，但要到2018年初才能完成。初级专业人员没有计划或预算进行发展性飞行试验。军方在内华达州内利斯空军基地的西部测试区进行隐形飞机的开发和操作测试。测试是针对地面雷达模拟器设备和地对空导弹发射器进行的。正在测试的飞机飞过这些阵列以查看飞机的机载传感器 - 特别是其电子战系统和地面测绘雷达 - 与通过数据链路提供的机外情报相结合，可以检测到威胁并做出适当的响应，例如通过警告飞行员，干扰信号或发射防御抑制导弹。问题是一个复杂的问题，因为雷达信号显示SAM的存在，例如，从而允许飞机瞄准SAM或避免它，不一定是独特的，并且通常非常类似雷达的信号，不立即对飞机的威胁。F-35无法携带足够的武器轰炸一切。它的传感器和传感器融合系统必须能够区分构成真正威胁的敌方SAM雷达与可能在探测范围内的许多无害雷达之间的区别 - 通用空中监视雷达，短程，低空防空针对武器而非飞机的雷达，甚至是附近的民用空中交通管制和气象雷达系统。同样瘫痪，直到地面雷达模拟器设备到位，F-35程序将无法正确开发，验证和更新F-35的任务关键型机载软件文件，称为任务数据负载（MDL）。 MDL是指定所有目标和威胁位置的巨大文件，以及它们各自的电子和/或红外签名以及所有相关的映射数据。 如果没有准确，最新的MDL，F-35就无法找到目标或逃避和抵御威胁 - 它也无法实现据称是其主要优势的网络和传感器融合功能。如果没有MDL，F-35就无法开战。 MDL还需要不断更新有关每个F-35任务收集的威胁，目标和信号等信息。 F-35飞行员只有在配备必要的地面雷达模拟器设备的测试范围内进行测试后，才能确保他们所需的MDL能够正常工作。必须通过中央重编程实验室使用相关作战命令的海量数据输入为每个战区或冲突区创建新的和完整的MDL。例如，在英格兰境外运营的F-35将拥有与日本F-35不同的档案。今天只存在一个这样的重编程实验室，并且由于JPO管理不善，它最近才被安排接受必要的升级以产生经过验证的MDL。实验室需要15个月才能生成完整的MDL。如果在一个新的，未曾预料到的战区中突然需要F-35战斗机，那些F-35将无法至少执行15个月的战斗任务。由于重编程实验室尚未建立全套必要的地面雷达模拟器设备，DOT＆E表示，最早的重编程实验室将能够为IOT和E生产经过验证的MDL，将于2018年6月完成。这是在2017年8月计划的IOT＆E开始后近一年 - 也就是海军陆战队宣布F-35B最初具备运营能力两年后。 DOT＆E进一步表示，F-35 MDL适合战斗“将不会进行测试和优化，以确保F-35能够在2020年前检测，定位和识别现代部署的威胁。” 作为近距离空中支援平台无效F-35有很多不足之处，在远离战场的情况下执行空对地拦截任务，但在其他预定的空对地作用中更为严重，直接支持部队，近距离空中支援（CAS）。 DOT＆E得出结论认为，F-35目前的配置“还没有证明CAS能力与第四代飞机的能力相当。”鉴于空军部长最近的声明，该服务打算重新努力，这一说法特别令人不安。在2021年取消CAS战斗证明的A-10。 CAS是另一项主要任务，缺乏有效的大炮将极大地限制F-35的战斗实用性。对于许多CAS任务来说，有效的大炮是必不可少的，在这些任务中，任何大小的炸弹，无论是导弹还是非制导炸弹都会对地面上的友军造成危险，或者担心附带损害，例如在城市环境中。 当我们的部队被距离只有几米远的敌人伏击或超支时，大炮更加重要，在“危险关闭”的情况下，只有最准确的火力才能帮助我们的方面杀死或驱散敌人。 在最近的兰德研究中接受采访的地面指挥官表示，他们更倾向于使用A-10的炮火甚至是导弹弹药，因为80％的炮弹在瞄准点的20英尺半径范围内发射，提供了精确的精确度。危险关闭情况绝对需要。大炮对于击中移动目标也是最有用的，因为大炮爆发可以在预期移动时引导目标。 目前舰队中的三架F-35型号都不能在战斗中使用大炮。 事实上，他们都没有接近完成他们的发展飞行测试 - 更不用说他们的操作适用性测试 - 对于机身安全性，准确性和目标杀伤力。更糟糕的是，根据初步的测试经验，似乎所有三种F-35版本的头盔式瞄准具的严重不准确性使得大炮在空对空作战中无效 - 这也会使CAS无效 - 而且头盔的准确性问题可能在技术上是固有的，也是无法治愈的。 请注意，CAS的炮弹精度要求比空战要严格得多：在友军部队附近射击时，即使是轻微的精确度问题也会产生悲剧性后果。如前所述，海军陆战队的F-35B和海军的F-35C的炮舱可能会增加另一个不准确的来源 - 也可能是无法治愈的 - 并且仍未经CAS测试。 F-35大炮对CAS的战斗适用性直到3F区块IOT＆E结束时才会知道，这在2021年之前是不可能的。未能完全实现这些CAS测试 - 由于JPO管理不善和测试资源延迟，这种可能性很大 - 肯定会危害美军的生命。 除了关键的大炮不准确性问题之外，飞行员头盔显示器中符号杂乱的引起误差的混乱在CAS角色中尤其危险。 DOT＆E表示，由于符号杂乱模糊目标，难以读取关键信息和pipper [aimpoint]稳定性，目前的系统“在操作上无法使用，并且可能无法完成计划的测试。” 即使头盔显示的符号没有遮挡飞行员看到目标的能力，F-35的顶篷也可能。喷气式飞机的顶篷是一种厚的丙烯酸材料，具有低可观察的涂层，以保护隐形。这使得顶篷不太透明，并且根据DOT＆E似乎扭曲了飞行员的视野。 在F-35的每个版本中进一步限制加农炮的有效性是它携带的25毫米炮弹的数量–F-35A为182，B和C为220.这对于CAS来说是非常不足的，特别是与由A-10运载的超过1,100个30毫米炮弹。虽然A-10有足够的炮弹用于10到20次攻击，但F-35的任何变种只有两次，也许四次传球。 更有效地使用任何CAS武器，大炮或其他装置，更有限制的是，F-35无法飞得低而且速度慢，无法找到典型的难以看见的CAS目标并安全地将其识别为敌人或友军，即使是在提示 由地面或空中观察员。 由于其小而重载的机翼，F-35无法在寻找隐藏和伪装目标所需的低速下充分操纵 - 并且完全没有装甲和高度易燃，它将遭受来自小型步枪和轻型机枪的灾难性损失在低海拔和所需的低速度下不可避免地命中。与此形成鲜明对比的是，A-10专门设计用于出色的低速和低速机动性，并且在设计上具有前所未有的生存能力，可以抵御那些枪支，甚至可以抵抗肩射式导弹。 空军官员经常认为，缺乏有效的枪支或无法操纵低速和慢速在未来的战争中无关紧要，因为空军打算以不同的方式进行CAS，即在高海拔地区使用较小的精确弹药。但F-35将不会被清除至少携带这些武器五年。 与此同时，F-35现在只能携带两枚制导炸弹，而这些炸弹则为500磅或更大。这些模型都不适用于友军部队附近。根据军方的风险估算表，在250米（820英尺）处，500磅重的炸弹有10％的几率使友军失去能力。这意味着在那个泡沫中，敌人可以在没有近距离空中支援的情况下进行机动。250磅重的小直径炸弹II现在处于低速生产状态，并在F-15E上使用;然而，即便如此，在“危险关闭”的交火中，在友军部队附近使用它太大了，在F-35上使用它所需的软件和炸弹机架将无法在2021年之前完成战斗。近距离空中支援不仅仅是对目标投掷炸弹的飞机。为了真正有效，CAS任务需要飞行员和在地面作战的部队之间进行详细的战术协调。几十年来，这已经通过无线电通信有效地完成，并且近年来，运营中的飞机已经通过称为可变消息格式和链路-16的网络系统上的语音和数据的数字通信链路进行了升级。在飞行测试中，F-35的数字数据链路遇到了很大的困难，包括丢失的信息或以错误格式传输的信息。这迫使飞行员和地面控制器通过无线电通过语音重复信息来在系统周围工作。在近距离的交火中，当秒数计算时，这是部队无法承受的危险延误。F-35防守队员总是迅速指出近乎对等的对手防空系统所谓的致命能力，作为在CAS中使用F-35以及禁止轰炸的必要性的理由。空军上校Mike Pietrucha介绍了一个更健全的战术和历史观点，指出在一个重防空威胁领域飞行CAS任务的情景充其量不太可能。繁琐，缓慢，物流密集的“高威胁”导弹系统不太可能被近邻敌人进行现代机动战争拖累。正如他们在第二次世界大战期间面对的那样，韩国，越南，沙漠风暴，我们的近距离支持飞行员更有可能面对较轻的轻型和移动防空（机枪，轻型高射炮和人类携带的寻热导弹）以及过去15多年的战争。在宣布F-35 IOC时，海军陆战队员 - 曾经将CAS作为独特海军遗产的一部分而获奖 - 而空军显然认为这些F-35 CAS限制是可以接受的。但是，看到近距离空中支援作为F-35计划的事后补救被视为可耻是可耻的。为了提供足够的CAS，纳税人的资金将更好地用于维持经过战斗验证的A-10，直到测试和部署更加有效且更实惠的后续工作。 海军的F-35不适合航母作战海军F-35变型必须具备的最重要特征之一是它必须能够从航空母舰上运行。否则，设计飞机的专业海军版本有什么意义？但海军自己的飞行员说F-35C并不适用于这些船只。发展测试显示，弹射器发射期间发生了大量的抽搐 - 称为“过度垂直振荡” - “使F-35C在操作上不适合航母作战，”在最新一套船舶试验期间在美国乔治华盛顿号航空母舰上进行训练的舰队飞行员表示“。从承载的甲板上起飞的飞机需要大力提升以达到提升和起飞所需的速度，这是通过安装在驾驶舱内的弹射器实现的。在喷气式飞机发射之前，飞行员增加发动机推力。为了防止喷气式飞机在发射前从船的前部滚落，它们会被挡住杆挡住。当推力被压下时，推力会压缩齿轮的支柱。根据2017年1月泄露给内部防御的海军报告，当释放后退杆并且发射喷射时，F-35C的支柱被卸下，导致机头上下弹跳，震动飞行员。这里的严重程度可以在这里清楚地看到：这个问题对飞行员来说很危险。头盔式显示器非常重，目前重量为5.1磅，当它与弹射器发射时产生的力相结合时，额外的重量会使飞行员的头部前后晃动。在70％的F-35弹射器发射中，飞行员报告头部和颈部出现中度至重度疼痛。发射也会影响头盔的对齐。飞行员报告说难以读取头盔内的重要信息，他们必须在进入空中后重新调整它。飞行员说，这是不安全的，因为它发生在任何飞行的最关键阶段之一。飞行员试图通过收紧身体安全带来抵抗振荡，但这会在紧急情况下难以触及紧急开关和弹射手柄，从而产生新的问题。 F-35的项目经理克里斯托弗·波格丹中将表示，他将尝试对F-35C的前起落架支柱进行短期调整以解决问题，但实际上可能需要长期修复，例如重新设计整个前起落架组件。这种情况不太可能在2019年之前开始 - 同年海军已表示有意宣布F-35C准备战斗。 到那时，海军很可能在舰队中拥有36架F-35C，其中每架都需要更换前起落架，但需要确定成本。 F-35C的问题不仅限于飞行的开始。就像喷气式飞机需要从航空母舰起飞一样，它也需要在着陆期间停止帮助。这是通过横跨甲板的电缆实现的。当一架喷气式飞机降落时，飞机上的一个挂钩抓住其中一根电缆，该电缆使用船内的液压发动机吸收能量并使喷气机停止运转。 他的测试团队发现F-35C的制动装置上的钩点磨损速度比预期快三倍。 虽然它应该至少持续15次着陆，但在测试中持续时间最长的一个钩点是5。 据报道，该计划正在考虑重新设计制动装置以使其更加坚固。F-35C还有待解决的另一个结构性问题涉及机翼。在试飞期间，工程师发现机翼末端的强度不足以支撑AIM-9X短程空对空导弹的重量。 F-35C的机翼两端折叠，以便在飞机载体的甲板和机库的拥挤范围内节省空间。当导弹经过机翼折叠时，当飞机难以操纵和着陆时，重量超过结构限制。根据DOT＆E的说法，在问题得到解决之前，“F-35C对于导弹运输和就业将具有有限的飞行范围，这将对机动，[和]近距离接合产生不利影响。”这甚至比F-35的其他固有机动限制。问题非常严重，波格丹将军承认F-35C将需要一个完全重新设计的外翼。发射和恢复飞机只是海军航空挑战的一部分。维护人员还必须能够在海上保持喷气式飞机的飞行性。机组必须能够执行的关键维护功能之一是发动机拆卸和安装（R＆I）。 2016年8月，Crews在乔治华盛顿号航空母舰上进行了第一次R＆I概念验证演示。机组人员需要55个小时来完成发动机交换，这比在传统飞机上执行相同操作所花费的时间要长得多。例如，F / A-18上的发动机可以在六到八个小时内更换。 DOT＆E指出，为了安全起见，机组人员花时间执行所有必要步骤，并指出随着机组人员获得更多经验，未来的迭代可能会更快一些。也就是说，机组人员充分利用了整个机库海湾空间，这是他们乘坐飞机机翼时不会有的东西。这可能加快了演示期间的过程。 更换F-35中的发动机比F / A-18更复杂。工作人员必须拆除几个皮肤面板和一个称为尾钩支架的大型结构件，以便拆卸发动机，从而在维护机库中需要更多空间。这些部件以及与之相关的所有管子和电线必须妥善储存，以防止损坏，同时还要占用额外的空间。 维护人员必须在存在全空气翼的情况下执行此过程，以便了解系统是否在操作上合适。并且该过程必须变得非常有效，以产生战斗所需的出击率。 在乔治华盛顿试验期间发现的另一个问题涉及F-35C计算机生成的大量数据文件的传输。 F-35计划依赖于自动后勤信息系统（ALIS），这是庞大而复杂的计算机系统，所有F-35都用于任务规划，维护诊断，维护计划，零件订购等。为了正常工作，系统必须在船上和船外通过网络移动大量数据。 在华盛顿试验期间，机组人员必须通过船舶的卫星网络传输中等大小的200 MB ALIS文件。花了两天时间。带宽限制和不稳定的连接极大地阻碍了数据的传输。许多这样的变速器 - 甚至更大的变速器 - 将需要支持整个机翼。此外，舰队经常在“排放控制”或无线电静音期间运行，以避免将其位置泄露给敌人，进一步阻碍了保持F-35飞行所需的数据传输。 乔治华盛顿的审判产生了大量令人讨厌的新闻报道。至少在公开场合，海军声称成功了。然而，有证据表明海军对该计划不太兴奋，因为上面讨论过这类问题，当然还有成本 - 服务购买F-35C的速度很慢。 虽然空军准备在2017年购买44架新的F-35，但海军只会买两架。海军还在其2017年无资金优先顺序（“愿望”）名单中要求另外增加14架F / A-18，并再增加两架F-35C。此外，这是服务没有急于过早宣布战斗准备的唯一变种。五角大楼的一些领导人表示，海军的变种是唯一一个受特朗普政府下令审查的威胁，而国防部长詹姆斯马蒂斯目前正在进行审查。这可能证明是该计划的一部分，其中寻求F-35的可行替代方案。 关于F-35唯一隐形的东西 - 价格标签自大选以来，人们对F-35的进一步采购和可负担性表示了很多看法。唐纳德·特朗普总统在就职典礼前对一系列推文中的价值提出了质疑，但当他宣布洛克希德·马丁从最新一批F-35的价格中削减6亿美元时，他希望该计划能够大幅改变。 。洛克希德·马丁公司及其在日本特许厅内的合作伙伴已表示价格会降低，这主要是由于制造业的效率提高。从表面上看，这对美国纳税人来说似乎是一个巨大的发展，但现在任何“节省下来”的钱最终都会花费更多，因为我们购买了一堆未经测试的原型，后来需要进行大量昂贵的改造。如果洛克希德·马丁公司和联合计划办公室能够在计划完成测试和评估过程之前批准对400架F-35进行为期三年的“大量购买”，那么这个问题就更加复杂了。报刊上报的价格通常基于空军常规起飞变型F-35A的成本 - 这三种变型中成本最低。此外，这个成本数字只是对未来成本的估计，假设从现在开始，F-35的一切都将完美运行 - 这不太可能，因为该计划进入其技术最具挑战性的测试阶段。正如最新的DOT＆E报告显示，在F-35准备战斗之前，该计划还有很长的路要走。联合计划办公室最近声称F-35A的价格在2016财年合同中每个都低于1亿美元。然而，在2016财年的立法中，国会为每架F-35A拨款1.196亿美元。即使这个数字也不能说明整个故事 - 它只包括采购成本，而不是将F-35A带到最新批准的配置所需的成本，以及用于容纳和操作F-35A的额外军事建筑成本。当然，1.196亿美元的价格标签不包括开发和测试F-35A的任何研发费用。海军陆战队F-35B和海军F-35C的2016年仅生产成本分别为1.664亿美元和1.852亿美元。首先，它们不包括修复最近，当前和未来测试中发现的设计缺陷所需的成本 - 这不是一笔不大的金钱。它们也不包括计划的现代化努力的成本，例如飞机的第4座，将来将被纳入所有F-35A。政府问责办公室估计，该计划将在未来六年内至少花费30亿美元用于现代化工作。据GAO称，例如，到目前为止所解决的一些问题的修改费用为4.267亿美元。这些飞机中的每一架都已经过修改，将来需要更多。空军已经承认必须改装交付给它和运营舰队的所有108架F-35A。随着已知问题得到修复并且发现新问题，这些成本将继续增长，并且它们是每架飞机成本的组成部分。随着程序从测试的简单部分 - 开发或实验室测试 - 转移到未来几年的关键作战（运行）测试期，将会发现更多问题。一个很好的例子发生在2016年底，当时工程师在F-35的油箱内发现了碎片。经过仔细检查，他们发现包裹在冷却剂管线周围的绝缘材料已经解体，因为分包商未能使用适当的密封剂。并且，当GAO估计将花费4.267亿美元来修复已经在仓库中的F-35A中的一些已知问题时，尚未发现冷却剂管线绝缘问题。必须在已经生产和购买的飞机机队中设计，测试和实施对此问题和其他问题的修复。其次，JPO，洛克希德马丁公司和五角大楼所使用的不完整的单位成本估算 - 他们所谓的“飞行​​”单位成本 - 不包括购买支持设备（工具，ALIS计算机，培训模拟器，初始备件）需要使F-35A机队能够运行。从字面上看，国防部的“飞行”成本并没有购买能够进行飞行操作的系统。五角大楼已经承诺购买346架F-35，因为该计划已进入美国国防部委婉地称之为“低速初始生产”。798喷气式飞机服务将在2018年至2021年期间大约450架次购买的798架喷气式飞机将占总采购量的近33％……所有这些都在该计划完成初步运行测试之前，并且发现了什么按预定工作，什么没有吨。值得注意的是，真正的问题发现过程只会在2019年按计划开始运行测试时开始，或者更有可能在2020年或2021年开始，当时运营代表性飞机实际上已准备好进行测试。空军已经开始修改的108架飞机只是冰山一角，这个数字不包括数百架海军陆战队和海军飞机的类似修改。拟议的“大宗购买”提出了许多其他问题。也许Gilmore所提出的最相关的问题是：Block Buy是否与政府主张的“购买前飞行”方法一致，以及“美国法典”第10篇中规定的运营测试要求的理由，还是被视为“全额” “IOT＆E之前的决定是否已经完成并向国会报告，不符合法律规定？只要符合某些标准，联邦法律允许多年合同购买政府财产。国会通常每年批准大多数武器购买计划，以确保对计划进行适当的监督，并保持对承包商表现令人满意的激励。根据Title 10 U.S.C.，Section 2306b，对于有资格进行多年采购的计划，合同必须促进国家安全，应该节省大量资金，减少几率，并且设计稳定。 F-35似乎在前三个标准中至少有两个失败，并且肯定是第四个失败了。关于F-35成本问题的一个重要部分是购买大型飞机是否合理，并担心以后修复尚未发现的问题的成本。这肯定是增加成本的好方法，但在临时中隐藏它。 实际操作F-35机队的成本仍然存在。 美国国防部估计，该计划50年的所有培训和运营运营 - 假设每架飞机的寿命为30年 - 将为1万亿美元，使购买和运营F-35的成本至少达到1.4万亿美元。操作F-35的成本非常高，因为飞机与其他飞机相比非常复杂。根据空军自己的数据，2016财年，每架F-35飞机平均飞行163小时，每小时飞行4426美元。为了进行比较，在同一年，机队中的每架F-16飞机平均飞行258小时，每飞行小时20,398美元。 A-10平均每小时飞行358小时，每小时17,227美元。虽然这些时间从未经过独立审核，而且无法确定它们是否完整，但现有数据表明F-35的飞行成本是飞机的两倍以上。五角大楼隐藏F-35真正成本的一个更重要的方式是它推迟到第4区块开发和交付应该在第3区提供的许多关键能力。目前已计划但未包括根据政府问责局的数据，在F-35的官方成本估算中 - 或者甚至作为一个完整的单独收购计划 - 是一个由四部分组成的Block 4升级，至少耗资30亿美元。此外，DOT＆E报告称“有17个记录的失败，无法满足规范要求，程序承认并打算寻求合同规范变更，以便结束SDD [系统开发和演示]。”这意味着F-35计划无法提供17种关键作战能力，而且计划办公室正试图给洛克希德·马丁公司提供交付通行证，直到后期的高级开发过程。虽然没有人公开声明现在不会包括哪17种战斗能力，但它们都是F-35应该拥有的所有功能，并且美国人民正在为此付出全价。因此，我们将来会支付更多的钱来升级现在购买的F-35，以便他们能够执行我们已经支付的功能。2016年F-35A的1.196亿美元单位成本严重低估，多年来不会充分了解额外成本。那些假装2016年成本低于1亿美元的人只是在欺骗公众。 战斗力有效在每一流的空军中，出战优秀的战斗机飞行员要求他们每月至少飞行30个小时来磨练和提高他们的战斗技能。这就是F-35缺乏战斗力的最大原因：由于飞机前所未有的复杂性和相应的可靠性和维护负担，飞行员根本无法经常飞行以获得足够的实际飞行时间来发展他们需要的战斗技能。如果飞行员无法获得足够的飞行时间，飞行员技能就会萎缩。即使拥有卓越的技术，训练有素的飞行员也不会受到训练有素的飞行员在飞行不太复杂的飞机上的影响。飞行时间不足也会造成危险的安全状况，威胁到飞行员的训练生命。海军陆战队在过去一年中遭遇了九次严重的飞机坠毁事故，造成14人死亡。该军团的顶级飞行员最近表示，撞车事故的飙升主要是由于飞行员没有足够的飞行时间。这种趋势将随着F-35而恶化。鉴于其固有的复杂性和相关的成本，F-35极不可能经常飞行以获得成功的飞行员。 F-35可以在需要的时候到达吗？即使这是一个很大的中频，F-35也可以像洛克希德·马丁所说的那样在战斗中表现出来 - 更不用说F-16，A-10和F-18的替代品应该如何表现 - 如果喷气式飞机不能满足他们需要的地方，那么该计划仍然毫无价值。有几个因素导致难以及时部署F-35中队。一个是F-35的任务规划系统，它是ALIS网络的一部分。在完成战斗任务的细节 - 例如目标，预测的敌方雷达位置，要飞行的路线和武器装载 - 之后，需要将数据编程到飞机中。将该信息加载到墨盒上，然后将墨盒插入喷嘴中。F-35飞行员在Offboard Mission Support（OMS）系统上对这些弹药筒进行编程。DOT＆E发现的问题是，飞行员一直认为用于支持任务规划的系统“繁琐，无法使用，并且不适合操作使用。”他们报告说，构建任务计划文件所需的时间太长，以至于扰乱了超过一架飞机的任务计划周期。 这意味着当几架F-35接收任务时，如果分配了大量的计划时间，他们就无法足够快地完成所有飞行前过程以按时启动。2016年2月和3月，空军在加利福尼亚的爱德华兹空军基地向爱达荷州的山地空军基地进行了部署演示，对F-35计划进行了重大测试。这是该服务首次尝试使用更新ALIS的版本 - 基于地面的计算机系统，用于诊断机械问题，订购和跟踪更换零件，并指导维修人员进行维修。无论何时中队部署，都必须在部署F-35的任何地方建立一个ALIS枢纽。 Crews建立了一个ALIS标准操作单元（SOU），它由几个计算机设备组成。技术人员将使用这些设置一个小型主机，然后必须将其插入全球范围的ALIS网络。工作人员花了几天时间让ALIS在本地基础网络上工作。经过大量的故障排除后，IT人员发现他们必须在Internet Explorer上更改多个设置，以便ALIS用户可以登录系统。这包括降低安全设置，DOT＆E以值得称道的轻描淡写的方式指出这是“可能与所需的网络安全和网络保护标准不兼容的行为”。ALIS数据必须在中队所在的任何地方。在飞机被允许执行飞行任务之前，机组人员必须将数据从本垒站的中队主ALIS计算机传输到部署的ALIS SOU。在Mountain Home部署期间，此过程花了三天时间。这比之前的演示要快，但洛克希德·马丁为演习提供了8位额外的ALIS管理员。目前还不清楚承包商或空军是否会在未来的部署中包含这种级别的支持。当演习结束时中队重新部署回爱德华兹时，管理员花了四天时间将所有数据传回主ALIS计算机。这种延迟将限制F-35在危机时刻快速部署的能力。 即使喷气机能够定位足够的时间来应对危机，长时间上传时间等问题也可能使它们在空中需要时保持在地面上。固定在地面上的飞机是目标，而不是资产。 另一个耗时的过程涉及向每个ALIS标准操作单元添加新飞机。每次将F-35从一个基座移动到ALIS已经启动的另一个基座时，必须将其导入该系统。这需要24小时。因此，当F-35部署到新基地时，整个一天会在处理数据时丢失。并且一次只能上传一架飞机。如果整个中队（通常是12架飞机）需要被引导，整个过程将需要将近两周时间，迫使指挥官慢慢将他的F-35飞机投入战斗。该计划的关键任务软件也出现延误。如前所述，F-35需要广泛的任务数据负载（MDL），以便飞机的传感器和任务系统正常运行。 MDL在某种程度上包括有关敌人和友好雷达系统的信息。他们发送喷气式传感器的搜索参数，以便他们正确识别威胁。这些需要更新以包含最新信息。它们也适用于每个主要地理区域。MDL都在佛罗里达州埃格林空军基地的美国重编程实验室进行编程，然后发送给所有相关的中队。该实验室是整个F-35计划中最重要的组成部分之一。据DOT＆E称，该实验室必须能够“快速创建，测试和优化MDL，并在代表现实情景的压力条件下验证其功能，以确保F-35任务系统的正常运行和飞机的运行效率。战斗以及F-35与Block 3F的IOT＆E。“官员们在2012年确定了该实验室管理层的严重缺陷。纳税人在2013年至2016年期间花费了4500万美元来解决这些问题。尽管有警告和额外资金，但实验室的开发仍然受到管理不善的困扰，这种管理不能阻止在当前的基本作战配置中“有效地创建，测试和优化运营飞机的MDL”。需要升级实验室以支持F-35上使用的每个软件版本。该实验室目前配置为支持块2B和3i软件版本。 F-35的第一个完全战斗版软件将是Block 3F。该实验室需要进行重大更改以支持此版本，这对于战斗测试是必要的，更重要的是，完全战斗准备就绪。实验室远远落后于一些必要的设备甚至尚未购买。例如，该设施还依赖于前面提到的专用射频发生器来重新创造潜在对手可能对F-35使用的那种信号。实验室将使用它们来测试MDL，然后将它们发送到机队飞机上，以确保喷气式飞机的传感器能​​够正确识别它们。在急于假装初始作战能力的情况下，空军和海军陆战队实际上已经制造出一架完全未准备好面对敌人的飞机。 F-35可靠性问题即使一架F-35中队能够到达需要的地方，在需要它的时候，如果它不能飞行任务会有什么好处呢？这是F-35计划中最持久的问题之一。 该车队的可靠性记录非常糟糕 - 它未能实现许多临时可靠性目标，并且在2016年之前仍然如此。随着该计划进入最重要的运行测试阶段，真正担心飞机会不能经常飞行以满足测试时间表。还有人担心喷气式飞机在召唤战斗服务时能够多久飞行一次。“可用性”衡量飞机在现场执行至少一次指定任务的频率。由于大多数飞机在1991年在波斯湾的沙漠风暴行动中取得了成功，这些服务努力维持其飞机可持续作战行动的80％可用率。这与测试机队为满足这一需求所需的速度相同。 IOT＆E时间表。到目前为止，F-35计划甚至无法实现其60％可用性的临时目标。 2016年度车队平均可用率为52％。这是近几年来的一次改善，但DOT＆E警告称“增长既不稳定也不连续。”而且增长曲线落后于进度。将用于运行测试的飞机需要配备专门的仪器来测量性能。目前有17架喷气式飞机驻扎在加州爱德华兹空军基地。 2016年前9个月，该测试车队的平均可用率为48％。 有几个因素拖累了F-35机队的可用率。许多飞机不得不被送回仓库进行大修，这是该程序高并发水平的结果。例如，15 F-35A需要被送回以纠正制造缺陷，其中喷射燃料箱内的泡沫绝缘材料使铸造碎屑劣化成燃料。其他大修是必要的，因为存在基本的设计缺陷，包括不满足寿命要求的主要结构部件，还有一些是由于飞机首次建造时已知缺乏的战斗能力设计的持续改进所驱动。 ” 即使飞机没有进行大修，它们也不会飞得太多。在可用的飞机中，它们可以分为两类：任务能力和完全任务能力。使命能力的飞机是那些准备进行至少一种任务的飞机，即使它只是一项训练任务;具有完全任务能力的飞机是准备执行飞机宣布能够执行的所有任务的飞机。后者是战斗准备飞机的真正衡量标准。 Mission Capable和Full Mission Capable F-35的可用率在去年有所下降。 2016财年的机队任务能力率为62％，低于2015财年的65％[DG3]。完全任务能力率仅为29％，而前一年为46％。 Gilmore的报告引用了分布式孔径系统，电子战系统，电光靶向系统和雷达等主要作战系统的失效，这是导致能力下降的最高驱动因素。值得注意的是，这些系统据说为F-35提供了独特的作战能力，这就是将F-35保持在地面上的系统 - 没有任何能力。根据最近发布的年度运营成本图表，平均而言，2016年空军的F-35飞机每周只能飞行两架次。相比之下，F-16平均每周近三架次，A-10机队平均近四架次。 F-35需要大量的维护才能实现。 虽然官方发布的公开声明说维护人员在喷气式飞机上工作是多么容易，但DOT＆E报告描绘了不同的画面。 供应链问题已经迫使维护者蚕食飞机;从一架飞机上取下部件安装在另一架飞机上，以确保至少有一架飞机飞行。食人化具有增加进行修复的总时间的效果，因为它增加了从供体喷射器剥离部件的额外步骤，而不仅仅是从盒子中取出新的或修复过的部件。它还需要将部件安装两次：首先是在修复的喷射器中，然后是在拆卸的喷射器中。 对于2016财年，维护人员不得不拆掉几乎每10架次飞行一次的部件，这远远超过了计划中每100架次不超过8次拆分行动的不起眼的目标。 随着产量的增加，供应问题可能会减少，但基本设计问题将持续存在。 一个典型的例子是F-35隐形涂层固有的独特维护要求。 对隐形飞机进行一些修理需要更长的时间，因为需要时间去除低可观察到的材料，修复破损的物体，然后修复隐形皮肤。这些修复通常涉及使用需要时间进行化学固化的粘合剂。 其中一些材料可能需要长达168小时 - 一整周 - 才能完全干燥。 官员隐瞒F-35问题和纳税人延误的真相当洛克希德·马丁在17年前首次赢得合同时，预计F-35将在2008年开始运行测试。一旦他们未能达到这一目标，2017年应该是战斗测试过程开始的重要一年。我们现在知道，这个过程几乎肯定会被推迟到2019年……甚至2020年。DOT＆E报告的第一页列出了F-35的13个未解决的主要问题，这些问题将阻止该计划于2017年8月开始进行战斗测试。但你不会从负责官员的公开评论中得知这些问题。该程序。在2月份众议院军事委员会小组委员会作证时，尽管DOT＆E报告在不到一个月前发布，但官员却没有向国会提出任何这些问题。F-35的挑战规模在今年的DOT＆E分析中很容易量化。根据该报告，F-35仍然有276个“至关重要的”缺陷 - 这些必须在开发过程结束之前修复，因为它们可能“导致IOT＆E或战斗期间的操作任务失败”。在276个中，有72个被列为“优先级1”，这些服务关键缺陷会阻止服务在固定之前部署喷气式飞机。关于F-35在战斗中的缺点已经有很多，但基本机身仍然存在结构性问题。这方面的一个例子是喷气机的垂直尾翼和机身之间的连接接头失效。这是一个长期存在的问题，因为在原始设计中发现了缺点。工程师在2010年的早期结构测试中发现用于加固接头的套管过早磨损。该接头在2014年进行了重新设计并纳入新飞机。 2016年9月，检查员发现重新设计的接头在经过250小时的飞行测试后失败了 - 远远低于JSF合同中规定的8,000个工作小时数。2016年F-35任务系统的测试继续落后于计划。项目经理确定并预算基线测试点，或“在特定飞行测试条件下的性能离散测量”。这些用于确定系统是否符合合同规范。测试团队还会出于各种原因添加非基线测试点，以全面评估整个系统。示例包括添加测试点以准备稍后的更复杂的测试，在软件更新后重新测试系统以确保新软件不会改变先前的结果，或“发现测试点”，这些测试点被添加以识别在其他测试期间发现问题的根本原因。该计划为2016年F-35的任务系统预算了3,578个测试点。测试团队无法完成所有测试，完成3,041，同时还在全年增加了250个未预算的测试点。尽管计划有所下滑，但F-35计划办公室已表示希望跳过许多所需的测试点，而是依赖测试以前飞行的数据 - 测试飞机使用早期软件版本 - 作为升级系统软件工作的证据。但DOT＆E警告说，较新的软件版本可能表现不同，使早期的结果没有实际意义。程序管理员基本上想要宣布开发测试过程并继续进行操作测试，即使他们还没有完成所有必要的步骤。这是一个风险很大的举动。 DOT＆E警告说遵循这个计划。 “可能会导致IOT&amp;E失败，导致需要进行额外的后续运行测试，最重要的是，将Block 3F运送到能力严重不足的现场 - 如果需要F-35，该部门必须具备的能力 与当前威胁作斗争。“程序办公室似乎在试图测试许多可能使F-35如此不可或缺的能力方面拖延了下来。一个例子是开发验证模拟器（VSim）需要多长时间。洛克希德·马丁公司的工程师在2001年负责创建VSim设施，该设施旨在成为一个超现实的，经过全面测试验证的“人在环，任务系统软件在环仿真，旨在满足Block 3F IOT＆E的运行测试要求。“也就是说，它旨在在虚拟现实中测试那些复杂而严谨的场景，这些场景在现实生活中不可能或太危险，而不是真正的战争。承包商远远落后于建设计划，JPO在2015年放弃了VSim。相反，海军航空系统司令部的任务是建立一个政府运行的联合仿真环境（JSE）来执行VSim的任务。承包商应该提供飞机和传感器模型，但到目前为止“F-35模型的谈判尚未成功。”这阻碍了该计划设计虚拟世界，其中F-35和敌方飞机和防御装置在现实世界中相互作用，造成进一步的延误。没有经过适当准备的JSE，F-35无法完全测试。必须根据飞行试验期间收集的真实数据设计模拟，否则模拟只会测试承包商所说的喷气机可以做什么。例如，真正的F-35必须飞越一个测试范围，我们的敌人使用相同的雷达系统是活跃的，以便它可以收集有关喷气式飞机的机载传感器如何反应的数据。该数据用于验证仿真软件。这是一个非常复杂的过程需要时间。正如DOT＆E报道的那样，“此前的努力已经花费了数年时间，因此NAVAIR不太可能及时按计划完成项目，以支持物联网和E。”该计划还制定计划，以便在计划最需要时减少测试人员和测试飞机的数量。这些计划将使测试飞机的数量从18个减少到9个，测试人员从1,768减少到600。吉尔摩尔在空军国际奥委会声明后不久报道该计划将无法在必要的最终配置中生产足够的F-35以进行操作测试。“由于在开发测试期间需要很长的程序延迟和发现，因此需要进行大量修改，以便将装配过程中连接到飞行测试仪器的OT飞机纳入所需的生产代表配置中，”报告指出。接着说，对23架飞机进行了超过155次改装，专门用于即将进行的战斗（“作战”）测试，其中一些甚至尚未签约，这意味着IOT和E的开始将更进一步延迟。 联合计划办公室不仅没有遵守其同意的运行测试计划，而且未能资助和测试进行测试所必需的设备。 这包括没有资金用于飞行测试数据采集记录和遥测舱，这是一种安装在F-35上的仪器，用于模拟飞机的武器。这对于报告和分析每个模拟武器射击的结果至关重要。 在飞机在接合和武器测试期间飞行的条件下，在吊舱功能和安全性被清除之前，不能进行此类测试。五角大楼和承包商是否会继续忽视有关F-35在测试中的表现以及看似无休止的延误的令人不快的信息，并试图在美国人民和他们的心中产生错误的印象，还有待观察。政策制定者。在最近特朗普总统和五角大楼之间的交流中，似乎没有人在日本特别行政区引导总统注意除波格丹将军以外的任何人。 很明显，他没有和任何批评该计划的人谈过，比如Gilmore。 根据这份报告的结果，如果他有，那么很难看出任何人都可以诚实地说F-35是“太棒了”。 向前进DOT＆E的最新报告更加证明F-35计划将在未来几年继续大量耗费时间和资源，并将为我们的武装部队提供一台二战战斗机，其执行任务不如它本来要取代的“传统”飞机。上天捍卫国家的男男女女应该得到更好的待遇。尽管在华盛顿有传统智慧，但服务并不一定要坚持使用F-35。其他选项确实存在。1.为了填补空对空中的近期空洞，启动一项计划，对所有可用的F-16A和F-18进行翻新和升级，使用寿命更长的机身和更高的推力F-110-GE- 132（F-16）和F-404-GE-402（F-18）发动机。使用功能更强大的现成电子系统升级其电子系统。 这将使我们的战斗机在空对空作战中比后来的F-16和F-18型号或F-35更有效。如果需要增加力量，从boneyard添加机身。最重要的是，将飞行员训练时间提高到每月30小时的最低可接受水平，部分原因是现在不购买欠发达的F-35而节省了资金。 2.为了填补近距离空中支援部队中更为严重的近期空洞，完成空军拒绝重新训练的100架A-10的重绕，然后通过整修/扩大现有的仅272架A-10的力量。将boneyard中所有可用的A-10重新调整为A-10C标准。 3.立即进行三个新的竞争性原型飞行计划，设计和建造一个更致命，更生存的近距离空中支援飞机，以取代A-10，并设计和建造两个不同的空对空战斗机，这些战斗机更小，更具战斗力 - 比F-16，F-22和F-18更有效。对所有配备雷达导弹和隐身对抗措施的合格敌人进行测试。这些程序应该遵循20世纪70年代轻型战斗机和A-X计划的模式，特别是在实弹，现实情景竞争飞越测试方面。这些计划产生了F-16和A-10两架无可争议的高效飞机，每架飞机都比当时五角大楼的首选飞机便宜。他们在不到10年的时间内进行了测试，但不超过25年。 4.绝对最低限度，F-35测试程序已经到位，JPO和Gilmore同意必须执行以便在进一步生产之前理解这架飞机能够胜任和不能胜任的事情。这意味着暂停进一步的F-35生产，直到这些测试完成并诚实地报告给国防部长，总统和国会。 结论F-35计划办公室已达到关键决策点。 现在需要采取大胆行动来挽救联合攻击战斗机这样的国家灾难。 政府应继续审查F-35计划。 但官员们不应该只是与将军和高管交谈，因为他们没有动力去讲述真相，因为他们在确保程序存活方面有着既得的经济利益 - 无论能力如何。 正如本报告所示，他们并没有讲述整个故事。 从其他观点来看，还有更多人在食物链中走下坡路。 他们是拥有真实故事的人。 而且，正如上述建议所示，仍有选择。 对于该计划进行重大改变还为时不晚，正如其维护者所声称的那样。 Dan Grazier是政府监督项目的Jack Shanahan研究员，这篇文章最初出现在那里。","categories":[],"tags":[]},{"title":"眼底水肿病变区域自动分割","slug":"yuque/眼底水肿病变区域自动分割","date":"2018-12-21T01:28:40.000Z","updated":"2019-04-04T13:49:28.590Z","comments":true,"path":"2018/12/21/yuque/眼底水肿病变区域自动分割/","link":"","permalink":"http://zhos.me/2018/12/21/yuque/眼底水肿病变区域自动分割/","excerpt":"","text":"链接 背景 视网膜水肿是一种眼疾，可导致视力模糊，影响正常生活。 OCT（光学相干断层扫描）可用于帮助医生判断视网膜水肿。 早期发现水肿症状可以在疾病的治疗中发挥关键作用。 我们的任务是设计算法，以自动检测视网膜水肿的类型，并根据OCT图像划分视网膜水肿区域。 数据统计信息 数据可视化 视网膜边缘弯曲 病变之间的包含关系 数据处理堆叠上部和下部切片以形成三通道输入正则化数据增强（仅随机水平翻转） 问题和挑战这两项任务（分割和检测）如何相互促进？ •多任务学习框架多尺度的视网膜水肿病变•UNet和UNet ++三种视网膜水肿样本不平衡•指数对数损失如何扩大感受野以检测边缘弯曲？•扩张模块 基线分段 - UNet•下采样16×•频道：[16,32,64,128,256]•输入：原始图像检测 - ResNet18•调整大小224 * 224 多任务框架 共同学习分割和检测 减少时间和计算成本 改善两项任务的效果 UNet ++和DeepSupervision 密集的未来联系 深度监督 更有效地融合低级和高级功能 Exponential Logarithmic Lossx : pixel position d il : Kronecker deltai : label e : pseudocount forl : ground truth label at xp i ( x ) : Softmax probability which acts as the portion of pixelx owned by label i小物件怎么样？细分？（2,3级） 扩张模块感受野计算公式UNet编码器的接收域大对象怎么样？细分？（第1类） 实验摘要Memory : 7.3 G(batch=8), Inference time : 9.5 s/patient 可视化 未来工作 检测框架（Mask R-CNN） 3D语义分割模型 在骨干中使用Res-block或Dense-block 考虑病变的关系 结论构建端到端的多任务框架，可以同时检测和分割视网膜水肿病变。•使用最新的UNet ++模型更好地集成高级和低级功能。使用新的指数对数损失函数来增强两种类型的小病变的分割。•引入扩张卷积模块，显着增加模型的感受野。只有随机水平翻转数据增强，没有后期处理。•测试装置上单个模型的骰子为0.736。 测试集上的融合模型的骰子为0.744，检测AUC为0.986。 另外，当我们设置批次为8时，推理阶段的记忆为7.3G，每个患者的推理时间为9.5s。","categories":[],"tags":[]},{"title":"语义分割 -  U-Net（第1部分）","slug":"yuque/语义分割 -  U-Net（第1部分）","date":"2018-12-20T07:47:26.000Z","updated":"2019-04-04T13:49:28.590Z","comments":true,"path":"2018/12/20/yuque/语义分割 -  U-Net（第1部分）/","link":"","permalink":"http://zhos.me/2018/12/20/yuque/语义分割 -  U-Net（第1部分）/","excerpt":"","text":"链接 这里再次写信给我6个月前的自我……在这篇文章中，我将主要关注语义分割，像素分类任务和特定的算法。 我将提供一些关于我最近一直在努力的案例的演练。根据定义，语义分割是将图像划分为连贯的部分。 例如，对属于我们的数据集中的人，汽车，树或任何其他实体的每个像素进行分类。 语义分割与实例分割与实例分割相比，语义分割相对容易。 在实例分割中，我们的目标不仅是对每个人，汽车或树进行像素预测，而且还分别将每个实体识别为人1，人2，树1，树2，汽车1，汽车2，汽车3 等等。 用于实例分割的现有技术算法是Mask-RCNN：具有多个子网络一起工作的两阶段方法：RPN（区域提议网络），FPN（特征金字塔网络）和FCN（完全卷积网络）[5， 6,7,8]。 案例研究：Data Science Bowl 2018数据科学比赛2018刚刚结束，我从中学到了很多东西。也许我学到的最重要的一课，即使是深入学习，与传统的ML相比，更自动化的技术，前后处理对于获得好的结果可能是至关重要的。这些是从业者获得的重要技能，它们定义了您构建和建模问题的方式。我不会详细讨论这个特定的比赛，因为对于任务本身和整个比赛中使用的方法都有大量的讨论和解释。但我会简要提及获胜的解决方案，因为它与这篇文章的基础有关。 [13]数据科学比赛2018就像其他数据科学比赛一样，由Booz Allen基金会组织。今年的任务是在给定的显微镜图像中识别细胞核并独立地为每个细胞核提供掩模。现在，花一两个时间来猜测这个任务需要哪种类型的细分;语义还是实例？这是一个掩盖图像样本，它是原始的显微镜图像。虽然起初听起来像是语义分段任务，但这里的任务是实例分割。我们需要独立地处理图像中的每个核，并将它们识别为核1，核2，核3，……类似于我们对汽车1，汽车2，人1等的示例。也许这项任务的动机是跟踪细胞样本中细胞核的大小，数量和特征。自动化该跟踪过程并进一步加速用于治疗各种疾病的不同治疗方法的实验是非常重要的。现在，您可能会认为如果本文是关于语义分段的，并且如果Data Science Bowl 2018是实例分段任务的一个示例，那么为什么我一直在谈论这个特定的竞争。如果你正在考虑这个问题，那么你肯定是对的，而且这次比赛的最终目标确实不是语义分割的一个例子。但是，随着我们将继续前进，您将看到如何将此实例分段问题实际转变为多类语义分段任务。这是我尝试但在实践中失败的方法，但也成为获胜解决方案的高级动机。在这3个月的比赛期间，只有两个模型（或其变体）在论坛中共享或至少明确讨论过; Mask-RCNN和U-Net。正如我之前提到的，Mask-RCNN是最先进的物体检测算法，它可以检测单个物体并预测其掩模，如实例分割。 Mask-RCNN的实施和培训更加困难，因为它采用了两阶段学习方法，您首先优化RPN（区域提案网络），然后同时预测边界框，类和掩模。另一方面，U-Net是一种非常流行的端到端编码器 - 解码器网络，用于语义分割[9]。它最初是发明并首次用于生物医学图像分割，这是我们为Data Science Bowl所做的一项非常类似的任务。竞争中没有银弹，没有任何一个没有邮政或预处理的建筑或建筑设计中的任何小调整都没有得到最高分。我没有机会为这次比赛尝试Mask-RCNN，所以我在U-Net周围进行了实验并学到了很多东西。此外，由于我们的主题是语义分割，我将把Mask-RCNN留给其他博客文章进行解释。但是如果你仍然坚持在自己的CV应用程序中尝试它们，这里有两个流行的github存储库，在Tensorflow和PyTorch中实现。 [10,11]现在，我们可以继续使用U-Net并深入了解它的细节……这是开始的架构：对于熟悉传统卷积神经网络的人来说，架构的第一部分（表示为DOWN）将是熟悉的。第一部分被调用，或者您可能认为它是编码器部分，您应用卷积块，然后进行maxpool下采样，将输入图像编码为多个不同级别的要素表示。网络的第二部分包括上采样和连接，然后是常规卷积操作。 CNN中的上采样可能是一些读者的新概念，但这个想法相当简单：我们正在扩展要素尺寸以使用左边相应的连接块来满足相同的尺寸。您可能会看到灰色和绿色箭头，我们将两个要素图连接在一起。与其他完全卷积分割网络相比，U-Net在这种意义上的主要贡献在于，在网络中进行上采样和深入研究的同时，我们将更高分辨率的特征从下部与上采样特征连接起来，以便更好地定位和学习表示。以下卷积。由于上采样是稀疏操作，因此我们需要从早期阶段获得良好的优先级以更好地表示本地化。在FPN（特征金字塔网络）中也可以看到组合匹配级别的类似想法。 [7]我们可以在下部定义一个操作块作为卷积→下采样。123456789101112131415# a sample down blockdef make_conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1): return [ nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True) ]self.down1 = nn.Sequential( *make_conv_bn_relu(in_channels, 64, kernel_size=3, stride=1, padding=1 ), *make_conv_bn_relu(64, 64, kernel_size=3, stride=1, padding=1 ),)# convolutions followed by a maxpooldown1 = self.down1(x)out1 = F.max_pool2d(down1, kernel_size=2, stride=2) 类似地，我们可以将一个操作块定义为上采样→连接→卷积。1234567891011121314151617181920# a sample up blockdef make_conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1): return [ nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True) ]self.up4 = nn.Sequential( *make_conv_bn_relu(128,64, kernel_size=3, stride=1, padding=1 ), *make_conv_bn_relu(64,64, kernel_size=3, stride=1, padding=1 ))self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1, stride=1, padding=0 )# upsample out_last, concatenate with down1 and apply conv operationsout = F.upsample(out_last, scale_factor=2, mode='bilinear') out = torch.cat([down1, out], 1)out = self.up4(out)# final 1x1 conv for predictionsfinal_out = self.final_conv(out) 通过仔细检查图形，您可能会注意到输出尺寸（388 x 388）与原始输入（572 x 572）不同。如果您希望获得一致的大小，您可以应用填充卷积来保持跨越级联级别的维度，就像我们在上面的示例代码中所做的那样。当提到这样的上采样时，您可能会遇到以下任一项：转置卷积，上卷积，反卷积或上移。包括我自己和PyTorch文档在内的许多人都不喜欢反卷积这个术语，因为在上采样阶段我们实际上正在进行常规的卷积运算，并且没有任何关于它的信息。如果您不熟悉基本的卷积运算及其算术，我会强烈建议您访问此处。 [12]我将解释从最简单到更复杂的上采样方法。以下是在PyTorch中对2D张量进行上采样的三种方法： 最近邻这是在将张量调整（转换）为更大张量时找到丢失像素值的最简单方法，例如， 2x2到4x4,5x5或6x6。让我们使用Numpy逐步实现这个基本的计算机视觉算法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def nn_interpolate(A, new_size): \"\"\" Nearest Neighbor Interpolation, Step by Step \"\"\" # get sizes old_size = A.shape # calculate row and column ratios row_ratio, col_ratio = new_size[0]/old_size[0], new_size[1]/old_size[1] # define new pixel row position i new_row_positions = np.array(range(new_size[0]))+1 new_col_positions = np.array(range(new_size[1]))+1 # normalize new row and col positions by ratios new_row_positions = new_row_positions / row_ratio new_col_positions = new_col_positions / col_ratio # apply ceil to normalized new row and col positions new_row_positions = np.ceil(new_row_positions) new_col_positions = np.ceil(new_col_positions) # find how many times to repeat each element row_repeats = np.array(list(Counter(new_row_positions).values())) col_repeats = np.array(list(Counter(new_col_positions).values())) # perform column-wise interpolation on the columns of the matrix row_matrix = np.dstack([np.repeat(A[:, i], row_repeats) for i in range(old_size[1])])[0] # perform column-wise interpolation on the columns of the matrix nrow, ncol = row_matrix.shape final_matrix = np.stack([np.repeat(row_matrix[i, :], col_repeats) for i in range(nrow)]) return final_matrix def nn_interpolate(A, new_size): \"\"\"Vectorized Nearest Neighbor Interpolation\"\"\" old_size = A.shape row_ratio, col_ratio = np.array(new_size)/np.array(old_size) # row wise interpolation row_idx = (np.ceil(range(1, 1 + int(old_size[0]*row_ratio))/row_ratio) - 1).astype(int) # column wise interpolation col_idx = (np.ceil(range(1, 1 + int(old_size[1]*col_ratio))/col_ratio) - 1).astype(int) final_matrix = A[:, row_idx][col_idx, :] return final_matrix [PyTorch] F.upsample(…, mode = “nearest”)1234567891011121314151617&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)&gt;&gt;&gt; input(0 ,0 ,.,.) = 1 2 3 4[torch.FloatTensor of size (1,1,2,2)]&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;nearest&apos;)&gt;&gt;&gt; m(input)(0 ,0 ,.,.) = 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4[torch.FloatTensor of size (1,1,4,4)] 双线性插值双线性插值算法的计算效率低于最近邻居，但它是一种更精确的近似。 根据距离计算单个像素值作为所有其他值的加权平均值。[PyTorch] F.upsample（…，mode =“bilinear”）12345678910111213141516&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)&gt;&gt;&gt; input(0 ,0 ,.,.) = 1 2 3 4[torch.FloatTensor of size (1,1,2,2)]&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&apos;bilinear&apos;)&gt;&gt;&gt; m(input)(0 ,0 ,.,.) = 1.0000 1.2500 1.7500 2.0000 1.5000 1.7500 2.2500 2.5000 2.5000 2.7500 3.2500 3.5000 3.0000 3.2500 3.7500 4.0000[torch.FloatTensor of size (1,1,4,4)] 转置卷积在转置卷积中，我们通过反向传播学习权重。 在论文中，我遇到了针对各种情况的所有这些上采样方法，并且在实践中，您可能会更改您的体系结构并尝试所有这些以查看哪种方法最适合您自己的问题。 我个人更喜欢转置卷积，因为我们对它有更多的控制权，但你也可以选择双线性插值或最近邻居。[PyTorch] nn.ConvTranspose2D(…, stride=…, padding=…)如果我们回到最初的案例，数据科学碗，在竞争中使用香草U-Net方法的主要缺点是重叠核。 如上图所示，如果您创建一个二元蒙版并将其用作目标，U-Net肯定会预测类似于此的东西，并且您将拥有多个核的组合掩模，这些核重叠或彼此非常接近。关于重叠实例问题，U-Net论文的作者使用加权交叉熵来强调学习细胞的边界。 此方法帮助他们分离重叠的实例。 基本的想法是更多地加权边界，并推动网络在近距离实例之间找到学习差距。[9]这种问题的另一种解决方案是许多竞争者使用的方法，包括获胜的解决方案，是将二进制掩码转换为多类目标。 U-Net的优点在于，您可以构建网络以根据需要输出任意数量的通道，并通过在最后一层使用1x1卷积来表示任何通道中的任何类。引自Data Science Bowl获奖解决方案： 用于具有S形激活的网络的2通道掩模，即（掩模 - 边界，边界）或用于具有softmax激活的网络的3通道掩模，即（掩模 - 边界，边界，1 - 掩模 - 边界）2通道全面罩即（面罩，边框） 在进行这些预测之后，诸如分水岭的经典图像处理算法可以用于后处理以进一步分割单个核。[14]这是第一次正式的计算机视觉竞赛，我有勇气参加Kaggle，它是一个数据科学碗。虽然我只在前20％（这被认为是平均分数）完成了比赛，但我感到很高兴参加数据科学碗并学习如果我实际上没有参与的话我可能永远不会学到的东西并试着靠自己。积极学习远比观看或阅读来自在线资源的类似方法更富有成效。作为一名刚刚开始用Fast.ai开始练习数月的深度学习练习者，这对我来说是一个重要的一步，也是我永无止境的旅程，在获得经验方面非常有价值。所以，对于那些在你之前从未见过或已经解决的挑战感到暗示的人，我强烈建议你专门去处理这些类型的挑战，以便感受学习以前你不知道的东西的乐趣。我在本次比赛中学到的另一个有价值的教训是，在计算机视觉中（这也适用于NLP）竞赛，通过眼睛检查每一个预测非常重要，看看哪些有效，哪些无效。如果您的数据足够小，您应该去检查每个输出。如果出现问题，这将允许您进一步提出更好的想法，甚至调试代码。 转移学习及其他到目前为止，我们已经定义了vanilla U-Net的构建块，并提到了我们如何操纵目标来解决实例分割问题。现在我们可以进一步讨论这些类型的编码器 - 解码器网络的灵活性。通过灵活性，我的意思是你拥有它的自由以及你可以对它进行设计的创造力。任何在某些时候进行深度学习的人都会转移学习，因为这是一个非常强大的想法。简而言之，转移学习是使用预训练网络的概念，该网络在许多样本上进行训练，以完成我们所面临的类似任务，但缺少相同数量的数据。即使有足够的数据传输，学习也可以在一定程度上提升性能，不仅适用于计算机视觉任务，也适用于NLP。迁移学习也被证明是U-Net类似架构的强大技术。我们之前已经定义了U-Net的两个主要组件;向上和向上。我们这次将这些部分重新编号为编码器和解码器。编码器部分基本上接受输入并将其编码在低维特征空间中，该特征空间表示我们在较低维度中的输入。现在想象用你最喜欢的ImageNet获胜者替换这个编码器; VGG，ResNet，Inception，NasNet，……你想要的。这些网络经过精心设计，可以做一个常见的事情：以最佳方式编码自然图像进行分类，ImageNet上的预训练重量等待您在线抓取它们。那么为什么不使用这些架构之一作为我们的编码器并以与原始U-Net相同的方式构建解码器，但更好的是，使用类固醇。TernausNet是Kaggle Carvana挑战赛的赢家架构，它使用与VGG11相同的传输学习理念作为编码器。 [15,16] Fast.ai：动态U-Net受到TernausNet论文和许多其他优秀资源的启发，我想概括为U-Net架构使用预训练或自定义编码器的想法。 所以，我想出了一个通用的架构：动态Unet。Dynamic Unet是这个想法的一个实现，它通过为您完成所有计算和匹配，自动为任何给定的编码器创建解码器部分。 编码器既可以是现成的预训练网络，也可以是您自己定义的任何自定义架构。它是用PyTorch编写的，目前在Fast.ai库中。 您可以参考此笔记本以查看其中的操作或查看来源。 Dynamic Unet的主要目标是节省实践者的时间，并允许使用尽可能少的代码更容易地使用不同的编码器进行实验。在第2部分中，我将解释体积数据的3D编码器解码器模型，例如MRI扫描，并给出我一直在研究的真实世界的例子。 References [5] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: https://arxiv.org/abs/1506.01497[6] Mask R-CNN: https://arxiv.org/abs/1703.06870[7] Feature Pyramid Networks for Object Detection: https://arxiv.org/abs/1612.03144[8] Fully Convolutional Networks for Semantic Segmentation: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf[9] U-net: Convolutional Networks for Biomedical Image Segmentation: https://arxiv.org/abs/1505.04597[10] Tensorflow Mask-RCNN: https://github.com/matterport/Mask_RCNN[11] Pytorch Mask-RCNN:_ _https://github.com/multimodallearning/pytorch-mask-rcnn[12] Convolution Arithmetic: https://github.com/vdumoulin/conv_arithmetic[13] Data Science Bowl 2018 Winning Solution, ods-ai: https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741[14] Watershed Algorithm https://docs.opencv.org/3.3.1/d3/db4/tutorial_py_watershed.html[15] Carvana Image Masking Challenge: https://www.kaggle.com/c/carvana-image-masking-challenge[16] TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation: https://arxiv.org/abs/1801.05746","categories":[],"tags":[]},{"title":"Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict","slug":"yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict","date":"2018-12-20T07:28:04.000Z","updated":"2019-04-04T13:49:28.590Z","comments":true,"path":"2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/","link":"","permalink":"http://zhos.me/2018/12/20/yuque/Pytorch：如何以及何时使用Module，Sequential，ModuleList和ModuleDict/","excerpt":"","text":"分享，重用和分解模型复杂性的有效方法 更新到 Pytorch 4.1版本 你可以找到代码 here Pytorch是一个开源的深度学习框架，提供了创建ML模型的智能方法。 即使文档制作完好，我仍然发现大多数人仍然能够编写错误的而不是有组织的PyTorch代码。 今天，我们将看到如何使用PyTorch的三个主要构建块：Module，Sequential和ModuleList。 我们将从一个例子开始，迭代地我们将使它变得更好。所有这四个类都包含在torch.nn中 12345import torch.nn as nn# nn.Module# nn.Sequential# nn.Module 模块：主要构建块Module是主要的构建块，它定义了所有神经网络的基类，你必须将它子类化。让我们创建一个经典的CNN分类器作为示例： 123456789101112131415161718192021222324252627282930import torch.nn.functional as Fclass MyCNNClassifier(nn.Module): def __init__(self, in_c, n_classes): super().__init__() self.conv1 = nn.Conv2d(in_c, 32, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 28 * 28, 1024) self.fc2 = nn.Linear(1024, n_classes) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = F.relu(x) x = self.conv2(x) x = self.bn2(x) x = F.relu(x) x = x.view(x.size(0), -1) # flat x = self.fc1(x) x = F.sigmoid(x) x = self.fc2(x) return x 12model = MyCNNClassifier(1, 10)print(model) 12345678MyCNNClassifier( (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=25088, out_features=1024, bias=True) (fc2): Linear(in_features=1024, out_features=10, bias=True)) 这是一个非常简单的分类器，其编码部分使用两层3x3 convs + batchnorm + relu和一个带有两个线性层的解码部分。 如果您不是PyTorch的新手，您可能以前看过这种类型的编码，但有两个问题。如果我们想要添加一个图层，我们必须在init和forward函数中再次编写大量代码。 此外，如果我们有一些我们想要在另一个模型中使用的公共块，例如 3x3 conv + batchnorm + relu，我们必须再写一次。 Sequential：堆叠和合并图层 Sequential是一个模块的容器，可以堆叠在一起并同时运行。你可以注意到我们必须将自己的一切存储起来。 我们可以使用Sequential来改进我们的代码。 12345678910111213141516171819202122232425262728293031class MyCNNClassifier(nn.Module): def __init__(self, in_c, n_classes): super().__init__() self.conv_block1 = nn.Sequential( nn.Conv2d(in_c, 32, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(32), nn.ReLU() ) self.conv_block2 = nn.Sequential( nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU() ) self.decoder = nn.Sequential( nn.Linear(32 * 28 * 28, 1024), nn.Sigmoid(), nn.Linear(1024, n_classes) ) def forward(self, x): x = self.conv_block1(x) x = self.conv_block2(x) x = x.view(x.size(0), -1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, 10)print(model) 1234567891011121314151617MyCNNClassifier( (conv_block1): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (conv_block2): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) )) 你觉得好多了？您是否注意到conv_block1和conv_block2看起来几乎相同？ 我们可以创建一个重新生成nn.Sequential的函数来简化代码！ 123456def conv_block(in_f, out_f, *args, **kwargs): return nn.Sequential( nn.Conv2d(in_f, out_f, *args, **kwargs), nn.BatchNorm2d(out_f), nn.ReLU() ) 然后我们可以在我们的模块中调用此函数 123456789101112131415161718192021222324class MyCNNClassifier(nn.Module): def __init__(self, in_c, n_classes): super().__init__() self.conv_block1 = conv_block(in_c, 32, kernel_size=3, padding=1) self.conv_block2 = conv_block(32, 64, kernel_size=3, padding=1) self.decoder = nn.Sequential( nn.Linear(32 * 28 * 28, 1024), nn.Sigmoid(), nn.Linear(1024, n_classes) ) def forward(self, x): x = self.conv_block1(x) x = self.conv_block2(x) x = x.view(x.size(0), -1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, 10)print(model) 1234567891011121314151617MyCNNClassifier( (conv_block1): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (conv_block2): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) )) Even cleaner! Still conv_block1 and conv_block2 are almost the same! We can merge them using nn.Sequential 123456789101112131415161718192021222324class MyCNNClassifier(nn.Module): def __init__(self, in_c, n_classes): super().__init__() self.encoder = nn.Sequential( conv_block(in_c, 32, kernel_size=3, padding=1), conv_block(32, 64, kernel_size=3, padding=1) ) self.decoder = nn.Sequential( nn.Linear(32 * 28 * 28, 1024), nn.Sigmoid(), nn.Linear(1024, n_classes) ) def forward(self, x): x = self.encoder(x) x = x.view(x.size(0), -1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, 10)print(model) 12345678910111213141516171819MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) )) self.encoder now holds booth conv_block. We have decoupled logic for our model and make it easier to read and reuse. Our conv_block function can be imported and used in another model. Dynamic Sequential: create multiple layers at onceWhat if we can to add a new layers in self.encoder, hardcoded them is not convinient: 1234567self.encoder = nn.Sequential( conv_block(in_c, 32, kernel_size=3, padding=1), conv_block(32, 64, kernel_size=3, padding=1), conv_block(64, 128, kernel_size=3, padding=1), conv_block(128, 256, kernel_size=3, padding=1), ) Would it be nice if we can define the sizes as an array and automatically create all the layers without writing each one of them? Fortunately we can create an array and pass it to Sequential 1234567891011121314151617181920212223242526class MyCNNClassifier(nn.Module): def __init__(self, in_c, n_classes): super().__init__() self.enc_sizes = [in_c, 32, 64] conv_blocks = [conv_block(in_f, out_f, kernel_size=3, padding=1) for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])] self.encoder = nn.Sequential(*conv_blocks) self.decoder = nn.Sequential( nn.Linear(32 * 28 * 28, 1024), nn.Sigmoid(), nn.Linear(1024, n_classes) ) def forward(self, x): x = self.encoder(x) x = x.view(x.size(0), -1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, 10)print(model) 12345678910111213141516171819MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) )) Let’s break it down. We created an array self.enc_sizes that holds the sizes of our encoder. Then we create an array conv_blocks by iterating the sizes. Since we have to give booth a in size and an outsize for each layer we ziped the size’array with itself by shifting it by one. Just to be clear, take a look at the following example: 1234sizes = [1, 32, 64]for in_f,out_f in zip(sizes, sizes[1:]): print(in_f,out_f) 121 3232 64 Then, since Sequential does not accept a list, we decompose it by using the * operator. Tada! Now if we just want to add a size, we can easily add a new number to the list. It is a common practice to make the size a parameter. 1234567891011121314151617181920212223242526class MyCNNClassifier(nn.Module): def __init__(self, in_c, enc_sizes, n_classes): super().__init__() self.enc_sizes = [in_c, *enc_sizes] conv_blokcs = [conv_block(in_f, out_f, kernel_size=3, padding=1) for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])] self.encoder = nn.Sequential(*conv_blokcs) self.decoder = nn.Sequential( nn.Linear(32 * 28 * 28, 1024), nn.Sigmoid(), nn.Linear(1024, n_classes) ) def forward(self, x): x = self.encoder(x) x = x.view(x.size(0), -1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, [32,64, 128], 10)print(model) 123456789101112131415161718192021222324MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) )) We can do the same for the decoder part 12345678910111213141516171819202122232425262728293031323334def dec_block(in_f, out_f): return nn.Sequential( nn.Linear(in_f, out_f), nn.Sigmoid() )class MyCNNClassifier(nn.Module): def __init__(self, in_c, enc_sizes, dec_sizes, n_classes): super().__init__() self.enc_sizes = [in_c, *enc_sizes] self.dec_sizes = [32 * 28 * 28, *dec_sizes] conv_blokcs = [conv_block(in_f, out_f, kernel_size=3, padding=1) for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])] self.encoder = nn.Sequential(*conv_blokcs) dec_blocks = [dec_block(in_f, out_f) for in_f, out_f in zip(self.dec_sizes, self.dec_sizes[1:])] self.decoder = nn.Sequential(*dec_blocks) self.last = nn.Linear(self.dec_sizes[-1], n_classes) def forward(self, x): x = self.encoder(x) x = x.view(x.size(0), -1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, [32,64], [1024, 512], 10)print(model) 12345678910111213141516171819202122232425MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() ) (1): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True)) We followed the same pattern, we create a new block for the decoding part, linear + sigmoid, and we pass an array with the sizes. We had to add a self.last since we do not want to activate the output Now, we can even break down our model in two! Encoder + Decoder 1234567891011121314151617181920212223242526272829303132333435363738class MyEncoder(nn.Module): def __init__(self, enc_sizes): super().__init__() self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1) for in_f, out_f in zip(enc_sizes, enc_sizes[1:])]) def forward(self, x): return self.conv_blokcs(x) class MyDecoder(nn.Module): def __init__(self, dec_sizes, n_classes): super().__init__() self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) for in_f, out_f in zip(dec_sizes, dec_sizes[1:])]) self.last = nn.Linear(dec_sizes[-1], n_classes) def forward(self, x): return self.dec_blocks() class MyCNNClassifier(nn.Module): def __init__(self, in_c, enc_sizes, dec_sizes, n_classes): super().__init__() self.enc_sizes = [in_c, *enc_sizes] self.dec_sizes = [32 * 28 * 28, *dec_sizes] self.encoder = MyEncoder(self.enc_sizes) self.decoder = MyDecoder(dec_sizes, n_classes) def forward(self, x): x = self.encoder(x) x = x.flatten(1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, [32,64], [1024, 512], 10)print(model) 12345678910111213141516171819202122232425MyCNNClassifier( (encoder): MyEncoder( (conv_blokcs): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) ) (decoder): MyDecoder( (dec_blocks): Sequential( (0): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) )) Be aware that MyEncoder and MyDecoder could also be functions that returns a nn.Sequential. I prefer to use the first pattern for models and the second for building blocks. By diving our module into submodules it is easier to share the code, debug it and test it. ModuleList : when we need to iterateModuleList allows you to store Module as a list. It can be useful when you need to iterate through layer and store/use some information, like in U-net. The main difference between Sequential is that ModuleList have not a forward method so the inner layers are not connected. Assuming we need each output of each layer in the decoder, we can store it by: 1234567891011class MyModule(nn.Module): def __init__(self, sizes): super().__init__() self.layers = nn.ModuleList([nn.Linear(in_f, out_f) for in_f, out_f in zip(sizes, sizes[1:])]) self.trace = [] def forward(self,x): for layer in self.layers: x = layer(x) self.trace.append(x) return x 123456model = MyModule([1, 16, 32])import torchmodel(torch.rand((4,1)))[print(trace.shape) for trace in model.trace] 12345678torch.Size([4, 16])torch.Size([4, 32])[None, None] ModuleDict: when we need to chooseWhat if we want to switch to LearkyRelu in our conv_block? We can use ModuleDict to create a dictionary of Module and dynamically switch Module when we want 123456789101112def conv_block(in_f, out_f, activation='relu', *args, **kwargs): activations = nn.ModuleDict([ ['lrelu', nn.LeakyReLU()], ['relu', nn.ReLU()] ]) return nn.Sequential( nn.Conv2d(in_f, out_f, *args, **kwargs), nn.BatchNorm2d(out_f), activations[activation] ) 12print(conv_block(1, 32,'lrelu', kernel_size=3, padding=1))print(conv_block(1, 32,'relu', kernel_size=3, padding=1)) 12345678910Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01))Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU()) Final implementationLet’s wrap it up everything! 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def conv_block(in_f, out_f, activation='relu', *args, **kwargs): activations = nn.ModuleDict([ ['lrelu', nn.LeakyReLU()], ['relu', nn.ReLU()] ]) return nn.Sequential( nn.Conv2d(in_f, out_f, *args, **kwargs), nn.BatchNorm2d(out_f), activations[activation] )def dec_block(in_f, out_f): return nn.Sequential( nn.Linear(in_f, out_f), nn.Sigmoid() )class MyEncoder(nn.Module): def __init__(self, enc_sizes, *args, **kwargs): super().__init__() self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) for in_f, out_f in zip(enc_sizes, enc_sizes[1:])]) def forward(self, x): return self.conv_blokcs(x) class MyDecoder(nn.Module): def __init__(self, dec_sizes, n_classes): super().__init__() self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) for in_f, out_f in zip(dec_sizes, dec_sizes[1:])]) self.last = nn.Linear(dec_sizes[-1], n_classes) def forward(self, x): return self.dec_blocks() class MyCNNClassifier(nn.Module): def __init__(self, in_c, enc_sizes, dec_sizes, n_classes, activation='relu'): super().__init__() self.enc_sizes = [in_c, *enc_sizes] self.dec_sizes = [32 * 28 * 28, *dec_sizes] self.encoder = MyEncoder(self.enc_sizes, activation=activation) self.decoder = MyDecoder(dec_sizes, n_classes) def forward(self, x): x = self.encoder(x) x = x.flatten(1) # flat x = self.decoder(x) return x 12model = MyCNNClassifier(1, [32,64], [1024, 512], 10, activation='lrelu')print(model) 12345678910111213141516171819202122232425MyCNNClassifier( (encoder): MyEncoder( (conv_blokcs): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) ) ) (decoder): MyDecoder( (dec_blocks): Sequential( (0): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) )) ConclusionSo, in summary. Use Module when you have a big block compose of multiple smaller blocks Use Sequential when you want to create a small block from layers Use ModuleList when you need to iterate through some layers or building blocks and do something Use ModuleDict when you need to parametise some blocks of your model, for example an activation function That’s all folks! Thank you for reading","categories":[],"tags":[]},{"title":"使用新的fastai库实现世界一流的结果","slug":"yuque/使用新的fastai库实现世界一流的结果","date":"2018-12-20T06:20:52.000Z","updated":"2019-04-04T13:49:28.594Z","comments":true,"path":"2018/12/20/yuque/使用新的fastai库实现世界一流的结果/","link":"","permalink":"http://zhos.me/2018/12/20/yuque/使用新的fastai库实现世界一流的结果/","excerpt":"","text":"链接 动机我目前正在做一个fast.ai Live MOOC，名为《Practical Deep learning for Coders》，将于2019年1月在fast.ai网站上公开发布。 以下代码基于该课程的第1课。 我将使用位于Pytorch 1.0顶部的fastai V1库。 fastai库提供了许多有用的功能，使我们能够快速，轻松地构建神经网络并训练我们的模型。 我正在撰写此博客，作为在数据集上试验课程示例的一部分，该数据集在结构和复杂性上有所不同，并表明使用fastai库是多么容易。在下面的例子中，您将看到在PlantVintage数据集上进行转移学习并获得世界级结果是多么荒谬。 PlantVintage数据包含植物叶片的图像，其中包括通常在作物上发现的38种疾病类别，是来自斯坦福大学背景图像开放数据集的一个背景类别–DAGS。 我从这个Github Repo上给出的链接下载了数据。 我对这个例子特别感兴趣，因为在我写这篇博客的时候，我为一个帮助农民发展业务的组织工作，提供产品和技术解决方案，以便更好地管理农场。 让我们开始吧！PS：这个博客也在我的GitHub个人资料中作为jupyter笔记本发布。 导入快速AI库让我们导入fastai库并将我们的batch_size参数定义为64.通常，图像数据库很大，所以我们需要使用批量将这些图像提供给GPU，批量大小64意味着我们将一次提供64个图像来更新深度参数 学习模式。 如果由于GPU RAM较小而导致内存不足，则可以将批量大小减小到32或16。123from fastai import *from fastai.vision import *bs =64 看数据我们处理问题时首先要做的是查看数据。 在我们弄清楚如何解决问题之前，我们总是需要很好地理解问题是什么以及数据是什么样的。 查看数据意味着了解数据目录的结构，标签是什么以及一些示例图像是什么样的。 我们的数据已经在train和validation文件夹中分割，在每个子目录中，我们的文件夹名称代表该子文件夹中存在的所有图像的类名。 幸运的是，fastai库有一个方便的功能，ImageDataBunch.from_folder自动从文件夹名称中获取标签名称。 fastai库提供了很棒的文档来浏览它们的库函数，并提供有关如何使用它们的实例。 加载数据后，我们还可以使用.normalize到ImageNet参数来规范化数据。123456## Declaring path of datasetpath_img = Path('/home/jupyter/fastai_v3_experimentation/data/PlantVillage/')## Loading data data = ImageDataBunch.from_folder(path=path_img, train='train', valid='valid', ds_tfms=get_transforms(),size=224, bs=bs, check_ext=False)## Normalizing data based on Image net parametersdata.normalize(imagenet_stats) 要查看随机的图像样本，我们可以使用.show_batch（）函数ImageDataBunch类。 正如我们在下面所看到的，我们有一些不同作物上的疾病病例加上来自DAGS数据集的一些背景噪声图像，这些图像将作为噪声。1data.show_batch(rows=3, figsize=(10,8)) 让我们打印数据库中存在的所有数据类。 总的来说，我们在动机部分中提到了39个课程中的图像。12print(data.classes)len(data.classes),data.c 使用预先训练的模型转移学习：ResNet 50现在我们将开始训练我们的模型。 我们将使用卷积神经网络骨干ResNet 50和具有单个隐藏层的完全连接头作为分类器。 如果您想了解所有架构细节，也可以阅读ResNet论文。 要创建转移学习模型，我们需要使用Learner类中的函数create_cnn，并从模型类中提供预先训练的模型。12## To create a ResNET 50 with pretrained weightslearn = create_cnn(data, models.resnet50, metrics=error_rate) 由create_cnn函数创建的ResNet50模型具有冻结的初始层，我们将学习最后完全连接的层的权重。1learn.fit_one_cycle(5) 正如我们在上面看到的那样，只使用默认设置运行五个周期，我们对这个细粒度分类任务的准确度在验证数据集上约为99.64％。 让我们保存模型，因为我们稍后会对其进行微调。 如果你想知道这个结果有多好，它已经超过了这个Github Page的96.53％的浅层学习（仅培训最后一层）基准。1learn.save('plant_vintage_stage1') FastAI库还提供了更快地探索结果的功能，并查找我们的模型是否正在学习应该学习的内容。 我们将首先看到模型最混淆的类别。 我们将尝试使用ClassificationInterpretation类来查看模型预测的是否合理。12interp = ClassificationInterpretation.from_learner(learn)interp.plot_top_losses(4, figsize=(20,25)) 在这种情况下，该模型在从玉米植株上的灰色叶斑病和番茄叶片中的早/晚疫病检测北叶枯病方面变得混乱，其在视觉上看起来非常相似。 这是我们的分类器正常工作的指示器。 此外，当我们绘制混淆矩阵时，我们可以看到大多数事物都被正确分类，并且它几乎是一个接近完美的模型。1interp.plot_confusion_matrix(figsize=(20,20), dpi=60) 所以到目前为止，我们只训练了最后的分类层，但是如果我们想要优化早期的层也会如此。 在迁移学习中，应谨慎调整初始图层，学习率应保持在较低水平。 FastAI库提供了一个功能，可以查看要训练的理想学习速率，让我们绘制它。 lr_find函数以多学习速率运行数据子集的模型，以确定哪种学习速率最佳。12learn.lr_find()learn.recorder.plot() 看起来我们应该保持低于10e-4的学习率。 对于网络中的不同层，我们可以使用切片函数来对数分布10e-6到10e-4之间的学习速率。 保持初始层的最低学习速率，并为后续层增加它。 让我们解冻所有层，以便我们可以使用unfreeze()函数训练整个模型。12learn.unfreeze()learn.fit_one_cycle(2, max_lr=slice(1e-7,1e-5)) 正如我们通过训练所有层次所看到的，我们将准确度提高到99.7％，这与使用Inception-v3模型的Github基准测试中的99.76％相当。 结论Fast.ai是Jeremy Howard和他的团队的一项出色的倡议，我相信fastai库可以通过使构建深度学习模型变得非常简单，真正实现将深度学习民主化的动机。我希望你喜欢阅读，并随意使用我的代码为你的目的尝试。 此外，如果对代码或博客文章有任何反馈，请随时联系LinkedIn或发送电子邮件至aayushmnit@gmail.com。PS：如果你喜欢这些内容，请留下评论或鼓掌，并希望我更频繁地写这样的博客。","categories":[],"tags":[]},{"title":"XGBoost不是黑魔法","slug":"yuque/XGBoost不是黑魔法","date":"2018-12-20T05:48:34.000Z","updated":"2019-04-04T13:49:28.594Z","comments":true,"path":"2018/12/20/yuque/XGBoost不是黑魔法/","link":"","permalink":"http://zhos.me/2018/12/20/yuque/XGBoost不是黑魔法/","excerpt":"","text":"链接 而不是对缺失值进行估算并不总是正确的选择 现在很容易在数据科学任务中获得不错的结果：只需要对流程有一个大致的了解，Python的基本知识和10分钟的时间来实例化XGBoost并适应模型。好的，如果这是你的第一次，那么你可能会花几分钟通过pip收集所需的包，但就是这样。这种方法的唯一问题是它运作得很好🤷🏻♂️：几年前我在大学竞赛中排名前5位，只是通过一些基本的特征工程将数据集提供给XGBoost，表现优于团队非常复杂的架构和数据管道。 XGBoost最酷的特征之一就是它如何处理缺失值：决定每个样本，这是最好的方法来判断它们。对于我在过去几个月中遇到的许多项目和数据集，此功能非常有用;为了更加值得以我的名义撰写的数据科学家的头衔，我决定深入挖掘，花几个小时阅读原始论文，试图了解XGBoost究竟是什么以及它如何处理它以某种神奇的方式缺少价值。 从决策树到XGBoost决策树可能是机器学习中最简单的算法：树的每个节点都是对特征的测试，每个分支代表测试的结果; leaves包含模型的输出，无论是离散标签还是实数。 决策树可能被描述为一个功能：函数f根据从根到叶子的路径，根据树结构T分配m大小的样本x所遵循的权重w。现在想象一下，不只有一棵决策树而且还有K; 最终产生的输出不再是与叶子相关的权重，而是与每棵树产生的叶子相关的权重之和。 这些结构不是固定的，并且与网络结构不变的经典梯度下降框架中发生的不同，并且在每个步骤更新权重时，在每次迭代时添加新函数（树）以改善模型的性能。 为了避免过度拟合和/或非常复杂的结构，误差由两部分组成：第一部分对在第k次迭代中获得的模型的优度进行评分，第二部分在相关权重的大小中惩罚复杂性。 叶子和发达的树木的深度和结构。然后使用二阶梯度统计简化该目标函数，并且 - 不输入太多细节 - 可以直接用于以封闭形式计算与固定树结构相关联的最佳叶子权重。 权重可以直接与错误相关联，因此与所使用的固定结构的优点相关联（3）。训练XGBoost是一个迭代过程，它在每个步骤计算第k个树的最佳可能分割，该第k个树枚举路径中该点仍然可用的所有可能结构。 所有可能的分裂的详尽列举非常适合本文的范围，但在实践中是不可行的，并且它被一个近似版本所取代，该版本不会尝试所有可能的分裂，而是根据百分位数列举相关的分裂。 每个功能分布。 XGBoost和缺失值：魔术发生的地方一旦树结构被训练，就不难考虑测试集中是否存在缺失值：它足以将默认方向附加到每个决策节点。如果缺少样本的特征并且决策节点在该特征上分裂，则路径采用分支的默认方向并且路径继续。但是为每个分支分配默认方向更复杂，这可能是本文中最有趣的部分。已经解释的拆分查找算法可以稍微调整一下，不仅返回每一步的最佳拆分，而且还返回分配给新插入的决策节点的默认方向。给定一个特征集I，枚举所有可能的分割，但是现在相应的丢失不会被计算一次而是两次，每个默认方向一次丢失该特征的缺失值。两者中最好的是根据特征m的值j进行分割时分配的最佳默认方向。最佳分割仍然是最大化计算分数的分割，但现在我们已经为其附加了默认方向。这种算法被称为稀疏感知的分裂发现，它是XGBoost背后的许多魔力所在。最后不要太复杂。稀疏性感知方法仅保证在已经遍历的分裂的情况下平均采用默认方向导致最佳可能结果，并不保证已经遍历的分裂（可能通过采用默认方向来解决）是最好的考虑整个样本。如果样本中缺失值的百分比增加，则内置策略的性能可能会恶化很多。 好的，默认方向是最佳选择，只要它到达当前位置，但考虑到当前样本的所有特征，无法保证当前位置是最佳情况。 克服此限制意味着处理同时考虑其所有特征的样本，并直接处理同一实现中可能同时存在多个缺失值。 改变缺失值并改善表现为了击败XGBoost内置策略，我们必须同时考虑样本的所有功能，并以某种方式处理可能存在的缺失值。 这种方法的一个很好的例子是K-Nearest Neighbors（KNN），它具有ad-hoc距离度量以正确处理缺失值。 一般而言，KNN是众所周知的算法，其将K（例如，3,10,50，……）最接近的样本检索到所考虑的样本。 它可以用于对看不见的输入进行分类或者用于估算缺失值，在分配给目标值的情况下，考虑K个最近邻居的均值或中值。 这种方法需要距离度量（或相应地，相似性度量）来实际对训练集中的所有样本进行排序并检索最相似的K.要超越XGBoost内置默认策略，我们需要两件事： 考虑缺失值的距离指标（感谢AirBnb的这篇文章的灵感） 1234567def dist_with_miss(a,b,l=0.0): if(len(a) != len(b)): return np.inf ls = l * np.ones(len(a)) msk = ~ (np.isnan(a) | np.isnan(b)) res = np.sum((np.abs(a-b)[msk]))+np.sum((ls[~msk])) return res 规范化数据集以获得有意义的距离，获得了不同域之间特征之间的差异（XGBoost并不严格要求，但KNN估算需要它！）。 使用K个最接近样本的所述特征的中值来估算特征的缺失值，并且在非特定情况下，在K个检索的邻居中不发现至少一个非缺失值，整个列的中值 用来。 实验结果我使用scikit-learn中免费提供的三个众所周知的数据集（两个分类和一个回归）进行了一些测试。 通过k-fold交叉验证比较三种不同的插补策略，测量了性能： XGBoost算法中内置的默认值 一个简单的列中位插值 一个KNN，如前一段所述 对于KNN案例，我已经绘制了针对考虑的缺失值百分比获得的最佳性能，其中k与（要考虑的邻居的数量）和λ（当至少一个特征缺失时要添加到距离的常数） 两个样本）。使用稀疏性感知KNN来估算缺失值与其他两种方法的表现一致。 差异的程度当然是数据集依赖的。 作为第一个天真的结论：数据集的质量越低，更好的插补策略的影响越大。 如图2所示，内置策略最终具有接近于平凡的列中值插值的性能。看看k和λ如何影响最终结果以及如何引入惩罚因素不仅仅是纸上谈兵，这是非常有趣的。 距离度量不仅丢弃缺失值而且还为每一个增加权重对于用该方法获得的性能是至关重要的，即使其值与缺失值的增加百分比不直接相关。测试表明，根据经验，缺失值的数量越多，为更好的插补而考虑的邻居数量越多。 再次，一个非常直观的结论。","categories":[],"tags":[]},{"title":"认识和实现：批量标准化","slug":"yuque/认识和实现：批量标准化","date":"2018-12-19T14:43:17.000Z","updated":"2019-04-04T13:49:28.594Z","comments":true,"path":"2018/12/19/yuque/认识和实现：批量标准化/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/认识和实现：批量标准化/","excerpt":"","text":"链接在本文中，我将回顾Ioffe和Svegedy的批量规范化的有用性。 我还将在Keras中实现批量标准化，并在训练性能方面取得实质性进展。 批量归一化的直观解释 训练中的问题问题1：随着网络训练，早期层的权重发生变化，因此后期层的输入变化很大。 每层必须根据每批输入的不同分布重新调整其权重。 这减缓了模型训练。 如果我们可以在分布中使层输入更相似，那么网络可以专注于学习类之间的差异。不同批次分布的另一个影响是消失的梯度。 消失的梯度问题是一个大问题，特别是对于S形激活函数。 如果g（x）表示sigmoid激活函数，则为| x | 增加，g’（x）趋于零。问题2.当输入分布变化时，神经元输出也会变化。 这导致神经元输出偶尔波动到S形函数的可饱和区域。 在那里，神经元既不能更新自己的权重，也不能将梯度传递回先前的层。 我们如何保持神经元输出不变为可饱和区域？ 如果我们可以将神经元输出限制在零附近的区域，我们可以确保每个层在反向传播期间都会传回一个实质的梯度。 这将导致更快的训练时间和更准确的结果。 批量标准作为解决方案。批量标准化减轻了不同层输入的影响。 通过归一化神经元的输出，激活函数将仅接收接近零的输入。 这确保了非消失的梯度，解决了第二个问题。批量归一化将层输出转换为单位高斯分布。 当这些输出通过激活功能馈送时，层激活也将变得更加正常分布。由于一层的输出是下一层的输入，因此层输入现在具有明显较小的批次间差异。 通过减少层输入的变化分布，我们解决了第一个问题。 数学解释通过批量归一化，我们为每个激活函数寻找以零为中心的单位方差分布。 在训练期间，我们采用激活输入x并将其减去批次均值μ以实现零中心分布。接下来，我们取x并将其除以批量方差和一个小数，以防止除以零σ+ε。 这可确保所有激活输入分布都具有单位差异。最后，我们将x hat进行线性转换以缩放并移动批量标准化的输出。 尽管在反向传播期间网络发生了变化，但仍能确保保持这种正常化效果。在测试模型时，我们不使用批处理均值或方差，因为这会破坏模型。 （提示：单个观察的平均值和方差是多少？）相反，我们计算训练群体的移动平均值和方差估计值。 这些估计值是培训期间计算的所有批次平均值和方差的平均值。 批量标准化的好处批量标准化的好处如下。1.有助于防止具有可饱和非线性（sigmoid，tanh等）的网络中的消失梯度通过批量标准化，我们确保任何激活函数的输入不会变为可饱和区域。 批量归一化将这些输入的分布转换为单位高斯（零中心和单位方差）。2.规范模型也许。 Ioffe和Svegeddy提出了这一主张，但没有就此问题进行广泛撰写。 也许这是归一化层输入的结果？3.允许更高的学习率通过在训练期间防止梯度消失的问题，我们可以设置更高的学习率。 批量标准化还降低了对参数标度的依赖性。 较大的学习速率可以增加层参数的规模，这导致梯度在反向传播期间回传时放大。 我需要阅读更多关于此的内容。 在Keras实施引入1234567891011121314151617181920import tensorflow as tfimport numpy as npimport osimport kerasfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_imgimport matplotlib.pyplot as pltimport matplotlib.image as mpimgfrom keras.models import Model, Sequentialfrom keras.layers import Inputfrom keras.callbacks import ModelCheckpoint, EarlyStoppingfrom keras.layers import BatchNormalizationfrom keras.layers import GlobalAveragePooling2Dfrom keras.layers import Activationfrom keras.layers import Conv2D, MaxPooling2D, Densefrom keras.layers import MaxPooling2D, Dropout, Flattenimport time 数据加载和预处理在这笔记本中，我们使用Cifar 100数据集，因为它具有相当的挑战性，并且不会永远用于训练。 唯一的预处理是零中心和图像变化发生器。123456789101112131415161718192021222324252627from keras.datasets import cifar100from keras.utils import np_utils(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=&apos;fine&apos;)#scale and regularize the datasetx_train = (x_train-np.mean(x_train))x_test = (x_test - x_test.mean())x_train = x_train.astype(&apos;float32&apos;)x_test = x_test.astype(&apos;float32&apos;)#onehot encode the target classesy_train = np_utils.to_categorical(y_train)y_test = np_utils.to_categorical(y_test)train_datagen = ImageDataGenerator( shear_range=0.2, zoom_range=0.2, horizontal_flip=True)train_datagen.fit(x_train)train_generator = train_datagen.flow(x_train, y = y_train, batch_size=80,) 在Keras中构建模型我们的架构将包括堆叠的3x3卷积，然后是最大池化和dropout。 每个网络中有5个卷积块。 最后一层是一个完全连接的层，有100个节点和softmax激活。我们将构建4个不同的卷积网络，每个网络都具有sigmoid或ReLU激活以及批量标准化或不标准化。 我们将比较每个网络的验证损失。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108def conv_block_first(model, bn=True, activation=&quot;sigmoid&quot;): &quot;&quot;&quot; The first convolutional block in each architecture. Only separate so we can specify the input shape. &quot;&quot;&quot; #First Stacked Convolution model.add(Conv2D(60,3, padding = &quot;same&quot;, input_shape = x_train.shape[1:])) if bn: model.add(BatchNormalization()) model.add(Activation(activation)) #Second Stacked Convolution model.add(Conv2D(60,3, padding = &quot;same&quot;)) if bn: model.add(BatchNormalization()) model.add(Activation(activation)) model.add(MaxPooling2D()) model.add(Dropout(0.15)) return modeldef conv_block(model, bn=True, activation = &quot;sigmoid&quot;): &quot;&quot;&quot; Generic convolutional block with 2 stacked 3x3 convolutions, max pooling, dropout, and an optional Batch Normalization. &quot;&quot;&quot; model.add(Conv2D(60,3, padding = &quot;same&quot;)) if bn: model.add(BatchNormalization()) model.add(Activation(activation)) model.add(Conv2D(60,3, padding = &quot;same&quot;)) if bn: model.add(BatchNormalization()) model.add(Activation(activation)) model.add(MaxPooling2D()) model.add(Dropout(0.15)) return modeldef conv_block_final(model, bn=True, activation = &quot;sigmoid&quot;): &quot;&quot;&quot; I bumped up the number of filters in the final block. I made this separate so that I might be able to integrate Global Average Pooling later on. &quot;&quot;&quot; model.add(Conv2D(100,3, padding = &quot;same&quot;)) if bn: model.add(BatchNormalization()) model.add(Activation(activation)) model.add(Conv2D(100,3, padding = &quot;same&quot;)) if bn: model.add(BatchNormalization()) model.add(Activation(activation)) model.add(Flatten()) return modeldef fn_block(model): &quot;&quot;&quot; I&apos;m not going for a very deep fully connected block, mainly so I can save on memory. &quot;&quot;&quot; model.add(Dense(100, activation = &quot;softmax&quot;)) return modeldef build_model(blocks=3, bn=True, activation = &quot;sigmoid&quot;): &quot;&quot;&quot; Builds a sequential network based on the specified parameters. blocks: number of convolutional blocks in the network, must be greater than 2. bn: whether to include batch normalization or not. activation: activation function to use throughout the network. &quot;&quot;&quot; model = Sequential() model = conv_block_first(model, bn=bn, activation=activation) for block in range(1,blocks-1): model = conv_block(model, bn=bn, activation = activation) model = conv_block_final(model, bn=bn, activation=activation) model = fn_block(model) return modeldef compile_model(model, optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;]): &quot;&quot;&quot; Compiles a neural network. model: the network to be compiled. optimizer: the optimizer to use. loss: the loss to use. metrics: a list of keras metrics. &quot;&quot;&quot; model.compile(optimizer = optimizer, loss = loss, metrics = metrics) return model#COMPILING THE 4 MODELSsigmoid_without_bn = build_model(blocks = 5, bn=False, activation = &quot;sigmoid&quot;)sigmoid_without_bn = compile_model(sigmoid_without_bn)sigmoid_with_bn = build_model(blocks = 5, bn=True, activation = &quot;sigmoid&quot;)sigmoid_with_bn = compile_model(sigmoid_with_bn)relu_without_bn = build_model(blocks = 5, bn=False, activation = &quot;relu&quot;)relu_without_bn = compile_model(relu_without_bn)relu_with_bn = build_model(blocks = 5, bn=True, activation = &quot;relu&quot;)relu_with_bn = compile_model(relu_with_bn) 模特训练没有批量标准化的Sigmoid训练陷入困境。 有100个课程，这个模型从未达到比随机猜测更好的性能（10％准确度）。1234567history1 = sigmoid_without_bn.fit_generator( train_generator, steps_per_epoch=2000, epochs=20, verbose=0, validation_data=(x_test, y_test), callbacks = [model_checkpoint]) 具有批量标准化的Sigmoid与没有批量标准化不同，该模型在训练期间开始实施。 这可能是批量标准化减轻消失梯度的结果。1234567history2 = sigmoid_with_bn.fit_generator( train_generator, steps_per_epoch=2000, verbose=0, epochs=20, validation_data=(x_test, y_test), callbacks = [model_checkpoint]) 没有批量标准化的ReLU在没有批量规范的情况下实施ReLU导致一些初始收益，然后收敛到非最优的局部最小值。1234567history3 = relu_without_bn.fit_generator( train_generator, steps_per_epoch=2000, epochs=20, verbose=0, validation_data=(x_test, y_test), callbacks = [model_checkpoint]) 具有批量标准化的ReLU与sigmoid模型一样，批量标准化提高了该网络的训练能力。1234567history4 = relu_with_bn.fit_generator( train_generator, steps_per_epoch=2000, verbose=0, epochs=20, validation_data=(x_test, y_test), callbacks = [model_checkpoint]) 比较架构我们在这里清楚地看到批量标准化的好处。 没有批量标准化的ReLU和S形模型都无法保持训练性能提升。 这可能是渐变消失的结果。 具有批量标准化的体系结构训练得更快，并且比没有批量标准化的体系结构表现更好。 Conclusion结论批量标准化减少了训练时间并提高了神经网络的稳定性。 此效果适用于sigmoid和ReLU激活功能。 原帖可以在我的网站上找到，代码可以在我的GitHub上找到。 Resources Original paper by Ioffe and Szegedy. here. Insert a batch normalization before or after nonlinearities? Usage explanation For an explanation of the math and implementation in TensorFlow. Pitfalls of Batch Norm Also this post How to use Batch Normalization with TensorFlow and tf.keras Further readingBelow are some more recent research papers that extend Ioffe and Svegedy’s work.[1] How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)[2] Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models[3] Layer Normalization[4] Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks[5] Group Normalization","categories":[],"tags":[]},{"title":"不要在卷积网络中使用Dropout","slug":"yuque/不要在卷积网络中使用Dropout","date":"2018-12-19T14:04:13.000Z","updated":"2019-04-04T13:49:28.594Z","comments":true,"path":"2018/12/19/yuque/不要在卷积网络中使用Dropout/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/不要在卷积网络中使用Dropout/","excerpt":"","text":"链接 如果您想知道如何使用dropout，这里有您要的答案。 我注意到有很多资源可以用来学习深度学习的内容和原因。 不幸的是，当需要制作模型时，他们很少有资源来解释何时以及如何。我正在为试图实施深度学习的其他数据科学家撰写本文。 因此，您不必像我一样通过研究文章和Reddit讨论。在本文中，您将了解为什么dropout在卷积体系结构中不再受欢迎。 DROPOUT如果你正在读这篇文章，我认为你已经了解了什么是dropout，以及它在正则化神经网络方面的作用。 如果您想要复习，请阅读Amar Budhiraja的这篇文章。 通常，当我们的网络存在过度拟合的风险时，我们只需要实现正规化。 如果网络太大，如果您训练时间过长，或者您没有足够的数据，则会发生这种情况。如果在卷积网络末端有完全连接的层，则实现dropout很容易。 使用Keras1keras.layers.Dropout(rate, noise_shape=None, seed=None) 以0.5的dropout率开始并将其调低，直到性能最大化。 （资源） 例如123model=keras.models.Sequential()model.add(keras.layers.Dense(150, activation=&quot;relu&quot;))model.add(keras.layers.Dropout(0.5)) 请注意，这仅适用于您的convnet的完全连接区域。 对于所有其他地区，您不应使用dropout。相反，您应该在卷积之间插入批量标准化。 这将使您的模型正常化，并使您的模型在训练期间更加稳定。 批正则化批标准化是规范卷积网络的另一种方法。除了正则化效应之外，批量归一化还可以使您的卷积网络在训练期间抵抗消失的梯度。 这可以减少训练时间并获得更好的性能。批量标准化可以消除消失的梯度 Keras实施要在Keras中实现批量标准化，请使用以下命令：1keras.layers.BatchNormalization() 构建具有批量规范化的卷积体系结构时： 在卷积和激活层之间插入批量标准化层。 （资源） 您可以在此功能中调整一些超参数，并使用它们。 您也可以在激活功能之后插入批量标准化，但根据我的经验，这两种方法都具有相似的性能。 例如123model.add(Conv2D(60,3, padding = &quot;same&quot;))model.add(BatchNormalization())model.add(Activation(&quot;relu&quot;)) 批量标准化取代了dropout。即使您不需要担心过度拟合，实现批量标准化也有很多好处。 正因为如此，它的正规化效应，批量归一化已经在很大程度上取代了现代卷积体系结构中的dropout。“我们提出了一种使用批量规范化网络构建，训练和执行推理的算法。 由此产生的网络可以通过饱和非线性进行训练，更能容忍增加的训练率，并且通常不需要Dropout进行正规化。“ - Ioffe and Svegedy 2015至于为什么dropout在最近的应用中失宠，主要有两个原因。首先，在对卷积层进行正则化时，dropout通常不太有效。原因？ 由于卷积层具有很少的参数，因此它们开始时需要较少的正则化。 此外，由于在特征图中编码的空间关系，激活可以变得高度相关。 这使得dropout无效。（资源）其次，擅长正规化的dropout现在已经过时了。像VGG16这样在网络末端包含完全连接的层的大型模型。 对于这样的模型，过度拟合是通过在完全连接的层之间包括dropout来解决的。不幸的是，最近的架构远离了这个完全连接块。通过用全局平均池替换密集层，现代的网络可以减少模型大小，同时提高性能。我将在未来再写一篇文章，详细说明如何在卷积网络中实现全球平均汇集。 在此之前，我建议阅读ResNet论文，以了解GAP的好处。 一个实验我创建了一个实验来测试批量标准化是否会减少在卷积之间插入时的泛化错误。 （链接）我构建了5个相同的卷积体系结构，并在卷积之间插入了dropout，批量规范或任何（控制）。通过在Cifar100数据集上训练每个模型，我获得了以下结果。批量标准化模型的良好表现说明应在卷积之间使用批量标准化。此外，不应在卷基层之间放置dropout，因为dropout的模型往往比控制模型表现更差。有关更多信息，请查看我的GitHub上的完整文章。小贴士如果你想知道是否应该在卷积网络中实现dropout，现在你知道了。 仅在完全连接的层上使用dropout，并在卷积之间实现批量标准化。如果您想了解有关批量标准化的更多信息，请阅读：https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b","categories":[],"tags":[]},{"title":"理解二进制交叉熵、对数损失函数:一种可视化解释","slug":"yuque/理解二进制交叉熵、对数损失函数:一种可视化解释","date":"2018-12-19T08:27:20.000Z","updated":"2019-04-04T13:49:28.594Z","comments":true,"path":"2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/理解二进制交叉熵、对数损失函数:一种可视化解释/","excerpt":"","text":"链接 介绍如果您正在训练二进制分类器，则可能使用二进制交叉熵/对数损失作为损失函数。你有没有想过使用这种损失函数究竟是什么意思？ 问题是，考虑到今天的库和框架的易用性，很容易忽略所使用的损失函数的真正含义。动机我正在寻找一篇博文，以一种视觉上清晰简洁的方式解释二进制交叉熵/对数损失背后的概念，所以我可以在Data Science Retreat向我的学生展示它。 由于我找不到任何符合我目的的东西，我自己负责编写任务:-)。一个简单的分类问题让我们从10个随机点开始：1x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6] 这是我们唯一的特征：x。现在，让我们为我们的点分配一些颜色：红色和绿色。 这些是我们的标签。因此，我们的分类问题非常简单：鉴于我们的特征x，我们需要预测其标签：红色或绿色。 由于这是一个二元分类，我们也可以将这个问题描述为：“是点绿色”，或者更好的是，“点是绿色的概率是多少”？ 理想情况下，绿点的概率为1.0（绿色），而红点的概率为0.0（绿色）。 在此设置中，绿点属于正类（YES，它们是绿色），而红点属于负类（NO，它们不是绿色）。如果我们拟合模型来执行此分类，它将预测每个点的绿色概率。 根据我们对点的颜色的了解，我们如何评估预测概率的优劣（或差）？ 这是损失功能的全部目的！ 它应该为错误预测返回高值，为良好预测返回低值。 对于像我们的例子那样的二进制分类，典型的损失函数是二进制交叉熵/对数损失函数。 损失函数：二进制交叉熵/对数损失函数如果你观察这个损失函数，这就是你会发现的：Binary Cross-Entropy / Log Loss其中y是标签（绿点为1，红点为0），p(y)是所有N点的点为绿色的预测概率。 阅读这个公式，它告诉你，对于每个绿点（y = 1），它将log（p（y））添加到损失中，即它是绿色的对数概率。 相反，它为每个红点（y = 0）添加log（1-p（y）），即，它为红色的对数概率。 不一定很难，当然也不是那么直观…… 此外，熵与这一切有什么关系？ 为什么我们首先记录概率？ 这些是有效的问题，我希望在下面的“给我看数学”部分回答它们。 计算损失-可视化方法首先，让我们根据他们的类别（正面或负面）分割点数，如下图所示：现在，让我们训练一个Logistic回归来对我们的点进行分类。 拟合回归是一个S形曲线，表示任何给定x点的绿色概率。 它看起来像这样：那么，对于属于正类（绿色）的所有点，我们的分类器给出的预测概率是多少？ 这些是S形曲线下的绿色条形，在与这些点对应的x坐标处。好的，到目前为止，真好！ 负面阶级的观点怎么样？ 请记住，S形曲线下的绿色条表示给定点为绿色的概率。 那么，给定点是红色的概率是多少？ 红色条在S形曲线上方，当然:-)总而言之，我们最终会得到这样的结论：条形表示与每个点的相应真实类别相关联的预测概率！ 好的，我们有预测的概率……通过计算二进制交叉熵/对数损失来评估它们的时间！ 这些概率就是我们所需要的，所以，让我们摆脱x轴并将条带彼此相邻：好吧，吊杆不再有意义了，所以让我们重新定位它们： 既然我们正试图计算损失，我们需要惩罚不好的预测，对吧？ 如果与真实类相关的概率为1.0，我们需要将其损失为零。 相反，如果这个概率很低，比如0.01，我们需要它的损失是巨大的！事实证明，对于这个目的，取概率的（负）对数就足够了（因为0.0和1.0之间的值的对数是负的，我们采用负对数来获得损失的正值）。实际上，我们使用日志的原因来自交叉熵的定义，请查看下面的“给我看数学”部分了解更多详情。下图给出了一个清晰的图片 - 如果真实类的预测概率接近零，则损失呈指数增长：很公平！ 让我们采用概率的（负）对数 - 这些是每个点的相应损失。最后，我们计算所有这些损失的平均值。瞧！ 我们已经成功计算了这个玩具示例的二进制交叉熵/对数损失。 它是0.3329！告诉我代码如果你想仔细检查我们找到的值，只需运行下面的代码并亲自看看:-)1234567891011121314151617from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import log_lossimport numpy as npx = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])logr = LogisticRegression(solver=&apos;lbfgs&apos;)logr.fit(x.reshape(-1, 1), y)y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()loss = log_loss(y, y_pred)print(&apos;x = &#123;&#125;&apos;.format(x))print(&apos;y = &#123;&#125;&apos;.format(y))print(&apos;p(y) = &#123;&#125;&apos;.format(np.round(y_pred, 2)))print(&apos;Log Loss / Cross Entropy = &#123;:.4f&#125;&apos;.format(loss)) 告诉我数学（真的吗？！）除了笑话之外，这篇文章并不打算在数学上倾向于……但是对于你们这些人，我的读者，想要了解熵的作用，所有这些中的对数，我们在这里:-)如果你想深入了解信息理论，包括所有这些概念 - 熵，交叉熵等等 - 检查Chris Olah的帖子，它非常详细！ 分布让我们从分配点开始吧。 由于y表示我们的点的类（我们有3个红点和7个绿点），这就是它的分布，我们称之为q（y），如下所示： 熵熵是与给定分布q（y）相关的不确定性的度量。如果我们所有的积分都是绿色怎么办 那个分布的不确定性是什么？ ZERO，对吗？ 毕竟，对于一个点的颜色毫无疑问：它总是绿色的！ 所以，熵是零！另一方面，如果我们确切地知道一半的点是绿色而另一半点是红色的呢？ 那是最糟糕的情况，对吗？ 猜测点的颜色绝对没有优势：它完全是随机的！ 对于这种情况，熵由下面的公式给出（我们有两个类别（颜色） - 红色或绿色 - 因此，2）：对于中间的其他情况，我们可以使用下面的公式计算分布的熵，如q（y），其中C是类的数量：因此，如果我们知道随机变量的真实分布，我们就可以计算其熵。 但是，如果是这样的话，为什么首先要费心去训练分类器呢？ 毕竟，我们知道真正的分布……但是，如果我们不这样做呢？ 我们可以尝试用其他一些分布近似真实分布，比如p（y）吗？ 我们当然可以！:-) 交叉熵让我们假设我们的观点遵循其他分布p（y）。 但我们知道它们实际上来自真实（未知）分布q（y），对吧？如果我们像这样计算熵，我们实际上是在计算两个分布之间的交叉熵：如果我们奇迹般地将p（y）与q（y）完美匹配，则交叉熵和熵的计算值也将匹配。由于这可能永远不会发生，因此交叉熵将具有比在真实分布上计算的熵更大的BIGGER值。事实证明，交叉熵和熵之间的这个区别有一个名字…… Kullback-Leibler发散Kullback-Leibler Divergence，简称“KL Divergence”，衡量两种发行版之间的差异：这意味着，p（y）越接近q（y），发散越低，因此交叉熵越低。所以，我们需要找到一个好的p（y）来使用……但是，这是我们的分类器应该做的，不是吗？！ 确实如此！ 它寻找最好的p（y），这是最小化交叉熵的那个。 损失函数在训练期间，分类器使用其训练集中的N个点中的每一个来计算交叉熵损失，有效地拟合分布p（y）！ 由于每个点的概率是1 / N，因此交叉熵由下式给出：还记得上面的图6到10吗？ 我们需要在与每个点的真实类相关联的概率之上计算交叉熵。 这意味着使用绿色条形作为正类（y = 1）中的点，使用红色条形作为负类中的点（y = 0），或者数学上说：最后一步是计算两个类中所有点的平均值，正面和负面：最后，通过一些操作，我们可以采用相同的公式，从正面或负面的类中获取任何一点：瞧！ 我们回到二进制交叉熵/日志丢失的原始公式:-) 最后的想法我真的希望这篇文章能够对一个经常被认为理所当然的概念，即二元交叉熵作为损失函数的概念有所启发。 此外，我也希望它能够向您展示机器学习和信息理论如何联系在一起。","categories":[],"tags":[]},{"title":"关于目标检测，所有你应该知道的深度学习模型","slug":"yuque/关于目标检测，所有你应该知道的深度学习模型","date":"2018-12-19T07:22:45.000Z","updated":"2019-04-04T13:49:28.598Z","comments":true,"path":"2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/关于目标检测，所有你应该知道的深度学习模型/","excerpt":"","text":"链接 Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO这篇是简介一些用来辨识影像中物体的AI 模型。 在前面有提到，透过CNN模型，你可以输入一张图片，得到该图片属于哪种类别的结果，这过程我们把他称作分类(Classification)。 但在真实世界的应用情境通常要从一张图片中辨识所有出现的物体， 并且标示出位置来(标出位置称之为Object Localization)。你一定在网路上看过类似底下的影片，这段影片可以看出中国闭路摄影机(CCTV)发展的概况，不只是可以框出影像中每个物件，辨别物件种类，侦测出移动物体的动量，甚至是人脸辨识，实现楚门世界的恶梦。要做到这就需要靠深度学习中的Object Detection 演算法，这也是最近几年来深度学习最蓬勃发展的一块领域。https://youtu.be/aE1kA0Jy0Xg基本的想法是，既然CNN 对于物体的分类又快又好，那我们可不可以拿CNN 来扫描并辨识图片中的任何物体？答案当然是 — 可以。 最简单的作法就是用Sliding Windows 的概念，也就是用一个固定大小的框框，逐一的扫过整张图片，每次框出来的图像丢到CNN 中去判断类别。由于物体的大小是不可预知的，所以还要用不同大小的框框去侦测。但是Sliding Window 是非常暴力的作法，对单一影像我们需要扫描非常多次，每扫一次都需要算一次CNN，这将会耗费大量的运算资源，而且速度慢，根本无法拿来应用！ 所以后来就有人提出了R-CNN (Regions with CNN) R-CNN与其用Sliding Window的方式扫过一轮，R-CNN的作法是预先筛选出约2000个可能的区域，再将这2000区域个别去作分类，所以他的演算法流程如下： 产生一群约2000 个可能的区域(Region Proposals) 经由一个预先训练好的CNN 模型如AlexNet 撷取特征，将结果储存起来。 然后再以SVM (Support Vector Machine) 分类器来区分是否为物体或者背景。 最后经由一个线性回归模型来校正bounding box 位置。 Selective SearchR-CNN用来筛选Region Proposals的方法称之为Selective Search，而Selective Search又是基于Felzenszwal于2004年发表的论文Graph Base Image Segmentation。 图像经由Graph Base Image Segmentation 可以切出数个Segment 来，如下图：而Selective Search 的作法是将Segment 的结果先各自画出bounding box，然后以一个回圈，每次合并相似度最高的两个box，直到整张图合并成单一个box 为止，在这过程中的所有box 便是selective search 出来的region proposals。Selective Search 的演算法如下：取自Selective Search 论文。先以Graph base image segmentation 取得一些区域，计算每个区域间的相似度，每次合并相似度最高的两个区域，直到整张图片成为单一区域为止。但是R-CNN 存在一些问题，速度仍然不够快： R-CNN 一开始必须先产生约2000 个区域，每个区域都要丢进CNN 中去撷取特征，所以需要跑过至少2000 次的CNN R-CNN 的model 是分开成三部份，分别是用来取出特征的CNN，分类的SVM，以及优化bounding box 的线性回归。所以R-CNN 不容易作训练。 所以R-CNN的其中一个作者Ross Girshick (RBG大神)在2015年又提出了一个改良版本，并称之为Fast R-CNN。 Fast R-CNNFast R-CNN的想法很简单，在R-CNN中，2000多个区域都要个别去运算CNN，这些区域很多都是重叠的，也就是说这些重叠区域的CNN很多都是重复算的。所以Fast R-CNN的原则就是全部只算一次CNN就好，CNN撷取出来的特征可以让这2000多个区域共用！ Fast R-CNN 采用的作法就是RoIPooling (Region of Interest Pooling)。Fast RCNN 一样要预选Region proposals，但是只做一次CNN。在跑完Convolution layers 的最后一层时，会得到一个HxW 的feature map，同时也要将region proposals 对应到HxW 上，然后在feature map 上取各自region 的MaxPooling，每个region 会得到一个相同大小的矩阵(例如2x2)。from https://blog.deepsense.ai/region-of-interest-pooling-explained/然后各自连接上FC 网路，以及softmax 去作分类。在分类的同时也作bounding box 的线性回归运算。 Fast RCNN 的优点是： 只需要作一次CNN，有效解省运算时间 使用单一网络，简化训练过程 Faster R-CNN不管是R-CNN还是Fast R-CNN都还是要先透过selective search预选region proposals，这是一个缓慢的步骤。在2015年时，Microsoft的Shaoqing Ren , Kaiming He , Ross Girshick ,以及Jian Sun提出了Faster R-CNN，一个更快的R-CNN。Faster R-CNN 的想法也很直觉，与其预先筛选region proposals，到不如从CNN 的feature map 上选出region proposals。 Region Proposal NetworkRPN (Region Proposal Network) 也是一个Convolution Network，Input 是之前CNN 输出的feature map，输出是一个bounding box 以及该bounding box 包含一个物体的机率。 RPN 在feature map 上取sliding window，每个sliding window 的中心点称之为anchor point，然后将事先准备好的k 个不同尺寸比例的box 以同一个anchor point 去计算可能包含物体的机率(score) ，取机率最高的box。这k 个box 称之为anchor box。所以每个anchor point 会得到2k 个score，以及4k 个座标位置(box 的左上座标，以及长宽，所以是4 个数值)。在Faster R-CNN 论文里，预设是取3 种不同大小搭配3 种不同长宽比的anchor box，所以k 为3x3 = 9 。 经由RPN 之后，我们便可以得到一些最有可能的bounding box，虽然这些bounding box 不见得精确，但是透过类似于Fast RCNN 的RoIPooling， 一样可以很快的对每个region 分类，并找到最精确的bounding box 座标。 Mask R-CNN前述几个方法都是在找到物体外围的bounding box，bounding box基本上都是方形，另外一篇有趣的论文是Facebook AI researcher Kaiming He所提出的Mask R-CNN，透过Mask R-CNN不只是找到bounding box，可以做到接近pixel level的遮罩(图像分割Image segmentation)。要了解Mask R-CNN 如何取遮罩，要先看一下FCN (Fully Convolutional Network) FCN (Fully Convolutional Network) for Image Segmentation有别于CNN 网络最后是连上一个全连接(Fully Connected)的网络，FCN (Fully Convolutional Network)最后接上的是一个卷积层。一般的CNN 只能接受固定大小的Input，但是FCN 则能接受任何大小的Input，例如W x H 。图上方一般的CNN 网络，只能接受大小固定的输入，得到单一维度的输出，分别代表每个类别的机率。图下则是FCN 网路，最后两层由卷积取代，输出为hxwx 1000，代表每个pixel 种类的机率，可以视为一个heapmap。 在CNN 的过程中会一直作downsampling，所以FCN 最后的输出可能为H/32 x W/32，实际上得到的会是一个像heapmap 的结果。但是由于这过程是downsampling，所以Segment 的结果是比较粗糙，为了让Segment 的效果更好，要再做upsampling，来补足像素。upsamping 的作法是取前面几层的结果来作差补运算。FCN 的结果会跟前面几层的输出作差补运算Mask R-CNN是建构于Faster R-CNN之上，如果是透过RoIPooling取得Region proposals之后，针对每个region会再跑FCN取得遮罩分割，但是由于RoIPooling在做Max pooling时，会使用最近插值法( Nearest Neighbor Interpolation )取得数值，所以出来的遮罩会有偏移现象，再加上pooling下来的结果，会让region的尺寸出现非整数的情况，然后取整数的结果就是没办法做到Pixel层级的遮罩。所以Mask R-CNN改采用双线性插值法( Bilinear Interpolation )来改善RoIPooling，称之为RoIAlign，RoIAlign会让遮罩位置更准确。Mask RCNN 架构，将原有的RoIPooling 改成 RoIAlign。Fast R-CNN 的RoIPool。将一个7x5 的Anchor box 取2x2 的MaxPool，由于使用最近插值法，会有偏差。 RoIAlign的作法是使用双线性插值法( Bilinear Interpolation )，减少mis-alignment的问题。 YOLO: You Only Look OnceYOLO有个很讨喜的名字，取自You Only Live Once，但用在Object detection上则为You only look once，意思是说YOLO模型的特性只需要对图片作一次CNN便能够判断里面的物体类别跟位置，大大提升辨识速度。 R-CNN 的概念是先提出几个可能包含物体的Region proposal，再针对每个region 使用CNN 作分类，最后再以regression 修正bounding box 位置，速度慢且不好训练。YOLO 的好处是单一网路设计，判断的结果会包含bounding box 位置，以及每个bounding box 所属类别及概率。整个网路设计是end-to-end 的，容易训练，而且速度快。 YOLO 速度快，在Titan X GPU 上可以达到每秒45 祯的速度，简化版的YOLO 甚至可以达到150 fps 的速度。这意味着YOLO 已经可以对影像作即时运算了。准确度(mAP) 也狠甩其他深度学习模型好几条街。看看底下YOLO2 的demo 视频，这侦测速度会吓到吃手手了https://youtu.be/VOC3huqHrss 有别于R-CNN 都是先提region 再做判断，看的范围比较小，容易将背景的background patch 看成物体。YOLO 在训练跟侦测时都是一次看整张图片，背景错误侦测率(background error, 抑或false positive) 都只有Fast R-CNN 的一半。 YOLO 的泛用性也比R-CNN 或者DPM 方式来得好很多，在新的domain 使用YOLO 依旧可以很稳定。YOLO 的概念是将一张图片切割成S x S 个方格，每个方格以自己为中心点各自去判断B 个bounding boxes 中包含物体的confidence score 跟种类。confidence score = Pr(Object) * IOU (ground truth)如果该bounding box 不包含任何物体(Pr(Object) = 0)，confidence score 便为零，而IOU 则为bounding box 与ground truth 的交集面积，交集面积越大，分数越高。每个方格预测的结果包含5 个数值，x 、y 、w 、 h 跟confidence，x 与y 是bounding box 的中间点，w 与h 是bounding box 的宽跟高。 S = 7，B = 2，PASCAL VOC label 20 种种类，所以tensor 为S x S x (5 * B + C) = 7 x 7 x 30YOLO 的网路设计包含了24 个卷积层，跟2 层的FC 网络。另外一个版本的YOLO Fast 则只有9 个卷积层，不过最后的输出都是7x7x30 的tensor。 YOLO 的缺点 由于YOLO 对于每个方格提两个bounding box 去作侦测，所以不容易去区分两个相邻且中心点又非常接近的物体 只有两种bounding box，所以遇到长宽比不常见的物体的检测率较差 YOLO 与其他模型的比较 YOLO2YOLO2建构于YOLO之上，但是有更好的准确度，更快速的判断速度，能够判断更多的物件种类(多达9000种)，所以是更好(Better)、更快(Faster)、更强大(Stronger)！YOLO2 在准确度上比YOLO 好，且追上什至超越其他模型像是Faster R-CNN 或者SSD 等，速度还是别人的2–10 倍以上。 YOLO2 采用了许多改善方式，例如batch normalization、anchor box 等，使用了这些改良方式让YOLO2 不管在辨识速度还是准确率上都有了提升，此外对于不同图档大小也有很好的相容性，提供了在速度与准确性上很好的平衡，所以也很适合运用在一些便宜的GPU 或者CPU 上，依旧提供水准以上的速度与准确率。 结语物体辨识(Object detection)的进展飞快，为了整理这篇大概也看了七八篇论文，还有很多都还没涵盖到的，例如SSD ( Single Shot Mulitbox Detector )。如果想更了解AI在Computer Vision最近几年的发展，也可以参考这篇搜文 A Year in Computer vision，内容涵盖了Classification、Object detection、Object tracking、Segmentation、Style transfer、Action recognition、3D object、Human post recognition等等，看完会大致知道在Computer Vision中有哪些AI所做的努力，以及各自的进展。 Google的Tensorflow也有提供Object detection API，透过使用API ，不用理解这些模型的实作也能快速实作出速度不错涵盖率又广的object detection。","categories":[],"tags":[]},{"title":"目标检测和定位算法的演变","slug":"yuque/目标检测和定位算法的演变","date":"2018-12-19T06:21:01.000Z","updated":"2019-04-04T13:49:28.598Z","comments":true,"path":"2018/12/19/yuque/目标检测和定位算法的演变/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/目标检测和定位算法的演变/","excerpt":"","text":"链接通过对基本概念的直观解释，了解对象检测和本地化的最新进展。目标检测是计算机视觉领域中非常迅速成熟的领域之一。 感谢深度学习！ 每年，新的算法/模型都会比以前更好。 事实上，Facebook AI团队上周刚刚发布了最先进的物体检测软件系统之一。 该软件称为Detectron，它包含许多用于物体检测的研究项目，并由Caffe2深度学习框架提供支持。今天，有大量用于目标检测的预训练模型（YOLO，RCNN，Faster RCNN，Mask RCNN，Multibox等）。 因此，只需花费少量精力即可检测视频或图像中的大多数对象。 但我的博客的目标不是谈论这些模型的实现。 相反，我试图以清晰简洁的方式解释基本概念。我最近完成了3周的Andrew Ng的卷积神经网络课程，其中他谈到了目标检测算法。 此博客的大部分内容都受到该课程的启发。编辑：我目前正在进行Fast.ai的尖端深度学习编码课程，由Jeremy Howard教授。 现在，我使用PyTorch和fast.ai库实现了下面讨论的算法。 这是代码的链接。 如果您想了解下面讨论的算法的实现部分，请查看此信息。 该实现已经从fast.ai课程笔记本借用，附有评论和注释。 关于CNN的简介在我解释目标检测算法的工作之前，我想在卷积神经网络（也称为CNN或ConvNets）上写上几笔。 CNN是深度学习时代大多数计算机视觉任务的基本构建模块。我们想要什么？ 我们需要一些查看图像的算法，查看图像中的图案并告知图像中的对象类型。 例如，是猫或狗的形象。什么是电脑的图像？ 只是数字矩阵。 对于例如 见上图1。 左边的图像只是手写数字2的28 * 28像素图像（取自MNIST数据），在Excel电子表格中表示为数字矩阵。我们怎样才能教电脑学会识别图像中的物体？ 通过让计算机学习垂直边缘，水平边缘，圆形以及许多其他人类未知的模式。计算机如何学习模式？卷积！（阅读本文时请看上图）卷积是两个矩阵之间的数学运算，给出第三个矩阵。 我们称之为滤波器或内核（图1中的3x3）的较小矩阵在图像像素矩阵上操作。 根据滤波器矩阵中的数字，输出矩阵可以识别输入图像中存在的特定模式。 在上面的示例中，滤波器是垂直边缘检测器，其学习输入图像中的垂直边缘。 在深度学习的背景下，输入图像及其后续输出从许多这样的滤波器传递。 过滤器中的数字是通过神经网络学习的，模式是自己导出的。为什么卷积有效？ 因为在大多数图像中，对象具有可以通过卷积来利用的相对像素密度（数字的大小）的一致性。我知道CNN上只有几行对于不了解CNN的读者来说是不够的。 但CNN不是本博客的主题，我已经提供了基本介绍，因此读者可能不需要再打开10个链接来先了解CNN，然后再继续。阅读本博客后，如果您仍想了解更多关于CNN的信息，我强烈建议您阅读Adam Geitgey撰写的这篇博客。 计算机视觉任务的分类以图2中的猫狗图像为例，以下是计算机视觉建模算法最常见的任务： 图像分类：这是最常见的计算机视觉问题，其中算法查看图像并对其中的对象进行分类。 图像分类具有广泛的应用，从社交网络上的面部检测到医学中的癌症检测。 通常使用卷积神经网络（CNN）对这些问题进行建模。 目标分类和定位：假设我们不仅想知道图像中是否有猫，而且猫的确切位置。 对象定位算法不仅标记对象的类，还在图像中的对象位置周围绘制边界框。 多个目标检测和定位：如果图像中有多个物体（如上图中的3只狗和2只猫），我们想要检测它们，该怎么办？ 这将是一个对象检测和定位问题。 众所周知的应用是在自动驾驶汽车中，该算法不仅需要检测汽车，还需要检测车架中的行人，摩托车，树木和其他物体。 这些问题需要利用从图像分类和对象定位中学到的思想或概念。 现在回到计算机视觉任务。 在深度学习的背景下，上述3种任务之间的基本算法差异就是选择相关的输入和输出。 让我用信息图解释这一行。 1 图像分类 图2中的信息图显示了用于图像分类的典型CNN的外观。通过n滤波器（图3中n = 4）卷积一些高度，宽度和通道深度（上面的情况下为940,550,3）的输入图像[如果你仍然感到困惑究竟卷积是什么意思，请检查 这个链接来理解深度神经网络中的卷积]。 卷积的输出用非线性变换处理，通常是Max Pool和RELU。 Convolution，Max Pool和RELU的上述3个操作被执行多次。 最终层的输出被发送到Softmax层，该层转换0和1之间的数字，从而给出图像特定类的概率。 我们将损失降至最低，以便使最后一层的预测接近实际值。 2.物体分类和定位 现在，为了使我们的模型绘制对象的边界框，我们只需更改前一算法的输出标签，以使我们的模型学习对象类以及对象在图像中的位置。 我们在输出层添加4个数字，包括对象的质心位置和图像中边界框的宽度和高度的比例。简单吧？ 只需添加一堆输出单元即可吐出您想要识别的不同位置的x,y坐标。 对于我们拥有的所有图像中的特定对象，这些不同的位置或界标将是一致的。 对于例如对于汽车而言,高度将小于宽度，并且与图像中的其他点相比，质心将具有一些特定的像素密度。隐含相同的逻辑，如果图像中有多个对象并且我们想要对所有这些对象进行分类和定位，您认为会发生什么变化？ 我建议你暂时停下来思考，你可能会自己得到答案。 3.目标检测和定位 为了检测图像中的各种对象，我们可以直接使用我们从目前为止学到的东西。 不同之处在于我们希望我们的算法能够对图像中的所有对象进行分类和定位，而不仅仅是一个。 因此，我们的想法是，只需将图像裁剪成多个图像，然后为所有裁剪的图像运行CNN以检测对象。算法的工作方式如下： 制作一个比实际图像尺寸小得多的窗口。 裁剪它并将其传递给ConvNet（CNN）并让ConvNet进行预测。 继续滑动窗口并将裁剪后的图像传递到ConvNet。 在使用此窗口大小裁剪图像的所有部分后，再次重复所有步骤以获得更大的窗口大小。 再次将裁剪后的图像传递到ConvNet并让它进行预测。 最后，您将拥有一组裁剪区域，这些区域将包含一些对象，以及对象的类和边界框 该解决方案被称为具有滑动窗口的物体检测。 这是非常基本的解决方案，有以下几点需要注意：A.计算成本高：裁剪多个图像并通过ConvNet传递它将在计算上非常昂贵。解决方案：有一个简单的hack来提高滑动窗口方法的计算能力。 它是用1x1卷积层替换ConvNet中的完全连接层，对于给定的窗口大小，只传递一次输入图像。 因此，在实际实现中，我们不会一次传递一个裁剪后的图像，但我们会立即传递完整的图像。 B.不准确的边界框：我们在整个图像上滑动方形窗口，也许对象是矩形的，或者没有一个正方形与对象的实际大小完全匹配。 虽然该算法具有查找和定位图像中多个对象的能力，但是边界框的准确性仍然很差。我已经谈到了对象检测问题的最基本的解决方案。 但它有许多警告，并不是最准确的，并且实施起来计算成本很高。 那么，我们如何才能使我们的算法更好更快？好的解决方案？YOLO 事实证明，我们有YOLO（你只看一次），它比滑动窗口算法更准确，更快。 它仅基于我们已经知道的算法顶部的微小调整。 我们的想法是将图像分成多个网格。 然后我们改变数据的标签，以便我们为每个网格单元实现定位和分类算法。 让我再向您解释一下这个信息图。 YOLO简单的步骤： 将图像分成多个网格。 为了说明，我在上图中绘制了4x4网格，但YOLO的实际实现具有不同的网格数量。 （7x7用于在PASCAL VOC数据集上培训YOLO） 标记训练数据，如上图所示。 如果C是我们数据中唯一对象的数量，S S是我们分割图像的网格数，那么我们的输出向量将是长度S S （C + 5）。 对于例如 在上面的例子中，我们的目标向量是4 4 （3 + 5），因为我们将图像划分为4 4网格，并训练3个独特的对象：汽车，光和行人。 制作一个具有损失函数的深度卷积神经网络作为输出激活和标签矢量之间的误差。 基本上，该模型通过ConvNet仅在输入图像的一个前向通道中预测所有网格的输出。 请记住，对象存在于网格单元格（P.Object）中的标签由该网格中对象的质心的存在决定。 重要的是不允许在不同的网格中多次对一个对象进行计数。 YOLO的注意事项及其解决方案： A.无法检测同一网格中的多个对象。 通过选择较小的网格大小可以解决此问题。 但即使选择较小的网格大小，在对象彼此非常接近的情况下，算法仍然会失败，如鸟群的图像。解决方案：Anchor boxes。 除了每个网格单元具有5 + C标签（其中C是不同对象的数量）之外，Anchor boxes的想法是每个网格单元具有（5 + C）* A标签，其中A是必需的Anchor boxes。 如果将一个对象分配给一个网格中的一个Anchor boxes，则可以将另一个对象分配给同一网格的另一个Anchor boxes。 B.可多次检测一个物体的可能性。 解决方案：非最大抑制。 非最大抑制消除了非常接近高概率边界框的低概率边界框。 结论：截至今天，有多种版本的预训练YOLO模型可用于不同的深度学习框架，包括Tensorflow。 最新的YOLO论文是：“YOLO9000：更好，更快，更强”。 该模型接受了9000个课程的培训。 还有一些基于选择性区域提案的区域CNN（R-CNN）算法，我没有讨论过。 由Facebook AI开发的Detectron软件系统也实现了R-CNN，Masked R-CNN的变体。 参考文献: You Only Look Once: Unified, Real-Time Object Detectionhttps://arxiv.org/pdf/1506.02640.pdf YOLO9000: Better, Faster, Strongerhttps://arxiv.org/pdf/1612.08242.pdf Convolutional Neural Networks by Andrew Ng (deeplearning.ai)https://www.coursera.org/learn/convolutional-neural-networks","categories":[],"tags":[]},{"title":"评论：U-Net（生物医学图像分割）","slug":"yuque/评论：U-Net（生物医学图像分割）","date":"2018-12-19T05:38:12.000Z","updated":"2019-04-04T13:49:28.598Z","comments":true,"path":"2018/12/19/yuque/评论：U-Net（生物医学图像分割）/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/评论：U-Net（生物医学图像分割）/","excerpt":"","text":"链接在这个U-Net评论中。 U-Net是生物医学图像分割领域着名的全卷积网络（FCN）之一，它在2015年MICCAI上发表，在我写这篇故事时引用了3000多篇。 （SH Tsang @ Medium）在生物医学图像标注领域，我们总是需要获得相关知识的专家来标注每个图像。 而且他们也会花费大量时间来标注。 如果标注过程变为自动，则可以实现较少的人力和较低的成本。 或者它可以作为减少人为错误的辅助角色。 您可能会问：“阅读有关生物医学图像分割的内容是否过于狭窄？”但是，我们可能会学习它的技术，并将其应用于不同的行业。 比如说，在施工/制造/制造过程中的质量控制/自动检查/自动机器人，或者我们可能想到的任何其他东西。 这些活动涉及定量诊断。 如果我们可以自动化，则可以以更高的精度节省成本。 在本文中，他们分割/标注电子显微镜（EM）图像。 他们还对网络进行了一些修改，以便在2015 ISBI中一下子对X射线图像进行分段/标注。 什么是涵盖的 A. EM图像分割 U-Net网络架构 重叠平铺策略 数据增强的弹性变形 触摸物体的分离 结果 B.牙科X射线图像分割 U-Net的一些修改 结果 A.1 U-net网络架构U-net架构如上所示。 它由收缩路径和扩展路径组成。 收缩路径连续两次3×3转换和2×2最大合并完成。 这有助于提取更多高级功能，但也会减少功能图的大小。 扩张路径 连续执行2×2 Up-conv和2×3×3 Conv以恢复分割图的大小。 但是，上述过程虽然增加了“什么”，但减少了“哪里”。 这意味着，我们可以获得高级功能，但我们也会丢失本地化信息。 因此，在每个up-conv之后，我们还具有相同级别的特征映射（灰色箭头）的串联。 这有助于将收缩路径的本地化信息提供给扩展路径。 最后，1×1转换将特征映射大小从64映射到2，因为输出特征映射只有2个类，单元格和膜。 A.2 重叠平铺策略由于使用了无填充卷积，因此输出大小小于输入大小。 不是在网络之前缩小尺寸而是在网络之后进行上采样，而是使用重叠切片策略。 由此，如上图所示，逐个部分地预测整个图像。 使用蓝色区域预测图像中的黄色区域。 在图像边界处，通过镜像外推图像。 A.3数据增强的弹性变形由于训练集只能由专家标注，因此训练集很小。 为了增加训练集的大小，通过随机变形输入图像和输出分割图来完成数据增加。 A.4接触目标的分离由于触摸物体彼此紧密放置，它们很容易被网络合并，将它们分开，重量图被应用于网络的输出。为了如上计算权重图，d1(x)是到位置x处最近的单元边界的距离，d2(x)是到第二个最近的单元边界的距离。 因此，在边界处，重量如图中高得多。因此，交叉熵函数在权重图处在每个位置处罚。 它有助于迫使网络学习触摸细胞之间的小分离边界。 A.5. 结果 A.5.1. ISBI 2012 Challenge 变形误差：一种惩罚拓扑分歧的分段指标。 Rand Error：两个聚类或分段之间相似性的度量。 像素错误：标准像素错误。 训练时间：10小时 测试速度：每张图像约1秒 A.5.2. PhC-U373 and DIC-HeLa 数据集 B.1. U-Net的一些修改这次，使用4×4 Up-conv，并使用1×1 Conv将特征映射从64映射到7，因为每个位置的输出有7个类。在重叠平铺策略中，使用零填充而不是在图像边界处镜像。 因为镜像对牙齿没有任何意义。使用softmax损失的低分辨率特征图还有额外的损耗层，以指导深层直接学习分段类。 B.2. 结果I have also reviewed CUMedVision1 and CUMedVision2. Please feel free to visit if interested. References [2015] [MICCAI]U-Net: Convolutional Networks for Biomedical Image Segmentation [2015] [ISBI]Dental X-ray Image Segmentation using a U-shaped Deep Convolutional Network My Related Reviews[CUMedVision1] [CUMedVision2] [FCN] [DeconvNet]","categories":[],"tags":[]},{"title":"去除正则化会避免模型过拟合吗？","slug":"yuque/去除正则化会避免模型过拟合吗？","date":"2018-12-19T04:28:16.000Z","updated":"2019-04-04T13:49:28.598Z","comments":true,"path":"2018/12/19/yuque/去除正则化会避免模型过拟合吗？/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/去除正则化会避免模型过拟合吗？/","excerpt":"","text":"medium有人可能会争辩说，最好是过拟合你的模型，然后对其进行逆向工程而不是相反。在这个项目中，我们可以看到将Dropout正则化实现到神经网络后的准确性和验证损失的差异。 我们将使用PyTorch库从头开始构建一个顺序神经网络，以便在fashion-MNIST数据集中对10个不同的类进行分类。 这个数据集是28x28灰度图像的衣服。 我们将深入研究dropout的方法，并证明它是否能防止过度拟合。该项目的灵感来自： 1Facebook Udacity PyTorch Challenge. 首先，我们将创建一个没有正则化实现的神经网络，我们的假设是我们可以推断，随着时间的推移，我们的模型在验证集中表现不佳，因为我们用训练集训练我们的模型越多， 通过对测试数据的特定特征进行分类则越好，从而创建不良的泛化模型去推理。 让我们导入Fashion-MNIST数据集让我们使用torchvision下载数据集，通常我们将20％的数据集分开用于验证集。 但在这种情况下，我们将直接从torchvision下载数据集。12345678910import torchfrom torchvision import datasets, transformsimport helpertransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])traindataset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True,train=True,transform=transform)trainloader = torch.utils.data.DataLoader(dataset=traindataset, batch_size=64, shuffle=True)testdataset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True,train=False,transform=transform)testloader = torch.utils.data.DataLoader(dataset=testdataset, batch_size=64, shuffle=True) 我们需要导入torchvision来下载数据集和转换。 然后我们使用变换库将图像转换为张量并进行标准化。 通常批量训练和验证集以提高训练速度并且改组数据也会增加训练和测试数据的学习差异。定义神经网络这个模型将有2个隐藏层，输入层将有784个单元，并且在最终层将有10个输出，因为我们有10个不同的类进行分类。 我们将使用交叉熵损失，因为它具有对数性质，可以将我们的输出归一化到接近零或一。123456789101112131415161718192021from torch import nnfrom torch.functional import Fclass FashionNeuralNetwork(nn.Module): def __init__(self): super().__init__() # Create layers here self.layer_input = nn.Linear(784,256) self.layer_hidden_one = nn.Linear(256,128) self.layer_hidden_two = nn.Linear(128,64) self.layer_output = nn.Linear(64,10) def forward(self, x): # Flattened the input to make sure it fits the layer input x = x.view(x.shape[0],-1) # Pass in the input to the layer and do forward propagation x = F.relu(self.layer_input(x)) x = F.relu(self.layer_hidden_one(x)) x = F.relu(self.layer_hidden_two(x)) # Dimension = 1 x = F.log_softmax(self.layer_output(x),dim=1) return x 该神经网络将使用ReLU作为隐藏层的非线性激活函数，并使用log-softmax激活输出和负对数似然函数用于我们的损失函数。 如果我们查看PyTorch库中的交叉熵损失的文档，该标准将nn.LogSoftmax()和nn.NLLLoss()组合在一个单独的类中。 损失可以描述为： 请注意，转发传播结束时的线性函数的dim = 1，这意味着输出结果的每一行的概率总和必须等于1.给单个元素的概率最高图像的概率最高 被归类为相应的类索引。我们必须确保模型的输出形状正确性，12345678910# Instantiate the modelmodel = FashionNeuralNetwork()# Get the images and labels from the test loaderimages, labels = next(iter(testloader))# Get the log probability prediction from our modellog_ps = model(images)# Normalize the probability by taking the exponent of the log-probps = torch.exp(log_ps)# Print out the sizeprint(ps.shape) 确保输出为：1torch.Size([64, 10]) 测量我们模型的准确性由于我们想要一个类的最高概率，我们将使用ps.topk来获得top-k值和top-k索引的元组，例如，如果在4个元素中最高为kth，我们将得到3作为指数。123top_p, top_class = ps.topk(1,dim=1)# Print out the most likely classes for the first 10 examplesprint(top_class[:10,:]) top_class是尺寸为64x1的2D张量，而我们的标签是尺寸为64的1D张量。为了测量标签和模型预测之间的准确度，我们必须确保张量的形状是相同的。123# We have to reshape the labels to 64x1 using the view() methodequals = top_class == labels.view(*top_class.shape)print(equals.shape) 比较张量的输出将是：1torch.Size([64, 1]) 为了计算模型的准确性，我们只需计算模型正确预测的次数。 如果我们的预测与标签相同，则上面的==运算符将逐行检查。 最终结果将是二进制0不相同，1正确预测。 我们可以使用torch.mean计算平均值，但我们需要将equals转换为FloatTensor。123accuracy = torch.mean(equals.type(torch.FloatTensor))# Print the accuracyprint(f&apos;Accuracy: &#123;accuracy.item()*100&#125;%&apos;) 训练我们的模型由于我们希望损失函数与Logarithm Softmax函数的行为相反，我们将使用负对数似然来计算我们的损失。1234567891011121314151617181920212223242526272829303132333435363738394041from torch import optim# Instantiate the modelmodel = FashionNeuralNetwork()# Use Negative Log Likelyhood as our loss functionloss_function = nn.NLLLoss()# Use ADAM optimizer to utilize momentumoptimizer = optim.Adam(model.parameters(), lr=0.003)# Train the model 30 cyclesepochs = 30# Initialize two empty arrays to hold the train and test lossestrain_losses, test_losses = [],[]# Start the trainingfor i in range(epochs): running_loss = 0 # Loop through all of the train set forward and back propagate for images,labels in trainloader: optimizer.zero_grad() log_ps = model(images) loss = loss_function(log_ps, labels) loss.backward() # Backpropagate optimizer.step() running_loss += loss.item() # Initialize test loss and accuracy to be 0 test_loss = 0 accuracy = 0 # Turn off the gradients with torch.no_grad(): # Loop through all of the validation set for images, labels in testloader: log_ps = model(images) ps = torch.exp(log_ps) test_loss += loss_function(log_ps, labels) top_p, top_class = ps.topk(1,dim=1) equals = top_class == labels.view(*top_class.shape) accuracy += torch.mean(equals.type(torch.FloatTensor)) # Append the average losses to the array for plotting train_losses.append(running_loss/len(trainloader)) test_losses.append(test_loss/len(testloader)) 打印我们的模型这证明了我们的假设，即完全说明我们的模型将训练的很好，但不能推广训练数据集之外的图像。 我们可以看到，30个周期的训练损失显著减少，但我们的验证损失在大约36-48％之间波动。 这是过度拟合的标志，这个情况说明，模型学习训练数据集的特定特征和模式，它无法正确分类数据集之外的图像。 这通常很糟糕，因为这意味着如果我们使用推理，模型就无法正确分类。 为了清楚的说明让我们画图看一下：1234567# Plot the graph here%matplotlib inline%config InlineBackend.figure_format = &apos;retina&apos;import matplotlib.pyplot as pltplt.plot(train_losses, label=&apos;Training Loss&apos;)plt.plot(test_losses, label=&apos;Validation Loss&apos;)plt.legend(frameon=True) 过度拟合从上图中我们可以清楚地看到，我们的模型并没有很好地泛华。 这意味着该模型在对训练数据集之外的图像进行分类方面做得不好。 这真的很糟糕，这意味着我们的模型只学习我们的训练数据集的具体内容，它变得如此个别，以至于它只能识别来自训练集的图像。 如果我们从图表中看到，每个周期的训练损失都会显著减少，但是，我们可以看到验证损失却没发生什么变化。 正则这就是正则化的用武之地，其中一种方法是进行L2正则化，也称为early-stopping，这基本上意味着我们将在验证损失最低时停止训练我们的模型。 在这种情况下，我们的验证损失在3-5个时期后达到最佳。 这意味着超过5个周期，我们的模型泛化会变得更糟。但是，还有另一种方法可以解决这个问题。 我们可以为我们的模型进行dropout，以进行更多的泛化。 基本上，我们的模型通过在大型重量上滚雪球并使其他要训练的重量不足而贪婪地行动。 通过具有随机丢失，具有较小权重的节点将有机会在循环期间被训练，从而在结束时给出更一般化的分数。 换句话说，它迫使网络在权重之间共享信息，从而提供更好的泛化能力。注意：在训练期间，我们希望实行dropout，但是，在验证过程中，我们需要我们模型的全部功能，因为那时我们可以完全测量模型对这些图像进行泛化其准确性。 如果我们使用model.eval()模式，我们将停止使用dropout，并且不要忘记在训练期间使用model.train()再次使用它。12345678910111213141516171819202122### Define our new Network with Dropoutsclass FashionNeuralNetworkDropout(nn.Module): def __init__(self): super().__init__() # Create layers here self.layer_input = nn.Linear(784,256) self.layer_hidden_one = nn.Linear(256,128) self.layer_hidden_two = nn.Linear(128,64) self.layer_output = nn.Linear(64,10) # 20% Dropout here self.dropout = nn.Dropout(p=0.2) def forward(self, x): # Flattened the input to make sure it fits the layer input x = x.view(x.shape[0],-1) # Pass in the input to the layer and do forward propagation x = self.dropout(F.relu(self.layer_input(x))) x = self.dropout(F.relu(self.layer_hidden_one(x))) x = self.dropout(F.relu(self.layer_hidden_two(x))) # Dimension = 1 x = F.log_softmax(self.layer_output(x),dim=1) return x 这个神经网络将与第一个模型非常相似，但是，我们将增加20％的丢失。 现在让我们训练这个模型吧！12345678910111213141516171819202122232425262728293031323334353637383940414243444546from torch import optim# Instantiate the modelmodel = FashionNeuralNetworkDropout()# Use Negative Log Likelyhood as our loss functionloss_function = nn.NLLLoss()# Use ADAM optimizer to utilize momentumoptimizer = optim.Adam(model.parameters(), lr=0.003)# Train the model 30 cyclesepochs = 30# Initialize two empty arrays to hold the train and test lossestrain_losses, test_losses = [],[]# Start the trainingfor i in range(epochs): running_loss = 0# Loop through all of the train set forward and back propagate for images,labels in trainloader: optimizer.zero_grad() log_ps = model(images) loss = loss_function(log_ps, labels) loss.backward() # Backpropagate optimizer.step() running_loss += loss.item() # Initialize test loss and accuracy to be 0 test_loss = 0 accuracy = 0 # Turn off the gradients with torch.no_grad(): # Turn on Evaluation mode model.eval() # Loop through all of the validation set for images, labels in testloader: log_ps = model(images) ps = torch.exp(log_ps) test_loss += loss_function(log_ps, labels) top_p, top_class = ps.topk(1,dim=1) equals = top_class == labels.view(*top_class.shape) accuracy += torch.mean(equals.type(torch.FloatTensor)) # Turn on Training mode again model.train() # Append the average losses to the array for plotting train_losses.append(running_loss/len(trainloader)) test_losses.append(test_loss/len(testloader)) 输出结果：这里的目标是使验证损失与我们的训练损失一样低，这意味着我们的模型相当准确。 让我们再次绘制图表，看看正则化后的差异。 即使精度水平仅整体上升0.3％，该模型也没有过度拟合，因为它保持了在训练期间训练的所有节点的平衡。 让我们绘制图表并查看差异：1234567# Plot the graph here%matplotlib inline%config InlineBackend.figure_format = &apos;retina&apos;import matplotlib.pyplot as pltplt.plot(train_losses, label=&apos;Training Loss&apos;)plt.plot(test_losses, label=&apos;Validation Loss&apos;)plt.legend(frameon=True) 推理现在我们的模型可以更好地泛化，让我们用提供模型尝试用训练数据集之外的图像进行预测，并可视化模型的分类。1234567891011121314# Make sure to make our model in the evaluation modemodel.eval()# Get the next image and labelimages, labels = next(iter(testloader))img = images[0]# Convert 2D image to 1D vectorimg = img.view(1, 784)# Calculate the class probabilities (log-softmax) for imgwith torch.no_grad(): output = model.forward(img)# Normalize the outputps = torch.exp(output)# Plot the image and probabilitieshelper.view_classify(img.view(1, 28, 28), ps, version=&apos;Fashion&apos;) 结论这很棒！ 我们可以看到培训损失和验证损失之间的显着平衡。 可以肯定地说，如果我们训练模型进行更多循环并微调我们的超参数，则验证损失将减少。 从上图中我们可以看出，我们的模型随着时间的推移更好地泛化，模型在6-8个时期之后可以获得更好的精度，并且可以肯定地说模型通过实现模型的丢失来防止过度拟合。 Thank you so much for your time, and please check out this repository for the full code!This is my Portfolio and Linked-In profile :)","categories":[],"tags":[]},{"title":"迁移学习：使用Fast.AI库对4种北极犬进行分类","slug":"yuque/迁移学习：使用Fast.AI库对4种北极犬进行分类","date":"2018-12-19T02:16:20.000Z","updated":"2019-04-04T13:49:28.602Z","comments":true,"path":"2018/12/19/yuque/迁移学习：使用Fast.AI库对4种北极犬进行分类/","link":"","permalink":"http://zhos.me/2018/12/19/yuque/迁移学习：使用Fast.AI库对4种北极犬进行分类/","excerpt":"","text":"链接 该项目的灵感来自Adrian Rosebrock，Francisco Ingham和Jeremy Howard。 在本课程中，我们将从Google Images创建自己的数据集。 我将使用FastAI库中的Resnet34的架构。 在这个特别的项目中，我们将从谷歌下载四种不同类型的北极狗（阿拉斯加雪橇犬，西伯利亚雪橇犬，萨摩耶犬和秋田犬）图像，并建立可以通过这些图像进行分类的最先进模型。 在这个项目中，我们将逐步从Google中为每个品种下载200多张图片。 有很多方法可以为我们的训练数据集找到最有效的谷歌图像，但是，在这个项目中，我们只需打开谷歌图像并记录我们需要的特定品种。 我们需要滚动到页面末尾，然后单击到最底部“显示更多结果”。 （700张图片是Google图片可以显示的最大数量） 使用浏览器中的Javascript代码将URL下载到文本文件中。 对于Mac用户，按Cmd Opt J，在Windows / Linux中按Ctrl Shift J打开javascript控制台并运行以下命令：12urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=&gt;JSON.parse(el.textContent).ou);window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n'))); 将下载的文件命名为’dogbreed’.txt文件（确保此时暂停广告块），在这种情况下，我们将有4种不同的犬种：阿拉斯加雪橇犬，萨摩耶犬，西伯利亚雪橇犬和秋田犬。 使用FastAI库下载图像Fast.AI有一个非常方便的功能，它将通过我们之前在文本文件中提供的URL为我们下载图像。 请注意，我们可以更改要下载的最大图片数量，我们只需要指定文本文件的路径和目标，然后函数将处理其余部分。注意：由于某些图像无法从URL打开，因此在训练步骤中可能会产生冲突。 加载的错误图像将被忽略并移至下一个URL。folder = ‘alaskan_malamute’file = ‘urls_alaskan_malamute.txt’path = Path(‘data/arctic_dogs’)destination = path/folderdestination.mkdir(parents=True, exist_ok=True) 123456folder = 'alaskan_malamute'file = 'urls_alaskan_malamute.txt'path = Path('data/arctic_dogs')destination = path/folderdestination.mkdir(parents=True, exist_ok=True)# download_images(path/file, destination, max_pics=300)download_images(path/file, destination, max_pics=300, max_workers=0) 因为我们想训练模型的所有不同类型的犬种，因此需要做4次下载。 在这种情况下，请将文件夹名称更改为其他四种不同的狗品种，并确保文件名与先前使用javascript命令下载的文本文件的名称相匹配。然后我们通过如下方式删除无法通过链接打开的图像：1234classes = ['alaskan_malamute', 'samoyed', 'siberian_husky', 'akita']for c in classes: print(c) verify_images(path/c, delete=True, max_workers=8) 让我们来看看我们的数据！我倾向于是每次运行时都有相同的随机图像，因此我们可以使用Numpy随机数种子来执行此操作。12345678np.random.seed(42)data = ImageDataBunch.from_folder( path, train=\".\", valid_pct=0.2, ds_tfms=get_transforms(), size=128, num_workers=4).normalize(imagenet_stats) 在这个项目中，我们将使用ImageDataBunch类来创建我们的数据集。 如果您查看文档，我们可以使用from_folder函数，因为我们的图像位于其尊重的文件夹名称（标签）中。 我们只需要传入我们的路径主目录’data / arctic_dogs’，在这种情况下指定我们要为数据集分区的验证集百分比为20％，注意我们从128x128的小尺寸图像开始，我们也是 指定使用4个进程来启动数据收集，并将数据规范化为张量。我们可以使用以下代码显示我们创建的一批数据集：1data.show_batch(rows=3, figsize=(10,12)) 让我们训练我们的模型！让我们为我们的模型使用Resnet34架构，并显示每个训练周期的error_rate。 123learn = create_cnn(data, models.resnet34, metrics=error_rate)#Let&apos;s do 4 cycles and see how good is our modellearn.fit_one_cycle(4) 在用resnet34架构的训练了几个周期之后，我们可以看到error_rate的减少。 我们发现错误率大约下降了2％，错误率大约为17％，这使得我们对这些北极狗的分类准确率达到了83％。 但这还不够好！ 模型行为的可解释我们可以使用FastAI的ClassificationInterpretation类来查看哪些类具有最多的错误和错误分类，以便我们可以微调我们的数据，学习速率，训练周期和我们的数据转换本身。12345learn.save(&apos;stage-1-128&apos;)learn.load(&apos;stage-1-128&apos;)interpretation = ClassificationInterpretation.from_learner(learn)#Plot the confusion matrix to see where does the most errors are madeinterpretation.plot_confusion_matrix() 我们可以清楚地看到，大多数错误都发生在阿拉斯加雪橇犬和西伯利亚雪橇之间，我们的模型训练非常适合预测萨摩耶和秋田犬。 清理数据我们在这里优化模型的方法是删除与我们的数据集无关的图像。 我们可以在目录中手动删除这些文件。123456# Get the top losses, that has the worse error and the indexes of these imageslosses, indexes = interpretation.top_losses()# Get the paths of these highest losses images from our validation data settop_loss_paths = data.valid_ds.x[indexes]# Print the paths of these imagesprint(top_loss_paths) 我们需要找到造成这个问题的原因。一般来讲，训练损失应小于验证损失，从而说明我们的模型训练正确。 而通过上面的结果，我们可以得出结论，在我们的模型中有许多可以改进的东西，使它将达到至少90％的准确度。 幸运的是，Fast.AI有一个功能，我们可以绘制模型丢失时使用的学习率。 让我们做迁移学习来优化我们的模型优化我们的模型的一个技巧是从较小的图像尺寸开始，并将学习的权重转移到更大图像尺寸的新数据集，并在优化学习速率的同时查看错误率的差异。 我们可以解冻我们的模型，并为下一个训练周期寻找最佳学习率。12345# Let&apos;s find the best learning ratelearn.unfreeze()learn.lr_find()# Plot the learning ratelearn.recorder.plot() 我们可以清楚地看到低于1e-03的学习率，损失比较低。 按照惯例，我们可以使用比1e-03小10倍的下一个学习率，在这种情况下1e-04，在反向传播期间给予它更加保守的变化率。1234# Train the model again using the same convention as beforelr = 1e-04# Train the model twice with the new learning ratelearn.fit_one_cycle(2, slice(lr)) 哇！ 使用新的学习率后，我们可以看到错误率差异减少2％。 现在让我们通过创建大小为256的新数据来开始转移学习，以查看差异。1234567891011121314# Create new ImageDataBunch with size 256data_bigger = ImageDataBunch.from_folder(path, train=&quot;.&quot;, valid_pct=.2, ds_tfms=get_transforms(), size=256, num_workers=4).normalize(imagenet_stats)# Update the learn data to use this bigger size datalearn.data = data_bigger# Unfreeze() the model and look for learning rateslearn.unfreeze()# Plot the learning rate graphlr_find(learn)learn.recorder.plot() 现在是该使用迁移学习方法的时候了。 我们将使用先前训练的权重，并输入具有256x256更大图片大小的新数据集，以查看训练差异。 我们解冻模型并开始寻找此模型的最佳学习率。 调整学习率按照惯例，当数据大小为128时，我们可以通过使用新建立的学习率和先前找到的学习率来切片学习率。切片（新学习率，先前学习率/ 5），我们将先前的学习率除以 5找到最大切片的中间点。12previous_lr = 1e-05learn.fit_one_cycle(3, slice(1e-04, previous_lr/5)) 1learn.save(&apos;stage-2-transfer&apos;) 别忘了保存模型。 结论我们在这里可以看到显着的差异。 从使用128图像尺寸开始，我们的最佳错误率约为14％。 在实施转移学习方案后，我们将错误率从大约14％降低到9％错误率，使我们对北极狗的分类准确率大约为91％！我们可以看到混淆矩阵，并与我们的第一个模型相比，看到误差的差异：123learn.load(&apos;stage-2-transfer&apos;)interpretation = ClassificationInterpretation.from_learner(learn)interpretation.plot_confusion_matrix() 我们可以看到，与我们的第一个模型相比，差异是巨大的，这次我们在用阿拉斯加雪橇犬对西伯利亚雪橇犬进行分类时只犯了3个错误。 此前该模型错误地预测了17次，并且显着下降。 让我们尝试提供图像并查看预测12img = open_image(path/&apos;siberian_husky&apos;/&apos;00000117.jpg&apos;)img 拾取的图像用于输入我们的模型预测。 123456789101112# Lets create a single data bunch and feed our model to predict the dog breed.classes = [&apos;alaskan_malamute&apos;, &apos;samoyed&apos;, &apos;siberian_husky&apos;, &apos;akita&apos;]# Create single data ImageDataBunchsingle_data = ImageDataBunch .single_from_classes(path, classes, tfms=get_transforms(), suze-256).normalize(imagenet_stats)# Create new learner with the single data learn = create_cnn(single_data, models.resnet34, metrics=accuracy)# Load the previous modellearn.load(&apos;stage-2-transfer&apos;) 现在让我们开始预测123# Get the predicted class, the index and the outputspredicted_class, predicted_index, outputs = learn.predict(img)predicted_class 完美！ 我们成功优化了我们的模型，从大约83％的精度到91％。 这是一个显着的增长，如果我们用几个周期再次训练模型，它可能更准确。非常感谢您的时间，请查看此存储库以获取完整代码！","categories":[],"tags":[]}]}