<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="A+">
<meta property="og:url" content="http://zhos.me/index.html">
<meta property="og:site_name" content="A+">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A+">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zhos.me/">





  <title>A+</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A+</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">武德</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/04/04/yuque/12产品管理的行为数据类型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/04/yuque/12产品管理的行为数据类型/" itemprop="url">12产品管理的行为数据类型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-04T18:09:41+08:00">
                2019-04-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>行为数据，描述观察到的用户或客户行为的数据，可以让您真正了解人们如何或将来可能使用您的产品。听到人们说他们想要的东西是一回事，但看看他们的实际表现会更好。<br>获得比您想象的行为数据更容易。无论您是在进行研究，运行实验来验证假设或功能，还是您刚刚发布了一项功能，行为数据都是您最好的，可以说是最有信息量的朋友。<br>本文旨在通过向您提供产品团队可用的所有类型的行为数据列表来帮助您。希望它提供了一些新的选项，如果您尝试使用数据来推动团队的决策，您可以考虑这些选项。<br>该列表大致从最常见到最不常见的顺序排列。<br>_我们是一家澳大利亚的公司，所以我们使用“behavio _<strong>_ü_</strong>_ RAL”不是美国的“行为”。_<br><a name="d4d6df97"></a></p>
<h3 id="1：网站分析数据"><a href="#1：网站分析数据" class="headerlink" title="1：网站分析数据"></a>1：网站分析数据</h3><p>您可能最熟悉此行为数据源：网页浏览量，点击次数，浏览器选择，设备选择等。网站分析是行为数据的一种形式，显示用户如何与您的网站，移动应用或用户进行互动（观看次数/点击次数）网络应用程序及其与网页浏览（设备/浏览器/分辨率）相关的选择。<br>此类数据的常用工具是<a href="https://analytics.google.com/analytics/web/" target="_blank" rel="noopener">Google Analytics</a>和更具企业价值的<a href="https://www.adobe.com/ca/experience-cloud/topics/web-analytics.html" target="_blank" rel="noopener">Adobe Experience Cloud</a>。<br><a name="2788d37f"></a></p>
<h3 id="2：App-Analytics数据"><a href="#2：App-Analytics数据" class="headerlink" title="2：App Analytics数据"></a>2：App Analytics数据</h3><p>如果您正在构建产品，您也会熟悉这些行为数据：按钮点击，活动使用和应用程序中的其他用户事件。您甚至可以在此处包含应用程序和错误日志。<br>这类数据的常用工具是<a href="https://mixpanel.com/report/984027/insights" target="_blank" rel="noopener">Mixpanel</a>，<a href="https://amplitude.com/" target="_blank" rel="noopener">Amplitude</a>和<a href="https://www.kissmetricshq.com/" target="_blank" rel="noopener">KISSmetrics</a>。它们可帮助您定义自定义事件以跟踪用户执行的操作。有什么更好的方式来理解行为，然后看看你的用户实际与之交互以及他们如何与之交互（或不与之交互）？<br>用户可能采取的行程以及何时停止使用也是有用的数据点。有很多关于您可以从用户分析中收集到的所有信息的书面文章，所以我不会在这里介绍它。<br><a name="86301aff"></a></p>
<h3 id="3：搜索数据"><a href="#3：搜索数据" class="headerlink" title="3：搜索数据"></a>3：搜索数据</h3><p>使用来自Google，Microsoft和其他人的搜索量数据，通过查看他们搜索的内容来了解行为。搜索行为本身就是一个行为数据点，它还可以提供对用户或客户正在进行的另一种行为的洞察。<br>您可以从<a href="https://ads.google.com/intl/en_au/home/tools/keyword-planner/" target="_blank" rel="noopener">Google的关键字规划师处</a>获取搜索数据。<br><a name="b9e95ad0"></a></p>
<h3 id="4：广告点击，竞争和展示数据"><a href="#4：广告点击，竞争和展示数据" class="headerlink" title="4：广告点击，竞争和展示数据"></a>4：广告点击，竞争和展示数据</h3><p>在LinkedIn，Google，Facebook或其他任何其他地方在线投放广告可提供行为数据。您可以看到人们实际回复的消息与他们所说的可能对他们有吸引力的消息。您还可以测试哪些细分市场对哪些细分市场的反应最佳。你几乎可以立即做到这一切。<br>例如，如果我想了解如何最好地将我的产品销售给银行，我可以在24-48小时内向银行经理定位facebook和linkedin广告。我可以看到广告/广告的哪些短语和变体效果最好。至少，我会根据观察到的实际行为与陈述的行为，得到一些初步的，数据支持的见解。这是非常具有革命性的。不再猜测在营销中使用哪些词。<br><a name="a5670656"></a></p>
<h3 id="5：产品评论-反馈"><a href="#5：产品评论-反馈" class="headerlink" title="5：产品评论/反馈"></a>5：产品评论/反馈</h3><p>了解人们实际体验产品的方式，他们所面临的挑战或他们最喜欢的事情是一种有用的见解。您无需将此限制为您的产品; 您可以通过查看网站查看竞争对手，补充和可比对象。亚马逊为不同的实体商品提供了大量信息。<br>在分析产品评论时，请记住并非所有产品评论都是真实的。<br><a name="376137df"></a></p>
<h3 id="6：客户支持查询"><a href="#6：客户支持查询" class="headerlink" title="6：客户支持查询"></a>6：客户支持查询</h3><p>用户和客户的反馈 - 请求，错误，问题 - 是另一种行为数据。事实上，有人已经不遗余力地与您互动，特别是在数字和软件即服务的世界中，必须承担一定的重量。如果你有足够的音量，你也可以通过分析来理解这一点。<br>您需要牢记客户的偏见或背景。也就是说，您推销产品的方式，描述功能的方式，某人经历的旅程都会影响您收到的反馈类型。<br><a name="6a9feb50"></a></p>
<h3 id="7：社交媒体"><a href="#7：社交媒体" class="headerlink" title="7：社交媒体"></a>7：社交媒体</h3><p>社交媒体的喜欢，评论，心灵和份额是另一种行为数据来源。您甚至可以进一步分析并分析评论本身的内容/文本或图像的内容。通过编译此信息，您可以了解人们实际上在说什么，分享或喜欢什么，以及他们可能实际与之交互的内容。这可以让您深入了解过去或未来的趋势。<br>当您分析社交媒体以了解行为时，您需要考虑用户喜欢/分享的容易程度。您还想要考虑某人可能喜欢或分享的动机。在许多情况下，许多人（包括我自己）实际上没有阅读内容，但看到一个流行语或标题听起来很合适，所以他们分享了。<br><a name="0fad7a8d"></a></p>
<h3 id="8光标跟踪"><a href="#8光标跟踪" class="headerlink" title="8光标跟踪"></a>8光标跟踪</h3><p>监视某人的光标在您的网站或应用上移动的位置是另一种行为数据。您可以了解他们关注的内容。<br>您可以使用<a href="https://www.hotjar.com/" target="_blank" rel="noopener">Hotjar等</a>工具来跟踪光标。<br>请记住，光标所在的位置并不是他们所关注的内容的精确表示。也就是说，我现在的眼睛正在查看Google文档中的“分享”按钮，但我正在打字，因为某些原因我的光标位于屏幕的死角。<br><a name="ef1f9893"></a></p>
<h3 id="9：眼动追踪"><a href="#9：眼动追踪" class="headerlink" title="9：眼动追踪"></a>9：眼动追踪</h3><p>如果您可以使用正确的设施和技术，您可以将行为数据提升到一个新的水平，并通过跟踪他们正在寻找的位置来跟踪人们聚焦的位置。这对数字产品很有帮助 - 网络应用程序，移动应用程序，数字资料 - 广告，网站，其他内容 - 以及超市货架和广告牌等非数字产品<br><a name="6d9f2e70"></a></p>
<h3 id="10：物理相互作用"><a href="#10：物理相互作用" class="headerlink" title="10：物理相互作用"></a>10：物理相互作用</h3><p>计算机视觉已经改进，可以跟踪人们触摸超市货架的内容。这可以扩展到他们在任何环境中接触的东西，比如他们在麦当劳使用的游乐场设备，调味品或座位。<br>以下是<a href="https://www.aipoly.com/" target="_blank" rel="noopener">AIPoly技术</a>的视频，<a href="https://www.aipoly.com/" target="_blank" rel="noopener">该技术</a>可追踪人们在<a href="https://www.aipoly.com/" target="_blank" rel="noopener">超市</a>中从货架上使用的内容：</p>
<p><a name="4d21edf9"></a></p>
<h3 id="11：面部表情分析"><a href="#11：面部表情分析" class="headerlink" title="11：面部表情分析"></a>11：面部表情分析</h3><p>计算机视觉还可以通过在与您的产品或产品营销互动时阅读某人的表达来帮助理解行为。<br>就行为数据而言，您需要仔细考虑您对面部表情识别的聚合分析的重量。例如，如果我们相信亚马逊的AI套件告诉我们我们的一个团队成员对某些事情的感受，那么我们就会认为它总是引起负面的，不愉快的反应。然而，亚马逊无法正确解释他的胡须和胡须，尤其是在他微笑的时候。你还需要考虑他们所表达的表达方式（它所推断的情感）是否重要 - 只是因为当我驾驶我的船时我看起来有点严厉，并不意味着我不喜欢它。<br><a name="a4ff36c2"></a></p>
<h3 id="12：购买历史和交易数据"><a href="#12：购买历史和交易数据" class="headerlink" title="12：购买历史和交易数据"></a>12：购买历史和交易数据</h3><p>看到有人买了什么是另一种形式的行为数据。购买通常是需要或想要的强烈迹象。有些组织以有限的形式提供此类产品 - 通过其产品或服务进行购买。其他组织可以访问个人进行的所有或许多购买。<br>您还可以访问匿名购买历史记录。我经常发现这些匿名来源有点过于笼统，不适用于特定产品。或者，数据集存在太多问题，无法在任何有意义的时间范围内进行分析。例如，交易数据可能显示在超市花了124美元而不是我买的东西。124美元的美食奶酪与124美元的尿布完全不同。<br><em>这篇文章最初出现在</em><a href="https://www.terem.com.au/blog/12-behavioural-data-types-product-management/" target="_blank" rel="noopener"><em>Terem的博客上</em></a>_。_</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/04/04/yuque/从Alexa的错误中学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/04/yuque/从Alexa的错误中学习/" itemprop="url">从Alexa的错误中学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-04T17:08:29+08:00">
                2019-04-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>虽然亚马逊的“唤醒词”功能可以改善隐私，但它提出了自己的挑战，例如Alexa录制私人谈话并未经同意将其发送出去。</em><br>通过<a href="https://www.pcmag.com/author-bio/ben-dickson" target="_blank" rel="noopener">本迪克森</a><br>Amazon Echo设备最近<a href="https://www.pcmag.com/news/361405/amazon-alexa-sends-familys-private-conversation-to-contact" target="_blank" rel="noopener">记录</a>了用户<a href="https://www.pcmag.com/news/361405/amazon-alexa-sends-familys-private-conversation-to-contact" target="_blank" rel="noopener">的私人谈话</a>，并在未经他们知情和同意的情况下将其发送给他们的一个联系人。这（再次）引起了对<a href="https://bdtechtalks.com/2018/06/05/google-home-amazon-echo-privacy-security-risks/" target="_blank" rel="noopener">智能扬声器</a>的<a href="https://bdtechtalks.com/2018/06/05/google-home-amazon-echo-privacy-security-risks/" target="_blank" rel="noopener">安全性和隐私性的</a>担忧。然而，后来变得明显，Alexa的奇怪行为并不是一个险恶的间谍情节的一部分 - 相反，它是由于智能扬声器的工作方式导致的一系列相关故障引起的。<br>根据亚马逊提供的一个<a href="https://www.theverge.com/2018/5/24/17391898/amazon-alexa-private-conversation-recording-explanation" target="_blank" rel="noopener">帐户</a>：“由于背景对话中的一个词听起来像’Alexa’，Echo醒了。然后，随后的对话被听作是“发送消息”请求。在这一点上，Alexa大声说’给谁？’ 此时，后台会话被解释为客户联系人列表中的名称。然后Alexa大声问道，’[联系人姓名]，对吗？Alexa然后将背景对话解释为“正确”。尽管这一系列事件不太可能，我们正在评估使这种情况更不可能的选择。“<br>该场景是一个边缘案例，很少发生的事件。但它也是人工智能技术极限的一项有趣研究，它为<a href="https://www.pcmag.com/review/356920/amazon-echo-2017" target="_blank" rel="noopener">Echo</a>和其他所谓的“智能”设备提供动力。<br><a name="dbda5084"></a></p>
<h4 id="太多的云依赖"><a href="#太多的云依赖" class="headerlink" title="太多的云依赖"></a>太多的云依赖</h4><p>为了理解语音命令，Echo和<a href="https://www.pcmag.com/review/349228/google-home" target="_blank" rel="noopener">Google Home</a>等智能扬声器依赖于深度学习算法，这需要大量的计算能力。由于他们没有在本地执行任务的计算资源，他们必须将数据发送到制造商的云服务器，其中AI算法将语音数据转换为文本并处理命令。<br>但<a href="https://www.pcmag.com/article/357520/the-best-smart-speakers" target="_blank" rel="noopener">智能扬声器</a>无法将他们听到的所有内容发送到他们的云服务器，因为这需要制造商在他们的服务器上存储过多的数据 - 其中大多数都是无用的。意外地记录和存储在用户家中发生的私人谈话也会带来隐私挑战，并可能使制造商陷入困境，尤其是<a href="https://www.pcmag.com/commentary/361258/how-gdpr-will-impact-the-ai-industry" target="_blank" rel="noopener">新的数据隐私法规</a>严格限制了科技公司存储和使用数据的方式。<br>这就是为什么智能扬声器被设计为在用户发出诸如“Alexa”或“Hey Google”之类的唤醒词之后触发的原因。只有在听到唤醒词后，他们才开始将麦克风的音频输入发送到云进行分析和处理。<br>虽然这项功能可以改善隐私，但它最近也出现了挑战，正如最近的Alexa事件所强调的那样。<br>Conversocial首席执行官Joshua March说：“如果[唤醒]这个词 - 或听起来非常类似的话 - 在谈话中途发出，Alexa就不会有任何先前的背景。” “在这一点上，对于任何与你设置的<a href="https://www.pcmag.com/article/352136/the-best-amazon-alexa-skills" target="_blank" rel="noopener">技能</a>相关的命令（如他们的消息应用程序），它都会非常难听。在大多数情况下，通过限制Alexa注意的环境（因为它不记录或听你的任何正常谈话）来大大增强隐私，尽管在这种情况下这种情况适得其反。“<br><a href="https://www.pcmag.com/article/360311/when-the-cloud-is-swamped-its-edge-computing-ai-to-the-re" target="_blank" rel="noopener">边缘计算的进步</a>可能有助于缓解这个问题。随着人工智能和深度学习进入越来越多的设备和应用程序，一些硬件制造商已经创建了专门用于执行AI任务的处理器，<a href="https://bdtechtalks.com/2017/08/14/edge-artificial-intelligence-fog-computing/" target="_blank" rel="noopener">而不必过多依赖云资源</a>。Edge AI处理器可以帮助Echo等设备更好地理解和处理对话，而不会通过将所有数据发送到云来侵犯用户的隐私。<br><a name="cc5f0d7a"></a></p>
<h4 id="语境和意图"><a href="#语境和意图" class="headerlink" title="语境和意图"></a>语境和意图</h4><p>除了收到不同的和零散的音频片段，亚马逊的AI还在努力理解人类对话的细微差别。<br>“尽管过去几年在深度学习方面取得了巨大进步，使软件能够比以往更好地理解语音和图像，但仍有很多限制，”March说。“虽然语音助理可以识别您所说的单词，但他们并不一定能够真正理解其背后的含义或意图。世界是一个复杂的地方，但今天任何一个人工智能系统都只能处理非常具体，狭窄的用例。“<br>例如，我们人类有很多方法可以确定一个句子是针对我们的，例如语调，还是跟随视觉提示 - 比如说话者正在看的方向。<br>相反，Alexa假定它是任何包含“A”字的句子的接收者。这就是用户<a href="https://www.wired.com/2017/02/keep-amazon-echo-google-home-responding-tv/" target="_blank" rel="noopener">经常意外触发它的</a>原因。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/219582/1554368966421-4b31ea14-2751-4acc-9e9c-fe2a50ae48dc.jpeg#align=left&amp;display=inline&amp;height=420&amp;name=0_6qM4_cxLwEpZattA.jpg&amp;originHeight=456&amp;originWidth=810&amp;size=61176&amp;status=done&amp;width=746" alt="0_6qM4_cxLwEpZattA.jpg"><br>部分问题在于我们<a href="https://www.pcmag.com/news/361929/why-everything-elon-musk-fears-about-ai-is-wrong" target="_blank" rel="noopener">夸大了</a>当前AI应用程序<a href="https://www.pcmag.com/news/361929/why-everything-elon-musk-fears-about-ai-is-wrong" target="_blank" rel="noopener">的功能</a>，经常使它们与人类思维相提并论，并且过于信任它们。这就是为什么当他们失败时我们会感到惊讶。<br>“这里的部分问题是”人工智能“这个术语如此积极地推向市场，以至于消费者对这个术语与他们的关系产生了不必要的信任，”神经科学家，Starmind的创始人帕斯卡尔考夫曼说。“这个故事说明Alexa有很多能力，对于如何以及何时适当应用它们的理解相对有限。”<br>当深度学习算法面临偏离数据和他们训练的场景的设置时，它们很<a href="https://www.pcmag.com/commentary/360196/4-reasons-not-to-fear-deep-learning-yet" target="_blank" rel="noopener">容易失败</a>。“人类人工智能的一个明确特征是自给自足的能力和对内容的真正理解，”考夫曼说。“这是真正认为AI’聪明’的关键部分，对其发展至关重要。创造自我意识的数字助理，让他们充分了解人性，将标志着他们从有趣的新奇转变为真正有用的工具。“<br>但是，创建人类级AI（也称为一般AI）说起来容易做起来难。几十年来，我们一直认为它即将到来，只是因为技术的进步表明人类的思想是多么复杂而变得沮丧。许多专家认为<a href="https://www.forbes.com/sites/gilpress/2016/12/21/artificial-intelligence-pioneers-peter-norvig-google/#110e624d38c6" target="_blank" rel="noopener">追逐一般人工智能是徒劳的</a>。<br>同时，狭窄的AI（作为当前的人工智能技术被描述）仍然提供了许多机会并且可以被修复以避免重复错误。需要明确的是，深度学习和机器学习仍处于初期阶段，像亚马逊这样的公司不断更新其AI算法，以便在每次发生时解决边缘情况。<br><a name="96eace11"></a></p>
<h4 id="我们需要做什么"><a href="#我们需要做什么" class="headerlink" title="我们需要做什么"></a>我们需要做什么</h4><p>“这是一个年轻的新兴领域。自然语言理解尤其处于起步阶段，因此我们可以在这里做很多事情，“Atomic X首席技术官Eric Moller说。<br>Moller认为可以调整语音分析AI算法以更好地理解语调和变形。“在更广泛的句子中使用’Alexa’这个词听起来与调用或命令不同。Alexa不应该醒来，因为你顺便说出了这个名字，“莫勒说。通过足够的训练，AI应该能够区分哪些特定音调指向智能扬声器。<br>科技公司也可以训练他们的AI，以便能够区分何时接收背景噪音而不是直接说话。“背景喋喋不休具有独特的听觉’特征’，人类非常善于接受并有选择地调整。我们没有理由不能训练AI模型来做同样的事情，“莫勒说。<br>作为预防措施，AI助理应评估他们所做出的决策的影响，并在他们想要做一些可能敏感的事情的情况下参与人为决策。制造商应在其技术中加入更多保护措施，以防止在未经用户明确和明确同意的情况下发送敏感信息。<br>Tonkean首席执行官Sagi Eliyahi表示：“虽然亚马逊确实报告说Alexa试图确认其解释的行动，但有些行动需要更加谨慎地管理并保持更高标准的用户意图确认。” “人类有相同的语音识别问题，偶尔会听到请求。然而，与Alexa不同，人类更可能完全确认他们理解不明确的请求，更重要的是，衡量请求与过去请求相比的可能性。“<br><a name="640c2ad0"></a></p>
<h4 id="同时…"><a href="#同时…" class="headerlink" title="同时…"></a>同时…</h4><p>虽然科技公司微调他们的AI应用程序以减少错误，但用户必须做出最终决定他们希望暴露于他们的AI驱动设备可能产生的潜在错误的程度。<br>“这些故事与人们愿意分享的数据量与新的人工智能技术的承诺相冲突，”数据科学专家道格罗斯和几本关于人工智能和软件的书的作者说。“你可能会因为速度缓慢而取笑Siri。但她获得更多智慧的最佳方式是入侵我们的私人谈话。因此，未来十年左右的一个关键问题是，我们将允许这些AI代理人查看我们的行为有多少？“<br>Starmind的神经科学家Kaufmann说：“哪个家庭会把人类助理放在起居室里让那个人一直听到任何形式的谈话？” “在隐私，保密或可靠性方面，我们至少应该对所谓的’AI’设备（如果不是更高的话）应用相同的标准，我们也适用于人类智能生物。”<br><strong>阅读更多：“ </strong><a href="https://www.pcmag.com/commentary/362085/3-big-tech-ideas-that-need-to-be-shelved-for-now" target="_blank" rel="noopener"><strong>需要搁置的三大技术创意（现在）</strong></a><strong> ”</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/04/04/yuque/语音助理在手机上被浪费了/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/04/yuque/语音助理在手机上被浪费了/" itemprop="url">语音助理在手机上被浪费了</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-04T16:55:53+08:00">
                2019-04-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>智能手机和智能扬声器在解锁语音接口的全部潜力方面受到限制。数字助理的未来是增强和虚拟现实。</em><br><strong>通过</strong><a href="https://www.pcmag.com/author-bio/ben-dickson" target="_blank" rel="noopener"><strong>本迪克森</strong></a><br>自2011年首次亮相以来，苹果公司的语音助手Siri已经激励众多公司开发竞争对手的数字助理，其中最受欢迎的是  <a href="https://www.pcmag.com/article/352136/the-best-amazon-alexa-skills" target="_blank" rel="noopener">亚马逊的Alexa</a>和Google智能助理 - 早已超越Siri的质量。<br>然而，语音助手仅用于我们使用计算设备完成的一小部分任务，因为它们目前所处的平台存在局限性。<br><a name="5b379941"></a></p>
<h4 id="智能手机和PC不是专为语音设计的"><a href="#智能手机和PC不是专为语音设计的" class="headerlink" title="智能手机和PC不是专为语音设计的"></a>智能手机和PC不是专为语音设计的</h4><p>调查将告诉您，越来越多的人在智能手机上使用语音助理。但是他们的使用占手机上花费的一小部分时间。<br>例如，<a href="https://voicebot.ai/2018/04/03/over-half-of-smartphone-owners-use-voice-assistants-siri-leads-the-pack/" target="_blank" rel="noopener">Verto Analytics</a>在4月份进行的一项调查发现，52％的智能手机用户使用语音助手，但他们的使用量仅限于每天0.33次，与用户每天使用智能手机进行的<a href="https://www.emarketer.com/content/mobile-time-spent-2018" target="_blank" rel="noopener">数百次互动</a>相比，这一点可以忽略不计。<br>责备<a href="https://www.pcmag.com/article/361245/the-endless-scroll-how-to-tell-if-youre-a-tech-addict" target="_blank" rel="noopener">智能手机设计</a>。用户界面旨在让您盯着并滚动手机。对于任何带屏幕的设备，语音始终是次要输入。<br>语音助理最适合需要免提体验的环境。您可以让Siri在您准备工作时阅读您的电子邮件，但您最好坐下来通过手机屏幕阅读它们。因为<a href="https://www.wired.com/story/multitasking-problem-with-virtual-assistants/" target="_blank" rel="noopener">我们的大脑没有被优化</a>以同时专注于两项任务，所以当你试图在做其他事情时听Siri阅读你的电子邮件时，你会分心。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/219582/1554368242999-30532705-c563-42c9-be9e-e59d42332a2c.jpeg#align=left&amp;display=inline&amp;height=420&amp;name=0_QvCZJqXXKmAed554.jpg&amp;originHeight=456&amp;originWidth=810&amp;size=61108&amp;status=done&amp;width=746" alt="0_QvCZJqXXKmAed554.jpg"><br>因此，语音助理仅限于简单的任务，例如打开应用程序，拨打电话或简单查询，例如询问天气 - 大多数用户在与手机的触摸屏交互时更喜欢执行的所有任务。<br>台式机和笔记本电脑对语音助理来说甚至是不太直观的体验。自2016年以来，Siri已经在macOS上可用，而且微软已将Cortana作为<a href="https://www.pcmag.com/article2/0,2817,2488631,00.asp" target="_blank" rel="noopener">Windows 10</a>不可或缺的一部分。我拥有一台Mac和一台PC，我喜欢修补新的技术，但我从来没有在其中任何一个上设置数字助理。什么时候是您最后一次需要在免提情况下使用PC？在大多数情况下，我们正盯着屏幕并使用鼠标和键盘，这使得数字助理成为可选的，非常有用的功能，仅此而已。<br><a name="18d28dff"></a></p>
<h4 id="演讲者无法释放语音的全部潜力"><a href="#演讲者无法释放语音的全部潜力" class="headerlink" title="演讲者无法释放语音的全部潜力"></a>演讲者无法释放语音的全部潜力</h4><p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/219582/1554368276085-b843b128-041f-4692-985c-5d42a174c326.jpeg#align=left&amp;display=inline&amp;height=420&amp;name=0_JcKUe1rICD4YEKzT.jpg&amp;originHeight=456&amp;originWidth=810&amp;size=69591&amp;status=done&amp;width=746" alt="0_JcKUe1rICD4YEKzT.jpg"><br><a href="https://www.pcmag.com/article/357520/the-best-smart-speakers" target="_blank" rel="noopener">智能扬声器</a>显然更有效地使用语音助手; Verto发现，智能音箱使用者每天平均使用语音助手2.79次，这远远超过智能手机，但仍然不多。<br>在这种情况下，问题在于智能扬声器本身，因为使用没有显示器的设备几乎无法做到。尽管您可以在Amazon Echo和Google Home等智能扬声器上安装数以万计的技能，但大多数用户都会将它们用于有限数量的任务，包括<a href="https://www.pcmag.com/news/355153/how-to-listen-to-music-on-your-amazon-echo" target="_blank" rel="noopener">播放音乐</a>，设置定时器以及打开和关闭灯光。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/219582/1554368312783-30eafd93-e372-44d4-9578-7b1293adf4e8.jpeg#align=left&amp;display=inline&amp;height=420&amp;name=0_MVM06vp9o222zhHE.jpg&amp;originHeight=456&amp;originWidth=810&amp;size=47603&amp;status=done&amp;width=746" alt="0_MVM06vp9o222zhHE.jpg"><br>只要您想执行涉及多个步骤的复杂任务，智能扬声器的限制就会变得明显。例如，<a href="https://www.theinformation.com/articles/the-reality-behind-voice-shopping-hype?jwt=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ3aW50ZXJnaXJsMjA5MUBnbWFpbC5jb20iLCJleHAiOjE1NjUxOTc3NzgsIm4iOiJHdWVzdCIsInNjb3BlIjpbInNoYXJlIl19.J8S28txP5DVspKpg0ukqkfOhI5FUID3hdugfV2EQpgI&amp;unlock=cfcdc52689c42f6b" target="_blank" rel="noopener">The Information</a>报告称拥有<a href="https://www.pcmag.com/review/356920/amazon-echo-2017" target="_blank" rel="noopener">Amazon Echo</a>的人中只有2％使用它进行语音购物，这是智能扬声器最广告宣传的功能之一。在进行购物时，用户想要浏览项目，查看他们的选项并进行比较，这对于仅使用扬声器作为输出媒体的设备是不可能的。<br>为此，亚马逊推出了<a href="https://www.pcmag.com/review/364210/amazon-echo-show-2018" target="_blank" rel="noopener">Echo Show</a>，这是一款带触摸屏的智能扬声器，可以显示您设置的定时器列表，而不是为您阅读。谷歌本月早些时候跟随<a href="https://www.pcmag.com/news/364301/hands-on-with-the-google-home-hub-smart-display" target="_blank" rel="noopener">家庭中心</a>。<br><a name="b3bd436c"></a></p>
<h4 id="AR和VR是语音助手的真正家园"><a href="#AR和VR是语音助手的真正家园" class="headerlink" title="AR和VR是语音助手的真正家园"></a>AR和VR是语音助手的真正家园</h4><p><img src="https://cdn.nlark.com/yuque/0/2019/jpeg/219582/1554368341376-092531e4-69fb-45dc-bd9d-11be4eb5937d.jpeg#align=left&amp;display=inline&amp;height=420&amp;name=0_XBlO8zAGXatOBAfw.jpg&amp;originHeight=456&amp;originWidth=810&amp;size=57970&amp;status=done&amp;width=746" alt="0_XBlO8zAGXatOBAfw.jpg"><br>语音助手的完美环境是用户希望执行各种任务同时将注意力集中在周围环境（如增强和虚拟现实应用程序）上的设置。<br>与智能手机和计算机应用程序不同，<a href="https://www.pcmag.com/article/347242/ar-vs-vr-whats-the-difference" target="_blank" rel="noopener">AR和VR</a>应用程序要求用户查看其真实和虚拟环境并与之交互，而不是专注于显示屏。与智能扬声器不同，AR耳机不仅限于您在厨房和起居室中执行的任务。<br>由于基础技术的进步，AR在<a href="https://www.pcmag.com/article/356680/how-augmented-reality-is-transforming-work" target="_blank" rel="noopener">越来越多的专业领域</a>（如制造和医疗保健）中显示出前景。但主要挑战之一是用户界面。耳机没有键盘，鼠标或触摸屏。用户通常通过耳机侧面的手势，控制器和触摸板与应用程序进行交互。某些设备还支持基本语音命令，但与AR环境进行交互通常非常具有挑战性。<br>人工智能语音助手可以派上用场，特别是与眼动追踪等其他技术结合使用时。例如，用户可以盯着虚拟和物理元素而不是使用手势，并要求他们的语音助手执行不同的操作，例如查询信息或激活对象。这在用户已经完全无法使用手势或控制器的设置中尤其有用。<br>Magic Leap正在为其<a href="https://www.pcmag.com/review/363545/magic-leap-one" target="_blank" rel="noopener">Magic Leap One</a>混合现实耳机制作两名AI助手，首席执行官Rony Abovitz <a href="https://www.theverge.com/2018/8/8/17662040/magic-leap-one-creator-edition-preview-mixed-reality-glasses-launch" target="_blank" rel="noopener">告诉The Verge</a>。一个是“一个简单的机器人生物，用于执行低级任务”，另一个是“一个单独的类似人类的实体，你会被视为平等，如果你是粗鲁的话它会离开房间。 ”<br>我不同意Abovitz，数字助理必须有人类行为。像<a href="https://www.youtube.com/watch?v=ZwOxM0-byvc" target="_blank" rel="noopener">钢铁侠的JARVIS</a>这样的东西会更实用，更少分散注意力。尽管如此，Abovitz对此非常关注 - 数字助理将成为未来AR装备不可或缺的一部分。<br>当增强现实充分发挥其潜力并且虚拟现实设备达到主流时，数字助理将不再是您休闲时使用的可选功能。它们将成为您的AR / VR体验中不可分割的一部分，帮助您完成传统的用户交互方式无法完成的任务。<br><strong>阅读更多：“ </strong><a href="https://www.pcmag.com/news/362260/which-mobile-voice-assistant-is-used-the-most" target="_blank" rel="noopener"><strong>哪个移动语音助理使用最多？</strong></a><strong>”</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/04/03/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/03/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-03T10:27:06+08:00">
                2019-04-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/转移学习：重新启动Inception V3以进行自定义图像分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/转移学习：重新启动Inception V3以进行自定义图像分类/" itemprop="url">转移学习：重新启动Inception V3以进行自定义图像分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T18:00:40+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>让我们通过将现有的图像分类器（Inception V3）调整为自定义任务来体验转移学习的强大功能：对产品图像进行分类，以帮助食品和杂货零售商减少仓库和零售店库存管理过程中的人力。<br>这项工作的源代码可以在我下面的GitHub存储库中找到。<br><a href="https://github.com/wisdal/Image-classification-transfer-learning" target="_blank" rel="noopener">https://github.com/wisdal/Image-classification-transfer-learning</a><br>使用的工具：TensorFlow v1.1，Python 3.4，Jupyter。<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903355906-1736cd25-8311-4055-a6bd-aa237d022793.png#align=left&amp;display=inline&amp;height=436&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=584&amp;originWidth=1000&amp;size=759418&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p><br>深度神经网络的应用实际上是在滚动。无论是医疗保健，运输还是零售，各行各业的公司都对投资构建智能解决方案感到兴奋。同时，让我们希望人类的智慧仍然无可争议:)<br><a name="9dd10fb0"></a></p>
<h3 id="趋势AI文章："><a href="#趋势AI文章：" class="headerlink" title="趋势AI文章："></a>趋势AI文章：</h3><blockquote>
<p><a href="https://chatbotslife.com/how-neural-networks-work-ff4c7ad371f7" target="_blank" rel="noopener">1.神经网络如何工作</a>&gt; <a href="https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32" target="_blank" rel="noopener">2. ResNets，HighwayNets和DenseNets，哦，我的！</a>&gt; <a href="https://chatbotslife.com/machine-learning-for-dummies-part-1-dbaca076ec07" target="_blank" rel="noopener">3.机器学习傻瓜</a><br>在实际情况下，强大的图像分类器等解决方案可以帮助公司跟踪货架库存，对产品进行分类，记录产品量，从专用设备（无人机？机器人？）实时捕获的原始产品图像。当然，能够识别产品并从给定的图片预测其类别是交易的一部分，这就是这个实验的全部内容：我们将培训机器人从图像中分类食品和杂货产品。</p>
</blockquote>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903381892-b439e8f9-e1d2-49ed-9eb1-02be65c96a4f.png#align=left&amp;display=inline&amp;height=390&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=523&amp;originWidth=1000&amp;size=593236&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>“这是一项轻松的任务！”，我们认为是人类，但计算机程序并不一定如此。无论如何，这个假设甚至都不适用于所有类别的人类; 一个5岁的孩子是一个完美的反例！当我们放大时，一切都是为了在您的生活中看到足够的产品图像，您可以从给定的图像轻松识别您之前看过的任何产品。<br>当我们使用现有标记数据训练模型时，我们尝试将这种经验概念转移到模型，以便学习如何准确地区分训练数据集中存在的不同类别的数据。从这个意义上说，我们使用<strong>人工神经网络</strong>，它只不过是模仿人类大脑的实际运作方式。使用这些算法构建模型的知识稍后将在未标记的观察上进行测试。在我们的示例中，模型将基于其先前学习的内容标记输入图像，因此通常分配给该任务的名称<strong>监督学习 </strong>。<br>谈到性能，已经注意到，在大多数监督学习的情况下，训练有素的模型往往比人类提供更好的准确性。在这个实际任务中，您会惊讶地发现即使在困难的条件下（模糊的图像，质量差的图像等），我们的算法也非常优于人类。<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903421984-d099ab43-1a63-4ccb-9833-783d03b242b8.png#align=left&amp;display=inline&amp;height=381&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=420&amp;originWidth=823&amp;size=329410&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"><br><br><br>我们有一个来自hackerearth的产品图像数据集可以<a href="https://he-s3.s3.amazonaws.com/media/hackathon/deep-learning-challenge-1/identify-the-objects/a0409a00-8-dataset_dp.zip" target="_blank" rel="noopener">在这里</a>下载。我们应该如何进行，为什么我们使用转学习？</p>
<hr>
<p><a name="c0139b0c"></a></p>
<h3 id="为何转学？"><a href="#为何转学？" class="headerlink" title="为何转学？"></a><strong>为何转学？</strong></h3><p>当我们考虑对图像进行分类时，我们常常选择从头开始构建我们的模型以获得最佳匹配。这是一个选项，但构建自定义深度学习模型需要大量的计算资源和大量的培训数据。此外，已经存在的模型在分类来自各种类别的图像时表现得相当好。您可能听说过<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>及其大视觉识别挑战。在这个计算机视觉挑战中，模型试图将大量图像分类为1000个类，如“斑马”，“达尔马提亚”和“洗碗机”。<a href="https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html" target="_blank" rel="noopener">Inception V3</a>是Google Brain Team为此而建立的模型。毋庸置疑，该模型表现非常出色。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903675886-eeb54f5e-2e91-4943-9a2c-afc8761e6979.png#align=left&amp;display=inline&amp;height=278&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=373&amp;originWidth=1000&amp;size=109931&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>那么，我们可以利用这个模型的存在来进行像现在这样的自定义图像分类任务吗？嗯，这个概念有一个名字：<a href="https://en.wikipedia.org/wiki/Transfer_learning" target="_blank" rel="noopener"><strong>转移学习</strong></a>。它可能不如从头开始的完整培训那么高效，但对于许多应用来说都是惊人的有效。通过修改现有的丰富深度学习模型，它可以显着减少训练数据和时间。<br><a name="d2db47fa"></a></p>
<h3 id="为什么会这样"><a href="#为什么会这样" class="headerlink" title="为什么会这样"></a><strong>为什么会这样</strong></h3><p>在神经网络中，神经元被分层组织。不同的层可以对其输入执行不同类型的转换。信号可能在多次遍历各层之后从第一层（输入）传播到最后一层（输出）。作为最后一个隐藏层，“瓶颈”具有足够的汇总信息，以提供执行实际分类任务的下一层。<br>在<a href="https://github.com/wisdal/image-classification-transfer-learning/blob/master/retrain.py" target="_blank" rel="noopener">retrain.py</a>脚本中，我们删除旧的顶层，并在我们下载的图片上训练一个新的顶层。<br>我们的最后一层再训练可以用于新类的原因是，结果表明，区分ImageNet中所有1000个类所需的信息通常也可用于区分新类型的对象。<br>我们现在弄脏手！</p>
<hr>
<p><a name="81ef9f80"></a></p>
<h3 id="第1步：预处理图像"><a href="#第1步：预处理图像" class="headerlink" title="第1步：预处理图像"></a><strong>第1步：预处理图像</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">label_counts = train.label.value_counts（）</span><br><span class="line">plt.figure（figsize =（12,6））</span><br><span class="line">sns.barplot（label_counts.index，label_counts.values，alpha = 0.9）</span><br><span class="line">plt.xticks（rotation =&apos;vertical&apos;）</span><br><span class="line">plt.xlabel （&apos;Image Labels&apos;，fontsize = 12）</span><br><span class="line">plt.ylabel（&apos;Counts&apos;，fontsize = 12）</span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903798892-d0219c97-9167-48dc-a792-2f0f3a65e604.png#align=left&amp;display=inline&amp;height=419&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=419&amp;originWidth=721&amp;size=14783&amp;status=done&amp;width=721" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"><br>假设您已经下载了<a href="https://he-s3.s3.amazonaws.com/media/hackathon/deep-learning-challenge-1/identify-the-objects/a0409a00-8-dataset_dp.zip" target="_blank" rel="noopener">数据集</a>，您会发现它附带了一个我们需要正确设置的“train”文件夹。我们的目标是将每个图像放在代表其类别的子文件夹中。最后，我们应该有x个子文件夹，x是不同类别的数量。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552904730643-3f6be018-2297-4b30-af9e-7dd590e648ac.png#align=left&amp;display=inline&amp;height=426&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=481&amp;originWidth=843&amp;size=35297&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>为了这个预处理目的，我为您提供了<a href="https://github.com/wisdal/image-classification-transfer-learning/blob/master/pre_process.ipynb" target="_blank" rel="noopener">pre_process.ipynb</a>笔记本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">for img in tqdm(train.values):</span><br><span class="line">    filename=img[0]</span><br><span class="line">    label=img[1]</span><br><span class="line">    src=os.path.join(data_root,&apos;train_img&apos;,filename+&apos;.png&apos;)</span><br><span class="line">    label_dir=os.path.join(data_root,&apos;train&apos;,label)</span><br><span class="line">    dest=os.path.join(label_dir,filename+&apos;.jpg&apos;)</span><br><span class="line">    im=Image.open(src)</span><br><span class="line">    rgb_im=im.convert(&apos;RGB&apos;)</span><br><span class="line">    if not os.path.exists(label_dir):</span><br><span class="line">        os.makedirs(label_dir)</span><br><span class="line">    rgb_im.save(dest)  </span><br><span class="line">    if not os.path.exists(os.path.join(data_root,&apos;train2&apos;,label)):</span><br><span class="line">        os.makedirs(os.path.join(data_root,&apos;train2&apos;,label))</span><br><span class="line">    rgb_im.save(os.path.join(data_root,&apos;train2&apos;,label,filename+&apos;.jpg&apos;))</span><br></pre></td></tr></table></figure>
<p>笔记本不仅仅是配置图像子文件夹，所以一定要检查它。<br>因为我们的数据集带有<strong>25个独特的标签，</strong>而我们<strong>只有3215个训练图像</strong>，我们需要增加数据以防止我们的模型过度拟合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">        rotation_range=40,</span><br><span class="line">        width_shift_range=0.2,</span><br><span class="line">        height_shift_range=0.2,</span><br><span class="line">        shear_range=0.2,</span><br><span class="line">        zoom_range=0.2,</span><br><span class="line">        horizontal_flip=True,</span><br><span class="line">        fill_mode=&apos;nearest&apos;)</span><br><span class="line"></span><br><span class="line">class_size=600</span><br><span class="line"></span><br><span class="line">src_train_dir=os.path.join(data_root,&apos;train&apos;)</span><br><span class="line">dest_train_dir=os.path.join(data_root,&apos;train2&apos;)</span><br><span class="line">it=0</span><br><span class="line">for count in label_counts.values:</span><br><span class="line">    #nb of generations per image for this class label in order to make it size ~= class_size</span><br><span class="line">    ratio=math.floor(class_size/count)-1</span><br><span class="line">    print(count,count*(ratio+1))</span><br><span class="line">    dest_lab_dir=os.path.join(dest_train_dir,label_counts.index[it])</span><br><span class="line">    src_lab_dir=os.path.join(src_train_dir,label_counts.index[it])</span><br><span class="line">    if not os.path.exists(dest_lab_dir):</span><br><span class="line">        os.makedirs(dest_lab_dir)</span><br><span class="line">    for file in os.listdir(src_lab_dir):</span><br><span class="line">        img=load_img(os.path.join(src_lab_dir,file))</span><br><span class="line">        #img.save(os.path.join(dest_lab_dir,file))</span><br><span class="line">        x=img_to_array(img) </span><br><span class="line">        x=x.reshape((1,) + x.shape)</span><br><span class="line">        i=0</span><br><span class="line">        for batch in datagen.flow(x, batch_size=1,save_to_dir=dest_lab_dir, save_format=&apos;jpg&apos;):</span><br><span class="line">            i+=1</span><br><span class="line">            if i &gt; ratio:</span><br><span class="line">                break </span><br><span class="line">    it=it+1</span><br></pre></td></tr></table></figure>
<p><a name="a57a7ff4"></a></p>
<h3 id="第2步：重新训练瓶颈并微调模型"><a href="#第2步：重新训练瓶颈并微调模型" class="headerlink" title="第2步：重新训练瓶颈并微调模型"></a><strong>第2步：重新训练瓶颈并微调模型</strong></h3><p>由谷歌提供，我们立即开始使用retrain.py脚本。该脚本默认下载Inception V3 <a href="http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz" target="_blank" rel="noopener">预训练模型</a>。<br>重新训练脚本是我们算法的核心组件，也是使用从初始v3开始的转移学习的任何自定义图像分类任务的核心组件。它是由TensorFlow作者自己设计的，用于此特定目的（自定义图像分类）。<br><a name="6af7ed82"></a></p>
<h4 id="脚本的作用："><a href="#脚本的作用：" class="headerlink" title="脚本的作用："></a>脚本的作用：</h4><p>它训练一个新的顶层（瓶颈），可以识别特定类别的图像。顶层接收每个图像的2048维向量作为输入。然后在该表示之上训练softmax层。假设softmax层包含N个标签，这对应于学习与学习的偏差和权重相对应的N + 2048 <em> N（或1001 </em> N）个模型参数。<br>该脚本完全可以自定义，这里是可配置的参数列表：</p>
<ul>
<li><strong>image_dir</strong>：标记图像文件夹的路径。幸运的是，我们在预处理步骤中正确设置了它。</li>
<li><strong>output_graph，intermediate_output_graphs_dir，output_labels等</strong>：保存输出文件的位置。</li>
<li><strong>失真功能：</strong>我的最爱。仅此功能就值得整整一段。您可能已经注意到我们的训练集中的图像是完美的（清晰，高质量，明确）但不幸的是，在生产中并非总是如此。该算法可能会在部署后遇到，模糊图像，昏暗的图像等。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552904870948-60b8d20b-8035-44db-b9b4-909606809a52.png#align=left&amp;display=inline&amp;height=617&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=724&amp;originWidth=876&amp;size=572978&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>我们的算法应该足够智能，以捕捉这些图像代表相同的事情，并不是那么明显（这只是一个小例子）</p>
<ul>
<li>[…]这就是失真特征的全部内容。我们有意地在训练过程中随机变换图像（大小，颜色，方向等）以使机器人习惯于不良图像，以避免在这种情况下失去预测准确性。</li>
<li><strong>how_many_training_steps</strong>：时代数。</li>
<li><strong>学习率</strong>。</li>
<li>…</li>
</ul>
<p>您可以随意使用这些参数。<strong>学习率</strong>，<strong>nb。时期</strong>等是确定性参数。使用它们来微调您的模型并记住您可以随时使用TensorBoard来可视化您的训练结果。<br>您可以从一开始就获得约85％的准确度（零微调）。<br><a name="1fdd8e7a"></a></p>
<h3 id="第3步：在看不见的记录上测试模型"><a href="#第3步：在看不见的记录上测试模型" class="headerlink" title="第3步：在看不见的记录上测试模型"></a>第3步：在看不见的记录上测试模型</h3><p>这一步没什么可疯狂的。只是一个小脚本来测试在上一步中构建和保存的模型，在我们数据集的“test”文件夹中的图像上。<br>查看<a href="https://github.com/wisdal/Image-classification-transfer-learning/blob/master/test.ipynb" target="_blank" rel="noopener">测试</a>笔记本，了解需要完成的工作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def run_graph(src, labels, input_layer_name, output_layer_name,</span><br><span class="line">              num_top_predictions):</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">    i=0</span><br><span class="line">    #outfile=open(&apos;submit.txt&apos;,&apos;w&apos;)</span><br><span class="line">    #outfile.write(&apos;image_id, label \n&apos;)</span><br><span class="line">    for f in os.listdir(dest):</span><br><span class="line">        image_data=load_image(os.path.join(dest,test[i]+&apos;.jpg&apos;))</span><br><span class="line">        #image_data=load_image(os.path.join(src,f))</span><br><span class="line">        softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)</span><br><span class="line">        predictions, = sess.run(softmax_tensor, &#123;input_layer_name: image_data&#125;)</span><br><span class="line"></span><br><span class="line">        # Sort to show labels in order of confidence</span><br><span class="line">        top_k = predictions.argsort()[-num_top_predictions:][::-1]</span><br><span class="line">        for node_id in top_k:</span><br><span class="line">            human_string = labels[node_id]</span><br><span class="line">            score = predictions[node_id]</span><br><span class="line">            #print(&apos;%s (score = %.5f) %s , %s&apos; % (test[i], human_string))</span><br><span class="line">            print(&apos;%s, %s&apos; % (test[i], human_string))</span><br><span class="line">            #outfile.write(test[i]+&apos;, &apos;+human_string+&apos;\n&apos;)</span><br><span class="line">        i+=1</span><br><span class="line">    return 0</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552904928699-8cc5dd07-ebf5-4116-a3e3-5d4ece361238.png#align=left&amp;display=inline&amp;height=663&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=663&amp;originWidth=664&amp;size=265517&amp;status=done&amp;width=664" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"><br><a name="54bbba80"></a></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>而已！希望这篇文章对你有用。随意评论并提出改进建议。<br>我鼓励你试一试，并在评论中告诉我你能达到多少准确度。我很乐意收到你的来信。<br>正如我之前所说，你肯定能够获得<strong>超过85％</strong>的基准精度。剩下的就是微调！在我的情况下，我的最终模型在测试装置上的准确性让我感到震惊，考虑到需要的工作量很少。很好地理解事情的工作方式有时候会有所帮助:)。我认为该项目是任何想要尝试图像失真或超参数调整的人的良好基础。这就是为我增加了更多的百分点。<br>请随意查看我在<a href="https://github.com/wisdal/Image-classification-transfer-learning" target="_blank" rel="noopener">GitHub上的</a>代码。<br>资源：<a href="https://www.tensorflow.org/tutorials/image_recognition" target="_blank" rel="noopener">tensorflow.org/tutorials/image_recognition</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/Spark上的实际Python工作负载：独立群集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/Spark上的实际Python工作负载：独立群集/" itemprop="url">Spark上的实际Python工作负载：独立群集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T17:45:16+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于在Spark上运行Python有无数文章和论坛帖子，但大多数人认为要提交的工作包含在一个.py文件中：<code>spark-submit wordcount.py</code> - 完成！<br>如果你的Python程序不仅仅是一个脚本怎么办？也许它为Spark生成动态SQL来执行，或使用Spark的输出刷新模型。随着您的Python代码变得更像一个应用程序（具有目录结构，配置文件和库依赖项），将其提交给Spark需要更多考虑。<br>以下是我最近考虑使用Spark 2.3将一个这样的Python应用程序用于生产时的替代方案。第一篇文章重点介绍Spark独立集群。另一篇文章介绍了EMR Spark（YARN）。<br>我远不是Spark的权威，更不用说Python了。我的决定试图平衡正确性和易于部署，以及应用程序对群集的限制。让我知道你的想法。<br><a name="9dd10fb0"></a></p>
<h3 id="趋势AI文章："><a href="#趋势AI文章：" class="headerlink" title="趋势AI文章："></a>趋势AI文章：</h3><blockquote>
<p><a href="https://becominghuman.ai/predicting-buying-behavior-using-machine-learning-a-case-study-on-sales-prospecting-part-i-3bf455486e5d" target="_blank" rel="noopener">1.使用机器学习预测购买行为</a>&gt; <a href="https://becominghuman.ai/understanding-and-building-generative-adversarial-networks-gans-8de7c1dc0e25" target="_blank" rel="noopener">2.理解和构建生成对抗网络（GAN）</a>&gt; <a href="https://becominghuman.ai/building-a-django-post-face-detection-api-using-opencv-and-haar-cascades-dcf4e0e5f725" target="_blank" rel="noopener">3.使用OpenCV和Haar Cascades构建Django POST面部检测API</a>&gt; <a href="https://becominghuman.ai/learning-from-mistakes-with-hindsight-experience-replay-547fce2b3305" target="_blank" rel="noopener">4.通过Hindsight Experience Replay从错误中学习</a><br><a name="3cf6ae3c"></a></p>
</blockquote>
<h4 id="示例Python应用程序"><a href="#示例Python应用程序" class="headerlink" title="示例Python应用程序"></a>示例Python应用程序</h4><p>为了模拟完整的应用程序，下面的场景假定Python 3应用程序具有以下结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">project.py </span><br><span class="line">data / </span><br><span class="line">    data_source.py </span><br><span class="line">    data_source.ini</span><br></pre></td></tr></table></figure>
<p>_data_source.ini_包含各种配置参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[spark] </span><br><span class="line">app_name =myPySpark App </span><br><span class="line">master_url = spark：// sparkmaster：7077</span><br></pre></td></tr></table></figure>
<p>_data_source.py_是一个模块，负责在Spark中获取和处理数据，使用NumPy进行数学转换，并将Pandas数据帧返回给客户端。依赖关系：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkConf, SparkContext</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import StructType, StructField, FloatType</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import configparser</span><br></pre></td></tr></table></figure>
<p>它定义了一个创建和初始化的<em>DataSource</em>类……<code>SparkContext`</code>SparkSession`</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class DataSource:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        config = configparser.ConfigParser()</span><br><span class="line">        config.read(&apos;./data/data_source.ini&apos;)</span><br><span class="line">        master_url = config[&apos;spark&apos;][&apos;master_url&apos;]</span><br><span class="line">        app_name = config[&apos;spark&apos;][&apos;app_name&apos;]</span><br><span class="line">        conf = SparkConf().setAppName(app_name) \</span><br><span class="line">                          .setMaster(master_url)</span><br><span class="line">        self.sc = SparkContext(conf=conf)</span><br><span class="line">        self.spark = SparkSession.builder \</span><br><span class="line">                                 .config(conf=conf) \</span><br><span class="line">                                 .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>…和一个_get_data（）_方法：</p>
<ol>
<li>从NumPy正态分布创建RDD。</li>
<li>应用函数将每个元素的值加倍。</li>
<li>将RDD转换为Spark数据帧并在顶部定义临时视图。</li>
<li>应用Python UDF，使用SQL对每个dataframe元素的内容进行平方。</li>
<li>将结果作为Pandas数据帧返回给客户端。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def get_data(self, num_elements=1000) -&gt; pd.DataFrame:</span><br><span class="line">    mu, sigma = 2, 0.5</span><br><span class="line">    v = np.random.normal(mu, sigma, num_elements)</span><br><span class="line">    rdd1 = self.sc.parallelize(v)</span><br><span class="line">    def mult(x): return x * np.array([2])</span><br><span class="line">    rdd2 = rdd1.map(mult).map(lambda x: (float(x),))</span><br><span class="line">    schema = StructType([StructField(&quot;value&quot;, FloatType(), True)])</span><br><span class="line">    df1 = self.spark.createDataFrame(rdd2, schema)</span><br><span class="line">    df1.registerTempTable(&quot;test&quot;)</span><br><span class="line">    def square(x): return x ** 2</span><br><span class="line">    self.spark.udf.register(&quot;squared&quot;, square)</span><br><span class="line">    df2 = self.spark.sql(&quot;SELECT squared(value) squared FROM test&quot;)</span><br><span class="line">    return df2.toPandas()</span><br></pre></td></tr></table></figure>
<p><em>project.py</em>是我们的主程序，充当上述模块的客户端：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from data.data_source import DataSource</span><br><span class="line">def main():</span><br><span class="line">    src = DataSource()</span><br><span class="line">    df = src.get_data(num_elements=100000)</span><br><span class="line">    print(f&quot;Got Pandas dataframe with &#123;df.size&#125; elements&quot;)</span><br><span class="line">    print(df.head(10))</span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<p>克隆回购：<a href="https://bitbucket.org/calidoteam/pyspark.git" target="_blank" rel="noopener">https</a>：<a href="https://bitbucket.org/calidoteam/pyspark.git" target="_blank" rel="noopener">//bitbucket.org/calidoteam/pyspark.git</a><br>在开始之前，让我们回顾一下向Spark提交工作时可用的选项。<br><a name="3b2c7180"></a></p>
<h4 id="spark-submit，客户端和集群模式"><a href="#spark-submit，客户端和集群模式" class="headerlink" title="spark-submit，客户端和集群模式"></a>spark-submit，客户端和集群模式</h4><ul>
<li>Spark支持各种<a href="https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types" target="_blank" rel="noopener">集群管理器</a>：独立（即内置于Spark），Hadoop的YARN，Mesos，Kubernetes，所有这些都控制着工作负载在一组资源上的运行方式。</li>
<li><code>spark-submit</code>是唯一与所有集群管理器一致的接口。对于Python应用程序，<code>spark-submit</code>可以在需要时上载和暂存您提供的所有依赖项，如.py，.zip或.egg文件。</li>
<li>在<strong><em>客户端</em></strong><em>模式下</em>驱动程序_）将在运行的同一主机上<code>spark-submit</code>运行。确保此类主机靠近工作节点以减少网络延迟符合您的最佳利益。</li>
<li>在<strong>_集群_</strong><em>模式下</em>驱动程序_某个_工作节点并从其运行。从远程主机提交作业时，这很有用。从Spark 2.4独立开始，Spark 2.4.0集群模式不是一个选项。</li>
<li>或者，可以<code>spark-submit</code>通过<code>SparkSession</code>在Python应用程序中配置连接到群集来绕过。这需要正确的配置和匹配的PySpark二进制文件。您的Python应用程序将在<em>客户端模式下</em>有效运行：它将从您启动它的主机上运行。</li>
</ul>
<p>以下部分描述了几种部署方案，以及每种方案中需要的配置。</p>
<hr>
<p><a name="47d746c3"></a></p>
<h3 id="＃1：直接连接到Spark（客户端模式，无spark-submit）"><a href="#＃1：直接连接到Spark（客户端模式，无spark-submit）" class="headerlink" title="＃1：直接连接到Spark（客户端模式，无spark-submit）"></a>＃1：直接连接到Spark（客户端模式，无spark-submit）</h3><p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552902588026-b01dacd8-8d16-4298-b2d9-9237f34d8543.png#align=left&amp;display=inline&amp;height=462&amp;name=1_HcQRb-GgmYnWRF15yobCOw.png&amp;originHeight=619&amp;originWidth=1000&amp;size=293525&amp;status=done&amp;width=746" alt="1_HcQRb-GgmYnWRF15yobCOw.png"><br>针对Spark独立的客户端模式的Python应用程序</p>
<p>这是最简单的部署方案：Python应用程序通过指向Spark主URL直接建立spark上下文，并使用它来提交工作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(&quot;My PySpark App&quot;) \</span><br><span class="line">                  .setMaster(&quot;spark://192.168.1.10:7077&quot;)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">                    .config(conf=conf) \</span><br><span class="line">                    .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>在<strong>独立群集中</strong>，资源在作业持续时间内分配，默认配置为客户端应用程序提供所有可用资源，因此需要对多租户环境进行微调。执行程序进程（JVM或python）由每个节点本地的_工作_进程启动。<br>这类似于传统的客户端 - 服务器应用程序，因为客户端只是“连接”到“远程”集群。建议：<br><strong>确保</strong>驱动程序和群集之间<strong>有足够的带宽</strong>。大多数网络活动发生在驱动程序和它的执行程序之间，因此这个“远程”集群实际上必须在近距离（LAN）内。<br><strong>通过启用Apache Arrow改进Java-Python序列化：</strong> Python工作负载（NumPy，Pandas和其他应用于Spark RDD，数据帧和数据集的转换）默认需要大量的Java和Python进程的序列化和反序列化，并且会迅速降低性能。从Spark 2.3开始，<a href="https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html" target="_blank" rel="noopener">启用Apache Arrow</a>（包含在下面列出的步骤中）使这些传输<a href="https://databricks.com/session/improving-python-and-spark-performance-and-interoperability-with-apache-arrow" target="_blank" rel="noopener">更加高效</a>。<br><strong>跨所有群集节点和驱动程序主机部署依赖关系。</strong>这包括<a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">下载和安装Python 3</a>，pip安装PySpark（必须与目标集群的版本匹配），PyArrow以及其他库依赖项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install python36</span><br><span class="line">pip install pyspark==2.3.1 </span><br><span class="line">pip install pyspark[sql]</span><br><span class="line">pip install numpy pandas msgpack sklearn</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>在安装像PySpark这样的大型库（~200MB）时，可能会遇到以“ <code>MemoryError</code>” 结尾的错误。如果是这样，请尝试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --no-cache-dir pyspark==2.3.1</span><br></pre></td></tr></table></figure>
<p><strong>配置和环境变量：</strong>在客户端，<code>$SPARK_HOME</code>必须指向pip安装PySpark的位置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ pip show pyspark</span><br><span class="line">Name: pyspark</span><br><span class="line">Version: 2.3.1</span><br><span class="line">Summary: Apache Spark Python API</span><br><span class="line">Home-page: https://github.com/apache/spark/tree/master/python</span><br><span class="line">Author: Spark Developers</span><br><span class="line">Author-email: dev@spark.apache.org</span><br><span class="line">License: http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">Location: /opt/anaconda/lib/python3.6/site-packages</span><br><span class="line">Requires: py4j</span><br><span class="line">$ export SPARK_HOME=/opt/anaconda/lib/python3.6/site-packages</span><br></pre></td></tr></table></figure>
<p>在每个群集节点上，设置其他默认参数和环境变量。特别是对于Python应用程序：<br><code>$SPARK_HOME/conf/spark-defaults.sh</code></p>
<ul>
<li><code>spark.sql.execution.arrow.enabled true</code></li>
</ul>
<p><code>$SPARK_HOME/conf/spark-env.sh</code></p>
<ul>
<li><code>export PYSPARK_PYTHON=/usr/bin/python3</code> ：Python可执行文件，所有节点。</li>
<li><code>export PYSPARK_DRIVER_PYTHON=/usr/bin/python3</code> ：驱动程序的Python可执行文件，如果与执行程序节点不同。</li>
</ul>
<p><strong>注：</strong>环境变量是从哪里读<code>spark-submit</code>的_推出_，不一定是从群集主机内。<br><a name="d5f01497"></a></p>
<h4 id="运行它"><a href="#运行它" class="headerlink" title="运行它"></a>运行它</h4><p>将工作负载提交到集群只需运行Python应用程序（例如<code>spark-submit</code>，不需要）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd my-project-dir/</span><br><span class="line">$ python3 project.py</span><br></pre></td></tr></table></figure>
<p>在运行时，可以看到运行多个<em>python3</em>进程的从属节点在作业上运行：<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552902767067-19085e70-7bf9-4058-84d8-cc174302f2af.png#align=left&amp;display=inline&amp;height=304&amp;name=1_l-aJ47luaPs9wpj_FTlMSQ.png&amp;originHeight=408&amp;originWidth=1000&amp;size=413381&amp;status=done&amp;width=746" alt="1_l-aJ47luaPs9wpj_FTlMSQ.png"></p>
<p><a name="93533b7d"></a></p>
<h3 id="＃2：集装箱式应用（客户端模式，无火花提交）"><a href="#＃2：集装箱式应用（客户端模式，无火花提交）" class="headerlink" title="＃2：集装箱式应用（客户端模式，无火花提交）"></a>＃2：集装箱式应用（客户端模式，无火花提交）</h3><p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552902810496-c3ddc7d0-c9e2-4e34-823f-54fbc0e9966f.png#align=left&amp;display=inline&amp;height=512&amp;name=1_zcpriTNz1FM6kWUpQNr_RA.png&amp;originHeight=686&amp;originWidth=1000&amp;size=282197&amp;status=done&amp;width=746" alt="1_zcpriTNz1FM6kWUpQNr_RA.png"><br>这是前一个场景的扩展，由于可移植性等原因，最好将Python应用程序作为Docker容器运行，作为CI / CD管道的一部分。<br>除了针对上一个方案推荐的配置外，还需要以下内容：<br><strong>构建容器以包含所有依赖项：</strong>从包含Python 3和/或Java 8 OpenJDK的映像开始，然后pip-install PySpark，PyArrow以及应用程序所需的所有其他库。<br><strong>配置Spark驱动程序主机和端口，在容器中打开它们：</strong>这是执行程序到达容器内驱动程序所必需的。可以通过编程方式设置驱动程序的Spark属性（<code>spark.conf.set(“property”, “value”)</code>）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.host : host_ip_address (e.g. 192.168.1.10)</span><br><span class="line">spark.driver.port : static_port (e.g. 51400)</span><br><span class="line">spark.driver.bindAddress : container_internal_ip (e.g. 10.192.6.81)</span><br><span class="line">spark.driver.blockManagerPort : static_port (e.g. 51500)</span><br></pre></td></tr></table></figure>
<p>在Docker中，端口可以使用-p选项从命令行公开到外部：<code>-p 51400:51400 -p 51500:51500</code>。其他文章建议只发布此端口范围：<code>-p 5000–5010:5000–5010</code><br><a name="d5f01497-1"></a></p>
<h4 id="运行它-1"><a href="#运行它-1" class="headerlink" title="运行它"></a>运行它</h4><p>与前一个场景一样，运行容器将启动Python驱动程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 51400：51400 -p 51500：51500 &lt;docker_image_url&gt;</span><br></pre></td></tr></table></figure>
<p><a name="2efdb298"></a></p>
<h3 id="＃3：通过spark-submit提供Python应用程序（客户端模式）"><a href="#＃3：通过spark-submit提供Python应用程序（客户端模式）" class="headerlink" title="＃3：通过spark-submit提供Python应用程序（客户端模式）"></a>＃3：通过spark-submit提供Python应用程序（客户端模式）</h3><p>此方案实际上与方案＃1相同，仅为了清楚起见包含在此处。唯一的区别是Python应用程序是使用该<code>spark-submit</code>进程启动的。除了日志文件之外，还会将群集事件发送到<em>stdout</em>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd my-project-dir/</span><br><span class="line">$ ls -l </span><br><span class="line">rwxrwxr-x. 3 centos centos  70 Feb 25 02:11 data</span><br><span class="line">-rw-rw-r--. 1 centos centos 220 Feb 25 01:09 project.py</span><br><span class="line">$ spark-submit project.py</span><br></pre></td></tr></table></figure>
<p><strong>笔记：</strong></p>
<ul>
<li>根据我的经验，<code>spark-submit</code>只要从项目根目录（<code>my-project-dir/</code>）调用它就没有必要在调用时传递依赖的子目录/文件。</li>
<li>由于示例应用程序已指定主URL，因此无需将其传递给<code>spark-submit</code>。否则，更完整的命令将是：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master spark://sparkcas1:7077 --deploy-mode client project.py</span><br></pre></td></tr></table></figure>
<ul>
<li>从Spark 2.3开始，无法将集群模式下的Python应用程序提交给独立的Spark集群。这样做会产生错误：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master spark://sparkcas1:7077 --deploy-mode cluster project.py</span><br><span class="line">Error: Cluster deploy mode is currently not supported for python applications on standalone clusters.</span><br></pre></td></tr></table></figure>
<p><a name="938988a1"></a></p>
<h4 id="Takeaways-Python-on-Spark独立集群："><a href="#Takeaways-Python-on-Spark独立集群：" class="headerlink" title="Takeaways- Python on Spark独立集群："></a><strong>Takeaways- Python on Spark独立集群：</strong></h4><ul>
<li>虽然独立群集在生产中不受欢迎（可能是因为商业支持的分发包括群集管理器），但只要不需要多租户和动态资源分配，它们的占用空间就会更小并且做得很好。</li>
<li>对于Python应用程序，部署选项仅限于客户端模式。</li>
<li>使用Docker来容纳Python应用程序具有所有预期的优势，非常适合客户端模式部署。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/如何（以及为什么）创建一个好的验证集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/如何（以及为什么）创建一个好的验证集/" itemprop="url">如何（以及为什么）创建一个好的验证集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T12:19:06+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a name="d41d8cd9"></a></p>
<p>#<br>撰稿：2017年11月13日由<strong>Rachel Thomas</strong>撰写<br>一个非常常见的场景：看似令人印象深刻的机器学习模型在生产中实施时是完全失败的。这些影响包括那些现在对机器学习持怀疑态度且不愿意再次尝试的领导者。怎么会发生这种情况？<br>对于开发结果与生产结果之间的这种脱节的最可能的罪魁祸首之一是选择不当的验证集（或者更糟糕的是，根本没有验证集）。根据数据的性质，选择验证集可能是最重要的一步。尽管sklearn提供了一种<a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener"></a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split</a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">方法</a>，但该方法采用了数据的随机子集，这对于许多现实问题来说是一个糟糕的选择。<br><strong>培训</strong>，<strong>验证</strong>和<strong>测试</strong>集的定义可能相当细微，有时不一致地使用这些术语。在深度学习社区中，“测试时间推断”通常用于指对生产中的数据进行评估，这不是测试集的技术定义。如上所述，sklearn有一种train_test_split方法，但没有train_validation_test_split。Kaggle只提供培训和测试集，但要做得好，您需要将他们的训练集分成您自己的验证和训练集。而且，事实证明Kaggle的测试集实际上被细分为两组。很多初学者可能会感到困惑，这并不奇怪！我将在下面解决这些微妙之处。<br><a name="e7a3668b"></a></p>
<h2 id="首先，什么是“验证集”？"><a href="#首先，什么是“验证集”？" class="headerlink" title="首先，什么是“验证集”？"></a><strong>首先，什么是“验证集”？</strong></h2><p>在创建机器学习模型时，最终目标是使其准确处理新数据，而不仅仅是用于构建数据的数据。考虑下面一组数据的3种不同模型的例子：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770075-60b15664-d2d8-4359-9920-d9d56d8a789b.png#align=left&amp;display=inline&amp;height=281&amp;name=image.png&amp;originHeight=294&amp;originWidth=781&amp;size=83649&amp;status=done&amp;width=746" alt="image.png"><br>资料来源：Andrew Ng的机器学习课程<br>对于最右侧的模型，图示数据点的误差最小（蓝色曲线几乎完美地穿过红点），但它不是最佳选择。这是为什么？如果你要收集一些新的数据点，它们很可能不在右边图表中的那条曲线上，而是更接近中间图形中的曲线。<br>根本的想法是：</p>
<ul>
<li>训练集用于训练给定的模型<br></li>
<li>验证集用于在模型之间进行选择（例如，随机森林或神经网络是否更适合您的问题？您想要一个有40棵树或50棵树的随机森林吗？）<br></li>
<li>测试集告诉你你是怎么做的。如果您已经尝试了很多不同的模型，那么您可能只是偶然得到一个在验证集上表现良好的模型，并且拥有测试集有助于确保不是这种情况。<br></li>
</ul>
<p>验证和测试集的一个关键属性是它们必须代表<strong><strong>您将来会看到</strong></strong>的<strong><strong>新数据</strong></strong>。这可能听起来像一个不可能的命令！根据定义，您还没有看到这些数据。但是你仍然知道一些事情。<br><a name="7086b6ec"></a></p>
<h2 id="什么时候随机子集不够好？"><a href="#什么时候随机子集不够好？" class="headerlink" title="什么时候随机子集不够好？"></a><strong>什么时候随机子集不够好？</strong></h2><p>看一些例子是有益的。虽然这些例子中有很多来自Kaggle比赛，但它们代表了您在工作场所会遇到的问题。<br><a name="cc464807"></a></p>
<h3 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a><strong>时间序列</strong></h3><p>如果您的数据是时间序列，那么选择数据的随机子集将非常简单（您可以在您尝试预测的日期之前和之后查看数据）并且不代表大多数业务用例（您在哪里）正在使用历史数据来构建将来使用的模型。如果您的数据包含日期并且您要构建将来使用的模型，则需要选择具有最新日期的连续部分作为验证集（例如，可用数据的最后两周或上个月） 。<br>假设您要将下面的时间序列数据拆分为训练和验证集：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770129-8cad81d5-fbe9-4945-baca-d69af4bf8853.png#align=left&amp;display=inline&amp;height=492&amp;name=image.png&amp;originHeight=492&amp;originWidth=617&amp;size=31169&amp;status=done&amp;width=617" alt="image.png"><br><em>时间序列数据</em><br>随机子集是一个糟糕的选择（太容易填补空白，并不表示您在生产中需要什么）：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770176-99c4e0dd-3800-454b-bf9d-b9b73bfe4f44.png#align=left&amp;display=inline&amp;height=492&amp;name=image.png&amp;originHeight=492&amp;originWidth=617&amp;size=27107&amp;status=done&amp;width=617" alt="image.png"><br><em>训练集的选择不佳</em><br>使用较早的数据作为训练集（以及验证集的后续数据）：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770243-648aea97-36ed-4e23-a69e-ec2dcbf32598.png#align=left&amp;display=inline&amp;height=492&amp;name=image.png&amp;originHeight=492&amp;originWidth=617&amp;size=26915&amp;status=done&amp;width=617" alt="image.png"><br><em>为您的训练集提供更好的选择</em><br>Kaggle目前正在竞争<a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" target="_blank" rel="noopener"></a><a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" target="_blank" rel="noopener">预测一系列厄瓜多尔杂货店的销售情况</a>。Kaggle的“培训数据”从2013年1月1日至2017年8月15日运行，测试数据跨越2017年8月16日至2017年8月31日。一个好方法是使用2017年8月1日至8月15日作为验证集，以及所有早期数据作为你的训练集。<br><a name="574cf8ad"></a></p>
<h3 id="新人，新船，新…"><a href="#新人，新船，新…" class="headerlink" title="新人，新船，新…"></a><strong>新人，新船，新…</strong></h3><p>您还需要考虑在生产中进行预测的数据可能与您训练模型所需的数据<strong>有何不同</strong>。<br>在Kaggle <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection" target="_blank" rel="noopener"></a><a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection" target="_blank" rel="noopener">分心驾驶员竞赛中</a>，独立数据是汽车驾驶员的照片，因变量是一个类别，如发短信，吃饭或安全向前看。如果您是从这些数据构建模型的保险公司，请注意您最感兴趣的是模型在您之前没有见过的驱动程序上的表现（因为您可能只为一小部分人提供培训数据）。对于Kaggle比赛也是如此：测试数据由未在训练集中使用的人组成。<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770581-4e7e5ff9-8b12-415e-8197-bc32f52b5957.png#align=left&amp;display=inline&amp;height=354&amp;name=image.png&amp;originHeight=354&amp;originWidth=469&amp;size=208442&amp;status=done&amp;width=469" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770937-9c3f6222-3e65-46e1-a25d-3a7c7241b3bf.png#align=left&amp;display=inline&amp;height=359&amp;name=image.png&amp;originHeight=359&amp;originWidth=467&amp;size=208605&amp;status=done&amp;width=467" alt="image.png"><br><em>同一个人驾驶时在电话上交谈的两个图像。</em><br>如果您将上述图像之一放入训练集中，并将其中一个放在验证集中，那么您的模型似乎表现得比新人更好。另一个观点是，如果你使用所有人来训练你的模型，你的模型可能过分适应那些特定人的特殊性，而不仅仅是学习状态（发短信，吃饭等）。<br>在<a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring" target="_blank" rel="noopener"></a><a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring" target="_blank" rel="noopener">Kaggle渔业竞赛中也</a>有类似的动态，以确定渔船捕获的鱼类种类，以减少非法捕捞濒危种群。测试集由未出现在训练数据中的船组成。这意味着您希望验证集包含不在训练集中的船只。<br>有时可能不清楚您的测试数据会有何不同。例如，对于使用卫星图像的问题，您需要收集有关训练集是否仅包含某些地理位置的更多信息，或者是否来自地理位置分散的数据。<br><a name="af629453"></a></p>
<h2 id="交叉验证的危险"><a href="#交叉验证的危险" class="headerlink" title="交叉验证的危险"></a><strong>交叉验证的危险</strong></h2><p>sklearn没有train_validation_test拆分的原因是假设您经常使用<strong><strong>交叉验证</strong></strong>，其中训练集的不同子集用作验证集。例如，对于3倍交叉验证，数据被分为3组：A，B和C.首先训练模型A和B组合作为训练集，并在验证集C上进行评估。 ，将模型训练为A和C组合作为训练集，并在验证集B上进行评估。依此类推，最终将3倍的模型性能平均化。<br>但是，交叉验证的问题在于，由于上述各节中描述的所有原因，它很少适用于现实世界的问题。交叉验证仅适用于您可以随机调整数据以选择验证集的相同情况。<br><a name="5c6fff0d"></a></p>
<h2 id="Kaggle的“训练集”-你的训练-验证集"><a href="#Kaggle的“训练集”-你的训练-验证集" class="headerlink" title="Kaggle的“训练集”=你的训练+验证集"></a><strong>Kaggle的“训练集”=你的训练+验证集</strong></h2><p>Kaggle比赛的一个好处是它们会迫使你更严格地考虑验证集（为了做得好）。对于那些刚接触Kaggle的人来说，它是一个举办机器学习比赛的平台。Kaggle通常会将数据分为两组，您可以下载：</p>
<ol>
<li>一个<strong><strong>训练集</strong></strong>，其中包括<strong>独立的变量</strong>，以及对<strong>因变量</strong>（你正在尝试预测）。对于试图预测销售的厄瓜多尔杂货店的例子，自变量包括商店ID，商品ID和日期; 因变量是销售数量。对于尝试确定驾驶员是否在车轮后面进行危险行为的示例，自变量可以是驾驶员的图片，并且因变量是类别（例如发短信，吃饭或安全地向前看）。<br></li>
<li>一个<strong><strong>测试集</strong></strong>，它只有自变量。您将对测试集进行预测，您可以将其提交给Kaggle，并获得您的评分。<br></li>
</ol>
<p>这是开始机器学习所需的基本思想，但要做得好，理解起来要复杂一些。您将需要创建自己的培训和验证集（通过拆分Kaggle“培训”数据）。您将使用较小的训练集（Kaggle训练数据的子集）来构建模型，并且在提交给Kaggle之前，您可以在验证集（也是Kaggle的训练数据的子集）上对其进行评估。<br>最重要的原因是Kaggle将测试数据分为两组：公共和私人排行榜。您在公共排行榜上看到的分数仅适用于您预测的一部分（您不知道哪个子集！）。您的预测在私人排行榜上的表现将不会在比赛结束前公布。这一点很重要的原因是你最终可能会过度适应公共排行榜，直到最后你在私人排行榜上做得不好时才会意识到这一点。使用良好的验证集可以防止这种情况。您可以通过查看您的模型是否具有与Kaggle测试集相比较的相似分数来检查您的验证集是否有用。<br>创建自己的验证集很重要的另一个原因是Kaggle限制您每天提交两次，并且您可能希望尝试更多。第三，确切地看到你在验证集上出错的地方是有益的，而且Kaggle没有告诉你测试集的正确答案，甚至没有告诉你哪些数据点你的错误，只是你的整体得分。<br>理解这些区别不仅对Kaggle有用。在任何预测性机器学习项目中，您希望模型能够在新数据上表现良好。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发/" itemprop="url">如何使用良好的软件工程实践设置PySpark环境以进行开发</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T11:16:24+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本文中，我们将讨论如何设置我们的开发环境以创建高质量的python代码以及如何自动执行一些繁琐的任务来加速部署。<br>我们将介绍以下步骤：</p>
<ul>
<li>使用<a href="https://pipenv.readthedocs.io/en/latest/" target="_blank" rel="noopener"><strong>pipenv</strong></a>在隔离的虚拟环境中设置我们的依赖<a href="https://pipenv.readthedocs.io/en/latest/" target="_blank" rel="noopener"><strong>项</strong></a></li>
<li>如何为多个作业设置项目结构</li>
<li>如何运行pyspark工作</li>
<li>如何使用<a href="https://opensource.com/article/18/8/what-how-makefile" target="_blank" rel="noopener"><strong>Makefile</strong></a><strong> </strong>自动执行开发步骤</li>
<li>如何使用<a href="http://flake8.pycqa.org/en/latest/" target="_blank" rel="noopener"><strong>flake8</strong></a>测试代码的质量<a href="http://flake8.pycqa.org/en/latest/" target="_blank" rel="noopener"></a></li>
<li>如何使用<a href="https://pypi.org/project/pytest-spark/" target="_blank" rel="noopener"><strong>pytest-spark</strong></a>为PySpark应用程序运行单元测试<a href="https://pypi.org/project/pytest-spark/" target="_blank" rel="noopener"></a></li>
<li>运行测试覆盖率，看看我们是否使用<a href="https://pypi.org/project/pytest-cov/" target="_blank" rel="noopener"><strong>pytest-cov</strong></a>创建了足够的单元测试<a href="https://pypi.org/project/pytest-cov/" target="_blank" rel="noopener"></a><br><a name="54e5af6c"></a><h3 id="第1步：设置虚拟环境"><a href="#第1步：设置虚拟环境" class="headerlink" title="第1步：设置虚拟环境"></a>第1步：设置虚拟环境</h3>虚拟环境有助于我们将特定应用程序的依赖关系与系统的整体依赖关系隔离开来。这很好，因为我们不会涉及现有库的依赖性问题，并且在单独的系统（例如docker容器或服务器）上安装或卸载它们更容易。对于此任务，我们将使用<strong>pipenv。</strong><br>要在mac os系统上安装它，例如运行：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install pipenv</span><br></pre></td></tr></table></figure>
<p>要为应用程序声明我们的依赖项（库），我们需要在项目的路径路径中创建一个<strong>Pipfile</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[source]]</span><br><span class="line">url = &apos;https://pypi.python.org/simple&apos;</span><br><span class="line">verify_ssl = true</span><br><span class="line">name = &apos;pypi&apos;</span><br><span class="line">[requires]</span><br><span class="line">python_version = &quot;3.6&quot;</span><br><span class="line">[packages]</span><br><span class="line">flake8 = &quot;*&quot;</span><br><span class="line">pytest-spark = &quot;&gt;=0.4.4&quot;</span><br><span class="line">pyspark = &quot;&gt;=2.4.0&quot;</span><br><span class="line">pytest-cov = &quot;*&quot;</span><br></pre></td></tr></table></figure>
<p>这里有三个组件。在<strong>[[source]]</strong>标签中，我们声明了下载所有软件包的<strong>url</strong>，在<strong>[requires]中</strong>我们定义了python版本，最后在<strong>[packages]</strong>中声明了我们需要的依赖项。我们可以将依赖项绑定到某个版本，或者使用<strong>“*”</strong>符号来获取最新版本。<br>要创建虚拟环境并激活它，我们需要在终端中运行两个命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipenv --three install</span><br><span class="line">pipenv shell</span><br></pre></td></tr></table></figure>
<p>一旦完成这一步，你应该看到你在一个新的venv中，让项目的名字出现在命令行的终端中（默认情况下，env采用项目的名称）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(pyspark-project-template) host:project$</span><br></pre></td></tr></table></figure>
<p>现在，您可以使用两个命令进出。<br>停用env并返回标准环境：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure>
<p>再次激活虚拟环境（您需要位于项目的根目录中）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source `pipenv --venv`/bin/activate</span><br></pre></td></tr></table></figure></p>
<p><a name="bb546939"></a></p>
<h3 id="第2步：项目结构"><a href="#第2步：项目结构" class="headerlink" title="第2步：项目结构"></a>第2步：项目结构</h3><p>该项目可以具有以下结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">pyspark-project-template</span><br><span class="line">    src/</span><br><span class="line">        jobs/   </span><br><span class="line">            pi/</span><br><span class="line">                __init__.py</span><br><span class="line">                resources/</span><br><span class="line">                    args.json</span><br><span class="line">            word_count/</span><br><span class="line">                __init__.py</span><br><span class="line">                resources/</span><br><span class="line">                    args.json</span><br><span class="line">                    word_count.csv</span><br><span class="line">        main.py</span><br><span class="line">    test/</span><br><span class="line">        jobs/</span><br><span class="line">            pi/</span><br><span class="line">                test_pi.py</span><br><span class="line">            word_count/</span><br><span class="line">                test_word_count.py</span><br></pre></td></tr></table></figure>
<p>排除一些<strong>init</strong>.py文件以简化操作，但您可以在本教程末尾的github上找到完整项目的链接。我们基本上有源代码和测试。每个作业都分成一个文件夹，每个作业都有一个资源文件夹，我们在其中添加该作业所需的额外文件和配置。<br>在本教程中，我使用了两个经典示例 -  <strong>pi</strong>，生成最多小数的pi数和<strong>字数</strong>，以计算csv文件中的单词数。<br><a name="1b6fecdc"></a></p>
<h3 id="第3步：使用spark-submit运行作业"><a href="#第3步：使用spark-submit运行作业" class="headerlink" title="第3步：使用spark-submit运行作业"></a>第3步：使用spark-submit运行作业</h3><p>我们先来看看<strong>main.py</strong>文件的样子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser(description=&apos;My pyspark job arguments&apos;)</span><br><span class="line">    parser.add_argument(&apos;--job&apos;, type=str, required=True, dest=&apos;job_name&apos;,</span><br><span class="line">                        help=&apos;The name of the spark job you want to run&apos;)</span><br><span class="line">    parser.add_argument(&apos;--res-path&apos;, type=str, required=True, dest=&apos;res_path&apos;,</span><br><span class="line">                        help=&apos;Path to the jobs resurces&apos;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(args.job_name)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    job_module = importlib.import_module(&apos;jobs.%s&apos; % args.job_name)</span><br><span class="line">    res = job_module.run(spark, get_config(args.res_path, args.job_name))</span><br><span class="line"></span><br><span class="line">    print(&apos;[JOB &#123;job&#125; RESULT]: &#123;result&#125;&apos;.format(job=args.job_name, result=res))</span><br></pre></td></tr></table></figure>
<p>当我们运行我们的工作时，我们需要两个命令行参数：  <strong>- job</strong>，是我们想要运行的作业的名称（在例外pi或word_count中）和  <strong>- res-path</strong>，是作业的相对路径。我们需要第二个参数，因为spark需要知道我们资源的完整路径。在生产环境中，我们将代码部署在集群上，我们将资源转移到HDFS或S3，我们将使用该路径。<br>在进一步解释代码之前，我们需要提一下，我们必须压缩<strong>作业</strong>文件夹并将其传递给<strong>spark-submit</strong>语句<strong>。</strong>假设我们在项目的根目录中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd src/ </span><br><span class="line">zip -r ../jobs.zip jobs/</span><br></pre></td></tr></table></figure>
<p>这将使代码在我们的应用程序中作为模块提供。基本上在<strong>第16行的main.py中，</strong>我们以编程方式导入<strong>作业</strong>模块。<br>我们的作业<strong>pi</strong>和<strong>word_count</strong>都有一个<strong>run</strong>函数，所以我们只需要运行这个函数来启动这个作业（<strong>main.py中的第17行）。</strong>我们还在那里传递了工作的配置。<br>让我们看一下<strong>word_count</strong>作业，进一步了解这个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line">def get_keyval(row):</span><br><span class="line">    words = filter(lambda r: r is not None, row)</span><br><span class="line">    return [[w.strip().lower(), 1] for w in words]</span><br><span class="line"></span><br><span class="line">def run(spark, config):</span><br><span class="line">    df = spark.read.csv(config[&apos;relative_path&apos;] + config[&apos;words_file_path&apos;])</span><br><span class="line">    mapped_rdd = df.rdd.flatMap(lambda row: get_keyval(row))</span><br><span class="line">    counts_rdd = mapped_rdd.reduceByKey(add)</span><br><span class="line">    return counts_rdd.collect()</span><br></pre></td></tr></table></figure>
<p>此代码在<strong>word_count</strong>文件夹的<strong><strong>init</strong>.py</strong>文件中定义。我们在这里可以看到，我们使用两个配置参数来读取资源文件夹中的csv文件：相对路径和csv文件的位置。其余的代码只计算单词，所以我们不会在这里详细介绍。值得一提的是，每个作业在resources文件夹中都有一个<strong>args.json</strong>文件。这里我们实际定义了传递给作业的配置。这是<strong>word_count</strong>作业的配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;words_file_path&quot;: &quot;/word_count/resources/word_count.csv&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以我们现在有了所有细节来运行我们的<strong>spark-submit</strong>命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --py-files jobs.zip src/main.py --job word_count --res-path /your/path/pyspark-project-template/src/jobs</span><br></pre></td></tr></table></figure>
<p>要运行另一个作业<strong>pi，</strong>我们只需要更改<strong>- job</strong>标志的参数  。<br><a name="b336e1a0"></a></p>
<h3 id="第4步：编写单元测试，并使用覆盖率运行它们"><a href="#第4步：编写单元测试，并使用覆盖率运行它们" class="headerlink" title="第4步：编写单元测试，并使用覆盖率运行它们"></a>第4步：编写单元测试，并使用覆盖率运行它们</h3><p>要为pyspark应用程序编写测试，我们使用<strong>pytest-spark</strong>，一个非常易于使用的模块。<br>该<strong>WORD_COUNT</strong>工作单元测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from src.jobs.word_count import get_keyval, run</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test_get_keyval():</span><br><span class="line">    words=[&apos;this&apos;, &apos;are&apos;, &apos;words&apos;, &apos;words&apos;]</span><br><span class="line">    expected_results=[[&apos;this&apos;, 1], [&apos;are&apos;, 1], [&apos;words&apos;, 1], [&apos;words&apos;, 1]]</span><br><span class="line"></span><br><span class="line">    assert expected_results == get_keyval(words)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test_word_count_run(spark_session):</span><br><span class="line">    expected_results = [(&apos;one&apos;, 1), (&apos;two&apos;, 1), (&apos;three&apos;, 2), (&apos;four&apos;, 2), (&apos;test&apos;, 1)]</span><br><span class="line">    conf = &#123;</span><br><span class="line">        &apos;relative_path&apos;: &apos;/your/path/pyspark-project-template/src/jobs&apos;,</span><br><span class="line">        &apos;words_file_path&apos;: &apos;/word_count/resources/word_count.csv&apos;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    assert expected_results == run(spark_session, conf)</span><br></pre></td></tr></table></figure>
<p>我们需要从<strong>src</strong>模块导入我们想要测试的函数。这里更有趣的部分是我们如何进行<strong>test_word_count_run。</strong>我们可以看到没有初始化的spark会话，我们只是在测试中将其作为参数接收。这要归功于<strong>pytest-spark</strong>模块，因此我们可以专注于编写测试，而不是编写样板代码。<br>接下来让我们讨论一下代码覆盖率。我们怎么知道我们是否编写了足够的单元测试？很简单，我们运行测试覆盖工具，告诉我们尚未测试的代码。对于python，我们可以使用<strong>pytest-cov</strong>模块。要使用代码覆盖率运行所有测试，我们必须运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytest --cov=src test/jobs/</span><br></pre></td></tr></table></figure>
<p>where  <strong>- cov</strong> flag告诉pytest在哪里检查覆盖范围。<br>测试覆盖率结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---------- coverage: platform darwin, python 3.7.2-final-0 -----------</span><br><span class="line">Name                              Stmts   Miss  Cover</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">src/__init__.py                       0      0   100%</span><br><span class="line">src/jobs/__init__.py                  0      0   100%</span><br><span class="line">src/jobs/pi/__init__.py              11      0   100%</span><br><span class="line">src/jobs/word_count/__init__.py       9      0   100%</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">TOTAL                                20      0   100%</span><br></pre></td></tr></table></figure>
<p>我们的测试覆盖率是100％，但是等一下，缺少一个文件！为什么<strong>main.py</strong>没有在那里列出？<br>如果我们认为我们有不需要测试的python代码，我们可以将它从报告中排除。为此，我们需要在项目的根目录中创建一个  <strong>.coveragerc</strong>文件。对于此示例，它看起来像这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[run]</span><br><span class="line">omit = src/main.py</span><br></pre></td></tr></table></figure>
<p><a name="3760f981"></a></p>
<h3 id="第5步：运行静态代码分析"><a href="#第5步：运行静态代码分析" class="headerlink" title="第5步：运行静态代码分析"></a>第5步：运行静态代码分析</h3><p>很好，我们有一些代码，我们可以运行它，我们有良好的覆盖率的单元测试。我们做对了吗？还没！我们还需要确保按照python最佳实践编写易于阅读的代码。为此，我们必须使用名为<strong>flake8</strong>的python模块检查我们的代码<strong>。</strong><br>要运行它：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flake8 ./src</span><br></pre></td></tr></table></figure>
<p>它将分析<strong>src</strong>文件夹。如果我们有干净的代码，我们就不应该收到任何警告。但不，我们有一些问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flake8 ./src</span><br><span class="line">./src/jobs/pi/__init__.py:13:1: E302 expected 2 blank lines, found 1</span><br><span class="line">./src/jobs/pi/__init__.py:15:73: E231 missing whitespace after &apos;,&apos;</span><br><span class="line">./src/jobs/pi/__init__.py:15:80: E501 line too long (113 &gt; 79 characters)</span><br></pre></td></tr></table></figure>
<p>我们来看看代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from random import random</span><br><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUMBER_OF_STEPS_FACTOR = 100000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f(_):</span><br><span class="line">    x = random() * 2 - 1</span><br><span class="line">    y = random() * 2 - 1</span><br><span class="line">    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0</span><br><span class="line"></span><br><span class="line">def run(spark, config):</span><br><span class="line">    number_of_steps = config[&apos;partitions&apos;] * NUMBER_OF_STEPS_FACTOR</span><br><span class="line">    count = spark.sparkContext.parallelize(range(1, number_of_steps + 1),config[&apos;partitions&apos;]).map(f).reduce(add)</span><br><span class="line">    return 4.0 * count / number_of_steps</span><br></pre></td></tr></table></figure>
<p>我们可以看到在第<strong>13</strong>行我们有一个<strong>E302</strong>警告<strong>。</strong>这意味着我们需要在两种方法之间增加一条线。然后是第<strong>15</strong>行的<strong>E231</strong>和<strong>E501</strong>。这一行的第一个警告告诉我们，我们需要在和之间留出一个额外的空间，第二个警告通知我们线路太长，而且很难读（我们可以’甚至在要点中完整地看到它！）。<code>**range(1, number_of_steps +1),**</code><strong> </strong><code>**config[**</code><strong> </strong><br>解决所有警告后，代码看起来更容易阅读：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from random import random</span><br><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUMBER_OF_STEPS_FACTOR = 100000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f(_):</span><br><span class="line">    x = random() * 2 - 1</span><br><span class="line">    y = random() * 2 - 1</span><br><span class="line">    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def run(spark, config):</span><br><span class="line">    number_of_steps = config[&apos;partitions&apos;] * NUMBER_OF_STEPS_FACTOR</span><br><span class="line">    count = spark.sparkContext\</span><br><span class="line">        .parallelize(range(1, number_of_steps + 1),</span><br><span class="line">                     config[&apos;partitions&apos;]).map(f).reduce(add)</span><br><span class="line">    return 4.0 * count / number_of_steps</span><br></pre></td></tr></table></figure>
<p><a name="a448168b"></a></p>
<h3 id="第6步：将所有内容与Makefile放在一起"><a href="#第6步：将所有内容与Makefile放在一起" class="headerlink" title="第6步：将所有内容与Makefile放在一起"></a>第6步：将所有内容与Makefile放在一起</h3><p>因为我们在终端中运行了一堆命令，所以在最后一步中我们将研究如何简化和自动执行此任务。<br>我们可以在项目的根目录中创建一个<strong>Makefile</strong>，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.DEFAULT_GOAL := run</span><br><span class="line">init:</span><br><span class="line"> pipenv --three install</span><br><span class="line"> pipenv shell</span><br><span class="line">analyze:</span><br><span class="line"> flake8 ./src</span><br><span class="line">run_tests:</span><br><span class="line"> pytest --cov=src test/jobs/</span><br><span class="line">run:</span><br><span class="line"> find . -name &apos;__pycache__&apos; | xargs rm -rf</span><br><span class="line"> rm -f jobs.zip</span><br><span class="line"> cd src/ &amp;&amp; zip -r ../jobs.zip jobs/</span><br><span class="line"> spark-submit --py-files jobs.zip src/main.py --job $(JOB_NAME) --res-path $(CONF_PATH)</span><br></pre></td></tr></table></figure></p>
<p>如果我们想要使用coverage运行测试，我们只需输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make run_tests</span><br></pre></td></tr></table></figure>
<p>如果我们想要运行<strong>pi</strong>工作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make run JOB_NAME=pi CONF_PATH=/your/path/pyspark-project-template/src/jobs</span><br></pre></td></tr></table></figure>
<p>这就是所有人！希望这个对你有帮助。<br>一如既往，代码存储在<a href="https://github.com/BogdanCojocar/medium-articles/tree/master/pyspark-project-template" target="_blank" rel="noopener">github上</a>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测/" itemprop="url">使用Spark Structured Streaming，XGBoost和Scala进行实时预测</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T11:06:19+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本文中，我们将讨论构建完整的机器学习管道。第一部分将侧重于在标准批处理模式下训练二元分类器，在第二部分中我们将进行一些实时预测。我们将使用来自<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">泰坦尼克号的</a>数据<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">：机器学习灾难</a>中的众多Kaggle比赛之一。在开始之前，请知道您应该熟悉<a href="https://www.tutorialspoint.com/scala/index.htm" target="_blank" rel="noopener">Scala</a>，<a href="https://spark.apache.org/" target="_blank" rel="noopener">Apache Spark</a>和<a href="http://xgboost.readthedocs.io/en/latest/get_started/" target="_blank" rel="noopener">Xgboost</a>。所有源代码也将在<a href="https://github.com/BogdanCojocar/medium-articles/tree/master/titanic_spark" target="_blank" rel="noopener">Github上提供</a>。很酷，现在让我们开始吧！<br><a name="d8b16075"></a></p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>我们将使用Spark中的ML管道训练XGBoost分类器。分类器将保存为输出，并将在Spark Structured Streaming实时应用程序中用于预测新的测试数据。<br><strong>第1步：启动spark会话</strong><br>我们正在创建一个将在本地运行的spark应用程序，并将使用与使用的核心一样多的线程<code>local[*]</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark  = SparkSession.builder()</span><br><span class="line">  .appName(&quot;Spark XGBOOST Titanic Training&quot;)</span><br><span class="line">  .master(&quot;local[*]&quot;)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>
<p><strong>第2步：定义架构</strong><br>接下来，我们定义从csv读取的数据的模式。这通常比让火花推断模式更好，因为它消耗的资源更少，我们可以完全控制字段。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"PassengerId"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Survival"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Pclass"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Name"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Sex"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Age"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"SibSp"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Parch"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Ticket"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Fare"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Cabin"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Embarked"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br></pre></td></tr></table></figure>
<p><strong>第3步：读取数据</strong><br>我们把csv读成a <code>DataFrame</code>，确保我们提到我们有一个标题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val df_raw = spark</span><br><span class="line">  .read</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .schema(schema)</span><br><span class="line">  .csv(filePath)</span><br></pre></td></tr></table></figure>
<p><strong>第4步：删除空值</strong><br>所有空值都替换为0.这不是理想的，但是对于本教程的目的，它是可以的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df = df_raw.na.fill(0)</span><br></pre></td></tr></table></figure>
<p><strong>第5步：将名义值转换为数字</strong><br>在浏览此步骤的代码之前，让我们简要介绍一些Spark ML概念。他们介绍了ML管道的概念，它是一组构建在其上的高级API <code>DataFrames</code>，可以更轻松地将多个算法组合到一个流程中。管道的主要元素是<code>Transformer</code>和<code>Estimator</code>。第一个可以表示可以将a <code>DataFrame</code>转换为另一个<code>DataFrame</code>的算法，而后者是可以适合a <code>DataFrame</code>来生成a 的算法<code>Transformer</code> 。<br>为了将名义值转换为数字值，我们需要<code>Transformer</code>为每列定义一个：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sexIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">"Sex"</span>)</span><br><span class="line">  .setOutputCol(<span class="string">"SexIndex"</span>)</span><br><span class="line">  .setHandleInvalid(<span class="string">"keep"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> cabinIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">"Cabin"</span>)</span><br><span class="line">  .setOutputCol(<span class="string">"CabinIndex"</span>)</span><br><span class="line">  .setHandleInvalid(<span class="string">"keep"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> embarkedIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">"Embarked"</span>)</span><br><span class="line">  .setOutputCol(<span class="string">"EmbarkedIndex"</span>)</span><br><span class="line">  .setHandleInvalid(<span class="string">"keep"</span>)</span><br></pre></td></tr></table></figure>
<p>我们正在使用它<code>StringIndexer</code>来转换价值观。对于每个<code>Transformer</code>我们定义的输入列和输出列将包含修改后的值。<br><strong>步骤6：将列组合成特征向量</strong><br>我们将使用另一个<code>Transformer</code>将XGBoost分类中使用的列组合<code>Estimator</code>成一个向量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> vectorAssembler = <span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">  .setInputCols(<span class="type">Array</span>(<span class="string">"Pclass"</span>, <span class="string">"SexIndex"</span>, <span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"Fare"</span>, <span class="string">"CabinIndex"</span>, <span class="string">"EmbarkedIndex"</span>))</span><br><span class="line">  .setOutputCol(<span class="string">"features"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>第7步：添加XGBoost估算器</strong><br>定义<code>Estimator</code>它将产生模型。可以在地图中定义估计器的设置。我们还可以设置功能和标签列：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val xgbEstimator = new XGBoostEstimator(Map[String, Any](&quot;num_rounds&quot; -&gt; 100))</span><br><span class="line">  .setFeaturesCol(&quot;features&quot;)</span><br><span class="line">  .setLabelCol(&quot;Survival&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>第8步：构建管道和分类器</strong><br>在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作的顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val pipeline = new Pipeline().setStages(Array(sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgbEstimator))</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552878673454-f586d9ed-0fba-4140-b77d-605cc73a652d.png#align=left&amp;display=inline&amp;height=105&amp;name=1_5GXouKuoLYmdTCgu1onrzw.png&amp;originHeight=141&amp;originWidth=1000&amp;size=44837&amp;status=done&amp;width=746" alt="1_5GXouKuoLYmdTCgu1onrzw.png"><br>输入<code>DataFrame</code>将被多次转换，最终将生成使用我们的数据训练的模型。我们将保存输出，以便在第二个实时应用程序中使用它。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cvModel = pipeline.fit(df)</span><br><span class="line">cvModel.write.overwrite.save(modelPath)</span><br></pre></td></tr></table></figure>
<p><a name="fbee26a1"></a></p>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p>我们将使用Spark Structured Streaming来基本传输文件中的数据。在现实世界的应用程序中，我们从专用的分布式队列（例如Apache Kafka或AWS Kinesis）读取数据，但是对于此演示，我们将只使用一个简单的文件。<br>简要描述<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources" target="_blank" rel="noopener">Spark Structured Streaming</a>是一个构建在Spark SQL之上的流处理引擎。它使用相同的概念，<code>DataFrames</code>数据存储在一个无限制的表中，当数据流入时，该表随着新行的增长而增长。<br><strong>第1步：创建输入读取流</strong><br>我们再次创建一个spark会话并定义数据的架构。请注意，测试csv不包含标签<code>Survival</code> 。最后我们可以创建输入流<code>DataFrame,</code> <code>df</code>。输入路径必须是我们存储csv文件的目录。它可以包含一个或多个具有相同模式的文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">  .appName(<span class="string">"Spark Structured Streaming XGBOOST"</span>)</span><br><span class="line">  .master(<span class="string">"local[*]"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"PassengerId"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Pclass"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Name"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Sex"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Age"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"SibSp"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Parch"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Ticket"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Fare"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Cabin"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Embarked"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = spark</span><br><span class="line">    .readStream</span><br><span class="line">    .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">    .schema(schema)</span><br><span class="line">    .csv(fileDir)</span><br></pre></td></tr></table></figure>
<p><strong>第2步：加载XGBoost模型</strong><br>在对象中，<code>XGBoostModel</code>我们加载预先训练的模型，该模型将应用于我们在流中读取的每一批新行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">XGBoostModel</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> modelPath = <span class="string">"your_path"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> model = <span class="type">PipelineModel</span>.read.load(modelPath)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>(df: <span class="type">DataFrame</span>) = &#123;</span><br><span class="line">    <span class="comment">// replace nan values with 0</span></span><br><span class="line">    <span class="keyword">val</span> df_clean = df.na.fill(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// run the model on new data</span></span><br><span class="line">    <span class="keyword">val</span> result = model.transform(df_clean)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// display the results</span></span><br><span class="line">    result.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>第3步：定义自定义ML接收器</strong><br>为了能够将我们的分类器应用于新数据，我们需要创建一个新的接收器（流和输出之间的接口，在我们的例子中是XGBoost模型）。为此，我们需要一个自定义接收器（<code>MLSink</code>），一个抽象接收器提供器（<code>MLSinkProvider</code>）和provider（）的实现<code>XGBoostMLSinkProvider</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MLSinkProvider</span> <span class="keyword">extends</span> <span class="title">StreamSinkProvider</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(df: <span class="type">DataFrame</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createSink</span></span>(</span><br><span class="line">                  sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">                  parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">                  partitionColumns: <span class="type">Seq</span>[<span class="type">String</span>],</span><br><span class="line">                  outputMode: <span class="type">OutputMode</span>): <span class="type">MLSink</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MLSink</span>(process)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MLSink</span>(<span class="params">process: <span class="type">DataFrame</span> =&gt; <span class="type">Unit</span></span>) <span class="keyword">extends</span> <span class="title">Sink</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatch</span></span>(batchId: <span class="type">Long</span>, data: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    process(data)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XGBoostMLSinkProvider</span> <span class="keyword">extends</span> <span class="title">MLSinkProvider</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(df: <span class="type">DataFrame</span>) &#123;</span><br><span class="line">    <span class="type">XGBoostModel</span>.transform(df)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>第4步：在我们的自定义接收器中写入数据</strong><br>最后一步是定义一个将数据写入自定义接收器的查询。还必须定义检查点位置，以便应用程序“记住”在发生故障时在流中读取的最新行。如果我们运行程序，每个新批次的数据将显示在控制台上，同时包含预测的标签。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">"titanic.XGBoostMLSinkProvider"</span>)</span><br><span class="line">  .queryName(<span class="string">"XGBoostQuery"</span>)</span><br><span class="line">  .option(<span class="string">"checkpointLocation"</span>, checkpoint_location)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试/" itemprop="url">PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T10:32:11+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本教程中，我们将讨论使用标准机器学习管道集成PySpark和XGBoost。我们将使用来自<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">泰坦尼克号的</a>数据<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">：机器学习灾难</a>中的众多Kaggle比赛之一。在开始之前，请知道您应该熟悉<a href="https://spark.apache.org/" target="_blank" rel="noopener">Apache Spark</a>和<a href="http://xgboost.readthedocs.io/en/latest/get_started/" target="_blank" rel="noopener">Xgboost</a>以及Python。本教程中使用的代码可以在<a href="https://github.com/BogdanCojocar/medium-articles/blob/master/titanic_xgboost/titanic_xgboost.ipynb" target="_blank" rel="noopener">github</a>上的Jupyther笔记本中找到。<br><a name="fa501ee4"></a></p>
<h4 id="第1步：下载或构建XGBoost-jar"><a href="#第1步：下载或构建XGBoost-jar" class="headerlink" title="第1步：下载或构建XGBoost jar"></a>第1步：下载或构建XGBoost jar</h4><p>python代码需要两个scala jar依赖项才能工作。您可以直接从maven下载它们：</p>
<ul>
<li><a href="https://mvnrepository.com/artifact/ml.dmlc/xgboost4j/0.72" target="_blank" rel="noopener">xgboost4j</a></li>
<li><a href="https://mvnrepository.com/artifact/ml.dmlc/xgboost4j-spark/0.72" target="_blank" rel="noopener">xgboost4j火花</a></li>
</ul>
<p>如果您希望自己构建它们，可以从我之前的<a href="https://medium.com/@bogdan.cojocar/how-to-make-xgboost-available-in-the-spark-notebook-de14e425c948" target="_blank" rel="noopener">教程中</a>找到如何进行构建。<br><a name="059757b0"></a></p>
<h4 id="第2步：下载XGBoost-python包装器"><a href="#第2步：下载XGBoost-python包装器" class="headerlink" title="第2步：下载XGBoost python包装器"></a>第2步：下载XGBoost python包装器</h4><p>您可以从<a href="https://github.com/dmlc/xgboost/files/2161553/sparkxgb.zip" target="_blank" rel="noopener">此处</a>下载PySpark XGBoost代码。这是我们要编写的部分和XGBoost scala实现之间的接口。我们将在本教程后面的代码中看到它如何集成。<br><a name="f8c06a61"></a></p>
<h4 id="第3步：启动一个新的Jupyter笔记本"><a href="#第3步：启动一个新的Jupyter笔记本" class="headerlink" title="第3步：启动一个新的Jupyter笔记本"></a>第3步：启动一个新的Jupyter笔记本</h4><p>我们将开始一个新的笔记本，以便能够编写我们的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p><a name="cdde53ac"></a></p>
<h4 id="第4步：将自定义XGBoost-jar添加到Spark应用程序"><a href="#第4步：将自定义XGBoost-jar添加到Spark应用程序" class="headerlink" title="第4步：将自定义XGBoost jar添加到Spark应用程序"></a>第4步：将自定义XGBoost jar添加到Spark应用程序</h4><p>在开始Spark之前，我们需要添加我们之前下载的jar。我们可以使用<code>--jars</code>标志来做到这一点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os </span><br><span class="line">os.environ [&apos;PYSPARK_SUBMIT_ARGS&apos;] =&apos; -  jars xgboost4j-spark-0.72.jar，xgboost4j-0.72.jar pyspark-shell&apos;</span><br></pre></td></tr></table></figure>
<p><a name="13d9ee40"></a></p>
<h4 id="第5步：将PySpark集成到Jupyther笔记本中"><a href="#第5步：将PySpark集成到Jupyther笔记本中" class="headerlink" title="第5步：将PySpark集成到Jupyther笔记本中"></a>第5步：将PySpark集成到Jupyther笔记本中</h4><p>使PySpark可用的最简单方法是使用该<code>[findspark](https://github.com/minrk/findspark)</code>软件包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import findspark </span><br><span class="line">findspark.init（）</span><br></pre></td></tr></table></figure>
<p><a name="dd505c79"></a></p>
<h4 id="第6步：启动spark会话"><a href="#第6步：启动spark会话" class="headerlink" title="第6步：启动spark会话"></a>第6步：启动spark会话</h4><p>我们现在准备开始火花会议。我们正在创建一个将在本地运行的spark应用程序，并将使用与使用的核心一样多的线程<code>local[*]</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(&quot;PySpark XGBOOST Titanic&quot;)\</span><br><span class="line">        .master(&quot;local[*]&quot;)\</span><br><span class="line">        .getOrCreate()</span><br></pre></td></tr></table></figure>
<p><a name="47ff7d29"></a></p>
<h4 id="第7步：添加PySpark-XGBoost包装器代码"><a href="#第7步：添加PySpark-XGBoost包装器代码" class="headerlink" title="第7步：添加PySpark XGBoost包装器代码"></a>第7步：添加PySpark XGBoost包装器代码</h4><p>正如我们现在有了spark会话，我们可以添加先前下载的包装器代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sparkContext.addPyFile(&quot;YOUR_PATH/sparkxgb.zip&quot;)</span><br></pre></td></tr></table></figure>
<p><a name="7f1a0f62"></a></p>
<h4 id="第8步：定义架构"><a href="#第8步：定义架构" class="headerlink" title="第8步：定义架构"></a>第8步：定义架构</h4><p>接下来，我们定义从csv读取的数据的模式。这通常比让火花推断模式更好，因为它消耗的资源更少，我们可以完全控制字段。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">schema = StructType(</span><br><span class="line">  [StructField(&quot;PassengerId&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Survival&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Pclass&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Name&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Sex&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Age&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;SibSp&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Parch&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Ticket&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Fare&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Cabin&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Embarked&quot;, StringType())</span><br><span class="line">  ])</span><br></pre></td></tr></table></figure>
<p><a name="44ee9a9a"></a></p>
<h4 id="步骤9：将csv数据读入数据帧"><a href="#步骤9：将csv数据读入数据帧" class="headerlink" title="步骤9：将csv数据读入数据帧"></a>步骤9：将csv数据读入数据帧</h4><p>我们将csv读入a <code>DataFrame</code>，确保我们提到我们有一个标题，我们也<code>null</code>用0 替换值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df_raw = spark\</span><br><span class="line">  .read\</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)\</span><br><span class="line">  .schema(schema)\</span><br><span class="line">  .csv(&quot;YOUR_PATH/train.csv&quot;)</span><br><span class="line">df = df_raw.na.fill(0)</span><br></pre></td></tr></table></figure>
<p><a name="f50eacdc"></a></p>
<h4 id="步骤10：C-将标称值转换为数字"><a href="#步骤10：C-将标称值转换为数字" class="headerlink" title="步骤10：C 将标称值转换为数字"></a>步骤10：C <strong>将标称值转换为数字</strong></h4><p>在浏览此步骤的代码之前，让我们简要介绍一些Spark ML概念。他们介绍了ML管道的概念，它是一组构建在其上的高级API <code>DataFrames</code>，可以更轻松地将多个算法组合到一个流程中。管道的主要元素是<code>Transformer</code>和<code>Estimator</code>。第一个可以表示可以将a <code>DataFrame</code>转换为另一个<code>DataFrame</code>的算法，而后者是可以适合a <code>DataFrame</code>来生成a 的算法<code>Transformer</code> 。<br>为了将名义值转换为数字值，我们需要<code>Transformer</code>为每列定义一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sexIndexer = StringIndexer()\</span><br><span class="line">  .setInputCol(&quot;Sex&quot;)\</span><br><span class="line">  .setOutputCol(&quot;SexIndex&quot;)\</span><br><span class="line">  .setHandleInvalid(&quot;keep&quot;)</span><br><span class="line">    </span><br><span class="line">cabinIndexer = StringIndexer()\</span><br><span class="line">  .setInputCol(&quot;Cabin&quot;)\</span><br><span class="line">  .setOutputCol(&quot;CabinIndex&quot;)\</span><br><span class="line">  .setHandleInvalid(&quot;keep&quot;)</span><br><span class="line">    </span><br><span class="line">embarkedIndexer = StringIndexer()\</span><br><span class="line">  .setInputCol(&quot;Embarked&quot;)\</span><br><span class="line">  .setOutputCol(&quot;EmbarkedIndex&quot;)\</span><br><span class="line">  .setHandleInvalid(&quot;keep&quot;)</span><br></pre></td></tr></table></figure>
<p>我们正在使用它<code>StringIndexer</code>来转换价值观。对于每个<code>Transformer</code>我们定义的输入列和输出列将包含修改后的值。<br><a name="fd3cbfb0"></a></p>
<h4 id="步骤11：将列组合成特征向量"><a href="#步骤11：将列组合成特征向量" class="headerlink" title="步骤11：将列组合成特征向量"></a>步骤11：<strong>将列组合成特征向量</strong></h4><p>我们将使用另一个<code>Transformer</code>将XGBoost分类中使用的列组合<code>Estimator</code>成一个向量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vectorAssembler = VectorAssembler()\</span><br><span class="line">  .setInputCols([&quot;Pclass&quot;, &quot;SexIndex&quot;, &quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;, &quot;CabinIndex&quot;, &quot;EmbarkedIndex&quot;])\</span><br><span class="line">  .setOutputCol(&quot;features&quot;)</span><br></pre></td></tr></table></figure>
<p><a name="220de1c1"></a></p>
<h4 id="第12步：定义XGBoostEstimator"><a href="#第12步：定义XGBoostEstimator" class="headerlink" title="第12步：定义XGBoostEstimator"></a>第12步：定义XGBoostEstimator</h4><p>在这一步中，我们定义了<code>Estimator</code>将产生模型的东西。这里使用的大多数参数都是默认的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">xgboost = XGBoostEstimator(</span><br><span class="line">    featuresCol=&quot;features&quot;, </span><br><span class="line">    labelCol=&quot;Survival&quot;, </span><br><span class="line">    predictionCol=&quot;prediction&quot;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们只定义<code>feature, label</code>（必须匹配来自的列<code>DataFrame</code>）和<code>prediction</code>包含分类器输出的新列。<br><a name="07d0e7ba"></a></p>
<h4 id="步骤13：建立管道和分类器"><a href="#步骤13：建立管道和分类器" class="headerlink" title="步骤13：建立管道和分类器"></a>步骤13：<strong>建立管道和分类器</strong></h4><p>在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作的顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost])</span><br></pre></td></tr></table></figure>
<p>输入<code>DataFrame</code>将被多次转换，最终将生成使用我们的数据训练的模型。<br><a name="4e3e3575"></a></p>
<h4 id="步骤14：训练模型并预测新的测试数据"><a href="#步骤14：训练模型并预测新的测试数据" class="headerlink" title="步骤14：训练模型并预测新的测试数据"></a>步骤14：训练模型并预测新的测试数据</h4><p>我们首先将数据分成火车和测试，然后我们将模型与火车数据拟合，最后我们看到我们为每位乘客获得的预测：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)</span><br><span class="line">model = pipeline.fit(trainDF)</span><br><span class="line">model.transform(testDF).select(col(&quot;PassengerId&quot;), col(&quot;prediction&quot;)).show()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhos</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
