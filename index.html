<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="A+">
<meta property="og:url" content="http://zhos.me/index.html">
<meta property="og:site_name" content="A+">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A+">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zhos.me/">





  <title>A+</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A+</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">武德</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/04/03/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/03/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-03T10:27:06+08:00">
                2019-04-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/转移学习：重新启动Inception V3以进行自定义图像分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/转移学习：重新启动Inception V3以进行自定义图像分类/" itemprop="url">转移学习：重新启动Inception V3以进行自定义图像分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T18:00:40+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>让我们通过将现有的图像分类器（Inception V3）调整为自定义任务来体验转移学习的强大功能：对产品图像进行分类，以帮助食品和杂货零售商减少仓库和零售店库存管理过程中的人力。<br>这项工作的源代码可以在我下面的GitHub存储库中找到。<br><a href="https://github.com/wisdal/Image-classification-transfer-learning" target="_blank" rel="noopener">https://github.com/wisdal/Image-classification-transfer-learning</a><br>使用的工具：TensorFlow v1.1，Python 3.4，Jupyter。<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903355906-1736cd25-8311-4055-a6bd-aa237d022793.png#align=left&amp;display=inline&amp;height=436&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=584&amp;originWidth=1000&amp;size=759418&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p><br>深度神经网络的应用实际上是在滚动。无论是医疗保健，运输还是零售，各行各业的公司都对投资构建智能解决方案感到兴奋。同时，让我们希望人类的智慧仍然无可争议:)<br><a name="9dd10fb0"></a></p>
<h3 id="趋势AI文章："><a href="#趋势AI文章：" class="headerlink" title="趋势AI文章："></a>趋势AI文章：</h3><blockquote>
<p><a href="https://chatbotslife.com/how-neural-networks-work-ff4c7ad371f7" target="_blank" rel="noopener">1.神经网络如何工作</a>&gt; <a href="https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32" target="_blank" rel="noopener">2. ResNets，HighwayNets和DenseNets，哦，我的！</a>&gt; <a href="https://chatbotslife.com/machine-learning-for-dummies-part-1-dbaca076ec07" target="_blank" rel="noopener">3.机器学习傻瓜</a><br>在实际情况下，强大的图像分类器等解决方案可以帮助公司跟踪货架库存，对产品进行分类，记录产品量，从专用设备（无人机？机器人？）实时捕获的原始产品图像。当然，能够识别产品并从给定的图片预测其类别是交易的一部分，这就是这个实验的全部内容：我们将培训机器人从图像中分类食品和杂货产品。</p>
</blockquote>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903381892-b439e8f9-e1d2-49ed-9eb1-02be65c96a4f.png#align=left&amp;display=inline&amp;height=390&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=523&amp;originWidth=1000&amp;size=593236&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>“这是一项轻松的任务！”，我们认为是人类，但计算机程序并不一定如此。无论如何，这个假设甚至都不适用于所有类别的人类; 一个5岁的孩子是一个完美的反例！当我们放大时，一切都是为了在您的生活中看到足够的产品图像，您可以从给定的图像轻松识别您之前看过的任何产品。<br>当我们使用现有标记数据训练模型时，我们尝试将这种经验概念转移到模型，以便学习如何准确地区分训练数据集中存在的不同类别的数据。从这个意义上说，我们使用<strong>人工神经网络</strong>，它只不过是模仿人类大脑的实际运作方式。使用这些算法构建模型的知识稍后将在未标记的观察上进行测试。在我们的示例中，模型将基于其先前学习的内容标记输入图像，因此通常分配给该任务的名称<strong>监督学习 </strong>。<br>谈到性能，已经注意到，在大多数监督学习的情况下，训练有素的模型往往比人类提供更好的准确性。在这个实际任务中，您会惊讶地发现即使在困难的条件下（模糊的图像，质量差的图像等），我们的算法也非常优于人类。<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903421984-d099ab43-1a63-4ccb-9833-783d03b242b8.png#align=left&amp;display=inline&amp;height=381&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=420&amp;originWidth=823&amp;size=329410&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"><br><br><br>我们有一个来自hackerearth的产品图像数据集可以<a href="https://he-s3.s3.amazonaws.com/media/hackathon/deep-learning-challenge-1/identify-the-objects/a0409a00-8-dataset_dp.zip" target="_blank" rel="noopener">在这里</a>下载。我们应该如何进行，为什么我们使用转学习？</p>
<hr>
<p><a name="c0139b0c"></a></p>
<h3 id="为何转学？"><a href="#为何转学？" class="headerlink" title="为何转学？"></a><strong>为何转学？</strong></h3><p>当我们考虑对图像进行分类时，我们常常选择从头开始构建我们的模型以获得最佳匹配。这是一个选项，但构建自定义深度学习模型需要大量的计算资源和大量的培训数据。此外，已经存在的模型在分类来自各种类别的图像时表现得相当好。您可能听说过<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>及其大视觉识别挑战。在这个计算机视觉挑战中，模型试图将大量图像分类为1000个类，如“斑马”，“达尔马提亚”和“洗碗机”。<a href="https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html" target="_blank" rel="noopener">Inception V3</a>是Google Brain Team为此而建立的模型。毋庸置疑，该模型表现非常出色。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903675886-eeb54f5e-2e91-4943-9a2c-afc8761e6979.png#align=left&amp;display=inline&amp;height=278&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=373&amp;originWidth=1000&amp;size=109931&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>那么，我们可以利用这个模型的存在来进行像现在这样的自定义图像分类任务吗？嗯，这个概念有一个名字：<a href="https://en.wikipedia.org/wiki/Transfer_learning" target="_blank" rel="noopener"><strong>转移学习</strong></a>。它可能不如从头开始的完整培训那么高效，但对于许多应用来说都是惊人的有效。通过修改现有的丰富深度学习模型，它可以显着减少训练数据和时间。<br><a name="d2db47fa"></a></p>
<h3 id="为什么会这样"><a href="#为什么会这样" class="headerlink" title="为什么会这样"></a><strong>为什么会这样</strong></h3><p>在神经网络中，神经元被分层组织。不同的层可以对其输入执行不同类型的转换。信号可能在多次遍历各层之后从第一层（输入）传播到最后一层（输出）。作为最后一个隐藏层，“瓶颈”具有足够的汇总信息，以提供执行实际分类任务的下一层。<br>在<a href="https://github.com/wisdal/image-classification-transfer-learning/blob/master/retrain.py" target="_blank" rel="noopener">retrain.py</a>脚本中，我们删除旧的顶层，并在我们下载的图片上训练一个新的顶层。<br>我们的最后一层再训练可以用于新类的原因是，结果表明，区分ImageNet中所有1000个类所需的信息通常也可用于区分新类型的对象。<br>我们现在弄脏手！</p>
<hr>
<p><a name="81ef9f80"></a></p>
<h3 id="第1步：预处理图像"><a href="#第1步：预处理图像" class="headerlink" title="第1步：预处理图像"></a><strong>第1步：预处理图像</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">label_counts = train.label.value_counts（）</span><br><span class="line">plt.figure（figsize =（12,6））</span><br><span class="line">sns.barplot（label_counts.index，label_counts.values，alpha = 0.9）</span><br><span class="line">plt.xticks（rotation =&apos;vertical&apos;）</span><br><span class="line">plt.xlabel （&apos;Image Labels&apos;，fontsize = 12）</span><br><span class="line">plt.ylabel（&apos;Counts&apos;，fontsize = 12）</span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552903798892-d0219c97-9167-48dc-a792-2f0f3a65e604.png#align=left&amp;display=inline&amp;height=419&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=419&amp;originWidth=721&amp;size=14783&amp;status=done&amp;width=721" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"><br>假设您已经下载了<a href="https://he-s3.s3.amazonaws.com/media/hackathon/deep-learning-challenge-1/identify-the-objects/a0409a00-8-dataset_dp.zip" target="_blank" rel="noopener">数据集</a>，您会发现它附带了一个我们需要正确设置的“train”文件夹。我们的目标是将每个图像放在代表其类别的子文件夹中。最后，我们应该有x个子文件夹，x是不同类别的数量。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552904730643-3f6be018-2297-4b30-af9e-7dd590e648ac.png#align=left&amp;display=inline&amp;height=426&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=481&amp;originWidth=843&amp;size=35297&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>为了这个预处理目的，我为您提供了<a href="https://github.com/wisdal/image-classification-transfer-learning/blob/master/pre_process.ipynb" target="_blank" rel="noopener">pre_process.ipynb</a>笔记本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">for img in tqdm(train.values):</span><br><span class="line">    filename=img[0]</span><br><span class="line">    label=img[1]</span><br><span class="line">    src=os.path.join(data_root,&apos;train_img&apos;,filename+&apos;.png&apos;)</span><br><span class="line">    label_dir=os.path.join(data_root,&apos;train&apos;,label)</span><br><span class="line">    dest=os.path.join(label_dir,filename+&apos;.jpg&apos;)</span><br><span class="line">    im=Image.open(src)</span><br><span class="line">    rgb_im=im.convert(&apos;RGB&apos;)</span><br><span class="line">    if not os.path.exists(label_dir):</span><br><span class="line">        os.makedirs(label_dir)</span><br><span class="line">    rgb_im.save(dest)  </span><br><span class="line">    if not os.path.exists(os.path.join(data_root,&apos;train2&apos;,label)):</span><br><span class="line">        os.makedirs(os.path.join(data_root,&apos;train2&apos;,label))</span><br><span class="line">    rgb_im.save(os.path.join(data_root,&apos;train2&apos;,label,filename+&apos;.jpg&apos;))</span><br></pre></td></tr></table></figure>
<p>笔记本不仅仅是配置图像子文件夹，所以一定要检查它。<br>因为我们的数据集带有<strong>25个独特的标签，</strong>而我们<strong>只有3215个训练图像</strong>，我们需要增加数据以防止我们的模型过度拟合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">        rotation_range=40,</span><br><span class="line">        width_shift_range=0.2,</span><br><span class="line">        height_shift_range=0.2,</span><br><span class="line">        shear_range=0.2,</span><br><span class="line">        zoom_range=0.2,</span><br><span class="line">        horizontal_flip=True,</span><br><span class="line">        fill_mode=&apos;nearest&apos;)</span><br><span class="line"></span><br><span class="line">class_size=600</span><br><span class="line"></span><br><span class="line">src_train_dir=os.path.join(data_root,&apos;train&apos;)</span><br><span class="line">dest_train_dir=os.path.join(data_root,&apos;train2&apos;)</span><br><span class="line">it=0</span><br><span class="line">for count in label_counts.values:</span><br><span class="line">    #nb of generations per image for this class label in order to make it size ~= class_size</span><br><span class="line">    ratio=math.floor(class_size/count)-1</span><br><span class="line">    print(count,count*(ratio+1))</span><br><span class="line">    dest_lab_dir=os.path.join(dest_train_dir,label_counts.index[it])</span><br><span class="line">    src_lab_dir=os.path.join(src_train_dir,label_counts.index[it])</span><br><span class="line">    if not os.path.exists(dest_lab_dir):</span><br><span class="line">        os.makedirs(dest_lab_dir)</span><br><span class="line">    for file in os.listdir(src_lab_dir):</span><br><span class="line">        img=load_img(os.path.join(src_lab_dir,file))</span><br><span class="line">        #img.save(os.path.join(dest_lab_dir,file))</span><br><span class="line">        x=img_to_array(img) </span><br><span class="line">        x=x.reshape((1,) + x.shape)</span><br><span class="line">        i=0</span><br><span class="line">        for batch in datagen.flow(x, batch_size=1,save_to_dir=dest_lab_dir, save_format=&apos;jpg&apos;):</span><br><span class="line">            i+=1</span><br><span class="line">            if i &gt; ratio:</span><br><span class="line">                break </span><br><span class="line">    it=it+1</span><br></pre></td></tr></table></figure>
<p><a name="a57a7ff4"></a></p>
<h3 id="第2步：重新训练瓶颈并微调模型"><a href="#第2步：重新训练瓶颈并微调模型" class="headerlink" title="第2步：重新训练瓶颈并微调模型"></a><strong>第2步：重新训练瓶颈并微调模型</strong></h3><p>由谷歌提供，我们立即开始使用retrain.py脚本。该脚本默认下载Inception V3 <a href="http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz" target="_blank" rel="noopener">预训练模型</a>。<br>重新训练脚本是我们算法的核心组件，也是使用从初始v3开始的转移学习的任何自定义图像分类任务的核心组件。它是由TensorFlow作者自己设计的，用于此特定目的（自定义图像分类）。<br><a name="6af7ed82"></a></p>
<h4 id="脚本的作用："><a href="#脚本的作用：" class="headerlink" title="脚本的作用："></a>脚本的作用：</h4><p>它训练一个新的顶层（瓶颈），可以识别特定类别的图像。顶层接收每个图像的2048维向量作为输入。然后在该表示之上训练softmax层。假设softmax层包含N个标签，这对应于学习与学习的偏差和权重相对应的N + 2048 <em> N（或1001 </em> N）个模型参数。<br>该脚本完全可以自定义，这里是可配置的参数列表：</p>
<ul>
<li><strong>image_dir</strong>：标记图像文件夹的路径。幸运的是，我们在预处理步骤中正确设置了它。</li>
<li><strong>output_graph，intermediate_output_graphs_dir，output_labels等</strong>：保存输出文件的位置。</li>
<li><strong>失真功能：</strong>我的最爱。仅此功能就值得整整一段。您可能已经注意到我们的训练集中的图像是完美的（清晰，高质量，明确）但不幸的是，在生产中并非总是如此。该算法可能会在部署后遇到，模糊图像，昏暗的图像等。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552904870948-60b8d20b-8035-44db-b9b4-909606809a52.png#align=left&amp;display=inline&amp;height=617&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=724&amp;originWidth=876&amp;size=572978&amp;status=done&amp;width=746" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"></p>
<p>我们的算法应该足够智能，以捕捉这些图像代表相同的事情，并不是那么明显（这只是一个小例子）</p>
<ul>
<li>[…]这就是失真特征的全部内容。我们有意地在训练过程中随机变换图像（大小，颜色，方向等）以使机器人习惯于不良图像，以避免在这种情况下失去预测准确性。</li>
<li><strong>how_many_training_steps</strong>：时代数。</li>
<li><strong>学习率</strong>。</li>
<li>…</li>
</ul>
<p>您可以随意使用这些参数。<strong>学习率</strong>，<strong>nb。时期</strong>等是确定性参数。使用它们来微调您的模型并记住您可以随时使用TensorBoard来可视化您的训练结果。<br>您可以从一开始就获得约85％的准确度（零微调）。<br><a name="1fdd8e7a"></a></p>
<h3 id="第3步：在看不见的记录上测试模型"><a href="#第3步：在看不见的记录上测试模型" class="headerlink" title="第3步：在看不见的记录上测试模型"></a>第3步：在看不见的记录上测试模型</h3><p>这一步没什么可疯狂的。只是一个小脚本来测试在上一步中构建和保存的模型，在我们数据集的“test”文件夹中的图像上。<br>查看<a href="https://github.com/wisdal/Image-classification-transfer-learning/blob/master/test.ipynb" target="_blank" rel="noopener">测试</a>笔记本，了解需要完成的工作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def run_graph(src, labels, input_layer_name, output_layer_name,</span><br><span class="line">              num_top_predictions):</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">    i=0</span><br><span class="line">    #outfile=open(&apos;submit.txt&apos;,&apos;w&apos;)</span><br><span class="line">    #outfile.write(&apos;image_id, label \n&apos;)</span><br><span class="line">    for f in os.listdir(dest):</span><br><span class="line">        image_data=load_image(os.path.join(dest,test[i]+&apos;.jpg&apos;))</span><br><span class="line">        #image_data=load_image(os.path.join(src,f))</span><br><span class="line">        softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)</span><br><span class="line">        predictions, = sess.run(softmax_tensor, &#123;input_layer_name: image_data&#125;)</span><br><span class="line"></span><br><span class="line">        # Sort to show labels in order of confidence</span><br><span class="line">        top_k = predictions.argsort()[-num_top_predictions:][::-1]</span><br><span class="line">        for node_id in top_k:</span><br><span class="line">            human_string = labels[node_id]</span><br><span class="line">            score = predictions[node_id]</span><br><span class="line">            #print(&apos;%s (score = %.5f) %s , %s&apos; % (test[i], human_string))</span><br><span class="line">            print(&apos;%s, %s&apos; % (test[i], human_string))</span><br><span class="line">            #outfile.write(test[i]+&apos;, &apos;+human_string+&apos;\n&apos;)</span><br><span class="line">        i+=1</span><br><span class="line">    return 0</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552904928699-8cc5dd07-ebf5-4116-a3e3-5d4ece361238.png#align=left&amp;display=inline&amp;height=663&amp;name=1_OiM9iXjO4R8Vq_wCSfQZIg.png&amp;originHeight=663&amp;originWidth=664&amp;size=265517&amp;status=done&amp;width=664" alt="1_OiM9iXjO4R8Vq_wCSfQZIg.png"><br><a name="54bbba80"></a></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>而已！希望这篇文章对你有用。随意评论并提出改进建议。<br>我鼓励你试一试，并在评论中告诉我你能达到多少准确度。我很乐意收到你的来信。<br>正如我之前所说，你肯定能够获得<strong>超过85％</strong>的基准精度。剩下的就是微调！在我的情况下，我的最终模型在测试装置上的准确性让我感到震惊，考虑到需要的工作量很少。很好地理解事情的工作方式有时候会有所帮助:)。我认为该项目是任何想要尝试图像失真或超参数调整的人的良好基础。这就是为我增加了更多的百分点。<br>请随意查看我在<a href="https://github.com/wisdal/Image-classification-transfer-learning" target="_blank" rel="noopener">GitHub上的</a>代码。<br>资源：<a href="https://www.tensorflow.org/tutorials/image_recognition" target="_blank" rel="noopener">tensorflow.org/tutorials/image_recognition</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/Spark上的实际Python工作负载：独立群集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/Spark上的实际Python工作负载：独立群集/" itemprop="url">Spark上的实际Python工作负载：独立群集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T17:45:16+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于在Spark上运行Python有无数文章和论坛帖子，但大多数人认为要提交的工作包含在一个.py文件中：<code>spark-submit wordcount.py</code> - 完成！<br>如果你的Python程序不仅仅是一个脚本怎么办？也许它为Spark生成动态SQL来执行，或使用Spark的输出刷新模型。随着您的Python代码变得更像一个应用程序（具有目录结构，配置文件和库依赖项），将其提交给Spark需要更多考虑。<br>以下是我最近考虑使用Spark 2.3将一个这样的Python应用程序用于生产时的替代方案。第一篇文章重点介绍Spark独立集群。另一篇文章介绍了EMR Spark（YARN）。<br>我远不是Spark的权威，更不用说Python了。我的决定试图平衡正确性和易于部署，以及应用程序对群集的限制。让我知道你的想法。<br><a name="9dd10fb0"></a></p>
<h3 id="趋势AI文章："><a href="#趋势AI文章：" class="headerlink" title="趋势AI文章："></a>趋势AI文章：</h3><blockquote>
<p><a href="https://becominghuman.ai/predicting-buying-behavior-using-machine-learning-a-case-study-on-sales-prospecting-part-i-3bf455486e5d" target="_blank" rel="noopener">1.使用机器学习预测购买行为</a>&gt; <a href="https://becominghuman.ai/understanding-and-building-generative-adversarial-networks-gans-8de7c1dc0e25" target="_blank" rel="noopener">2.理解和构建生成对抗网络（GAN）</a>&gt; <a href="https://becominghuman.ai/building-a-django-post-face-detection-api-using-opencv-and-haar-cascades-dcf4e0e5f725" target="_blank" rel="noopener">3.使用OpenCV和Haar Cascades构建Django POST面部检测API</a>&gt; <a href="https://becominghuman.ai/learning-from-mistakes-with-hindsight-experience-replay-547fce2b3305" target="_blank" rel="noopener">4.通过Hindsight Experience Replay从错误中学习</a><br><a name="3cf6ae3c"></a></p>
</blockquote>
<h4 id="示例Python应用程序"><a href="#示例Python应用程序" class="headerlink" title="示例Python应用程序"></a>示例Python应用程序</h4><p>为了模拟完整的应用程序，下面的场景假定Python 3应用程序具有以下结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">project.py </span><br><span class="line">data / </span><br><span class="line">    data_source.py </span><br><span class="line">    data_source.ini</span><br></pre></td></tr></table></figure>
<p>_data_source.ini_包含各种配置参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[spark] </span><br><span class="line">app_name =myPySpark App </span><br><span class="line">master_url = spark：// sparkmaster：7077</span><br></pre></td></tr></table></figure>
<p>_data_source.py_是一个模块，负责在Spark中获取和处理数据，使用NumPy进行数学转换，并将Pandas数据帧返回给客户端。依赖关系：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkConf, SparkContext</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import StructType, StructField, FloatType</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import configparser</span><br></pre></td></tr></table></figure>
<p>它定义了一个创建和初始化的<em>DataSource</em>类……<code>SparkContext`</code>SparkSession`</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class DataSource:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        config = configparser.ConfigParser()</span><br><span class="line">        config.read(&apos;./data/data_source.ini&apos;)</span><br><span class="line">        master_url = config[&apos;spark&apos;][&apos;master_url&apos;]</span><br><span class="line">        app_name = config[&apos;spark&apos;][&apos;app_name&apos;]</span><br><span class="line">        conf = SparkConf().setAppName(app_name) \</span><br><span class="line">                          .setMaster(master_url)</span><br><span class="line">        self.sc = SparkContext(conf=conf)</span><br><span class="line">        self.spark = SparkSession.builder \</span><br><span class="line">                                 .config(conf=conf) \</span><br><span class="line">                                 .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>…和一个_get_data（）_方法：</p>
<ol>
<li>从NumPy正态分布创建RDD。</li>
<li>应用函数将每个元素的值加倍。</li>
<li>将RDD转换为Spark数据帧并在顶部定义临时视图。</li>
<li>应用Python UDF，使用SQL对每个dataframe元素的内容进行平方。</li>
<li>将结果作为Pandas数据帧返回给客户端。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def get_data(self, num_elements=1000) -&gt; pd.DataFrame:</span><br><span class="line">    mu, sigma = 2, 0.5</span><br><span class="line">    v = np.random.normal(mu, sigma, num_elements)</span><br><span class="line">    rdd1 = self.sc.parallelize(v)</span><br><span class="line">    def mult(x): return x * np.array([2])</span><br><span class="line">    rdd2 = rdd1.map(mult).map(lambda x: (float(x),))</span><br><span class="line">    schema = StructType([StructField(&quot;value&quot;, FloatType(), True)])</span><br><span class="line">    df1 = self.spark.createDataFrame(rdd2, schema)</span><br><span class="line">    df1.registerTempTable(&quot;test&quot;)</span><br><span class="line">    def square(x): return x ** 2</span><br><span class="line">    self.spark.udf.register(&quot;squared&quot;, square)</span><br><span class="line">    df2 = self.spark.sql(&quot;SELECT squared(value) squared FROM test&quot;)</span><br><span class="line">    return df2.toPandas()</span><br></pre></td></tr></table></figure>
<p><em>project.py</em>是我们的主程序，充当上述模块的客户端：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from data.data_source import DataSource</span><br><span class="line">def main():</span><br><span class="line">    src = DataSource()</span><br><span class="line">    df = src.get_data(num_elements=100000)</span><br><span class="line">    print(f&quot;Got Pandas dataframe with &#123;df.size&#125; elements&quot;)</span><br><span class="line">    print(df.head(10))</span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<p>克隆回购：<a href="https://bitbucket.org/calidoteam/pyspark.git" target="_blank" rel="noopener">https</a>：<a href="https://bitbucket.org/calidoteam/pyspark.git" target="_blank" rel="noopener">//bitbucket.org/calidoteam/pyspark.git</a><br>在开始之前，让我们回顾一下向Spark提交工作时可用的选项。<br><a name="3b2c7180"></a></p>
<h4 id="spark-submit，客户端和集群模式"><a href="#spark-submit，客户端和集群模式" class="headerlink" title="spark-submit，客户端和集群模式"></a>spark-submit，客户端和集群模式</h4><ul>
<li>Spark支持各种<a href="https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types" target="_blank" rel="noopener">集群管理器</a>：独立（即内置于Spark），Hadoop的YARN，Mesos，Kubernetes，所有这些都控制着工作负载在一组资源上的运行方式。</li>
<li><code>spark-submit</code>是唯一与所有集群管理器一致的接口。对于Python应用程序，<code>spark-submit</code>可以在需要时上载和暂存您提供的所有依赖项，如.py，.zip或.egg文件。</li>
<li>在<strong><em>客户端</em></strong><em>模式下</em>驱动程序_）将在运行的同一主机上<code>spark-submit</code>运行。确保此类主机靠近工作节点以减少网络延迟符合您的最佳利益。</li>
<li>在<strong>_集群_</strong><em>模式下</em>驱动程序_某个_工作节点并从其运行。从远程主机提交作业时，这很有用。从Spark 2.4独立开始，Spark 2.4.0集群模式不是一个选项。</li>
<li>或者，可以<code>spark-submit</code>通过<code>SparkSession</code>在Python应用程序中配置连接到群集来绕过。这需要正确的配置和匹配的PySpark二进制文件。您的Python应用程序将在<em>客户端模式下</em>有效运行：它将从您启动它的主机上运行。</li>
</ul>
<p>以下部分描述了几种部署方案，以及每种方案中需要的配置。</p>
<hr>
<p><a name="47d746c3"></a></p>
<h3 id="＃1：直接连接到Spark（客户端模式，无spark-submit）"><a href="#＃1：直接连接到Spark（客户端模式，无spark-submit）" class="headerlink" title="＃1：直接连接到Spark（客户端模式，无spark-submit）"></a>＃1：直接连接到Spark（客户端模式，无spark-submit）</h3><p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552902588026-b01dacd8-8d16-4298-b2d9-9237f34d8543.png#align=left&amp;display=inline&amp;height=462&amp;name=1_HcQRb-GgmYnWRF15yobCOw.png&amp;originHeight=619&amp;originWidth=1000&amp;size=293525&amp;status=done&amp;width=746" alt="1_HcQRb-GgmYnWRF15yobCOw.png"><br>针对Spark独立的客户端模式的Python应用程序</p>
<p>这是最简单的部署方案：Python应用程序通过指向Spark主URL直接建立spark上下文，并使用它来提交工作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(&quot;My PySpark App&quot;) \</span><br><span class="line">                  .setMaster(&quot;spark://192.168.1.10:7077&quot;)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">                    .config(conf=conf) \</span><br><span class="line">                    .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>在<strong>独立群集中</strong>，资源在作业持续时间内分配，默认配置为客户端应用程序提供所有可用资源，因此需要对多租户环境进行微调。执行程序进程（JVM或python）由每个节点本地的_工作_进程启动。<br>这类似于传统的客户端 - 服务器应用程序，因为客户端只是“连接”到“远程”集群。建议：<br><strong>确保</strong>驱动程序和群集之间<strong>有足够的带宽</strong>。大多数网络活动发生在驱动程序和它的执行程序之间，因此这个“远程”集群实际上必须在近距离（LAN）内。<br><strong>通过启用Apache Arrow改进Java-Python序列化：</strong> Python工作负载（NumPy，Pandas和其他应用于Spark RDD，数据帧和数据集的转换）默认需要大量的Java和Python进程的序列化和反序列化，并且会迅速降低性能。从Spark 2.3开始，<a href="https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html" target="_blank" rel="noopener">启用Apache Arrow</a>（包含在下面列出的步骤中）使这些传输<a href="https://databricks.com/session/improving-python-and-spark-performance-and-interoperability-with-apache-arrow" target="_blank" rel="noopener">更加高效</a>。<br><strong>跨所有群集节点和驱动程序主机部署依赖关系。</strong>这包括<a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">下载和安装Python 3</a>，pip安装PySpark（必须与目标集群的版本匹配），PyArrow以及其他库依赖项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install python36</span><br><span class="line">pip install pyspark==2.3.1 </span><br><span class="line">pip install pyspark[sql]</span><br><span class="line">pip install numpy pandas msgpack sklearn</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>在安装像PySpark这样的大型库（~200MB）时，可能会遇到以“ <code>MemoryError</code>” 结尾的错误。如果是这样，请尝试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --no-cache-dir pyspark==2.3.1</span><br></pre></td></tr></table></figure>
<p><strong>配置和环境变量：</strong>在客户端，<code>$SPARK_HOME</code>必须指向pip安装PySpark的位置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ pip show pyspark</span><br><span class="line">Name: pyspark</span><br><span class="line">Version: 2.3.1</span><br><span class="line">Summary: Apache Spark Python API</span><br><span class="line">Home-page: https://github.com/apache/spark/tree/master/python</span><br><span class="line">Author: Spark Developers</span><br><span class="line">Author-email: dev@spark.apache.org</span><br><span class="line">License: http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">Location: /opt/anaconda/lib/python3.6/site-packages</span><br><span class="line">Requires: py4j</span><br><span class="line">$ export SPARK_HOME=/opt/anaconda/lib/python3.6/site-packages</span><br></pre></td></tr></table></figure>
<p>在每个群集节点上，设置其他默认参数和环境变量。特别是对于Python应用程序：<br><code>$SPARK_HOME/conf/spark-defaults.sh</code></p>
<ul>
<li><code>spark.sql.execution.arrow.enabled true</code></li>
</ul>
<p><code>$SPARK_HOME/conf/spark-env.sh</code></p>
<ul>
<li><code>export PYSPARK_PYTHON=/usr/bin/python3</code> ：Python可执行文件，所有节点。</li>
<li><code>export PYSPARK_DRIVER_PYTHON=/usr/bin/python3</code> ：驱动程序的Python可执行文件，如果与执行程序节点不同。</li>
</ul>
<p><strong>注：</strong>环境变量是从哪里读<code>spark-submit</code>的_推出_，不一定是从群集主机内。<br><a name="d5f01497"></a></p>
<h4 id="运行它"><a href="#运行它" class="headerlink" title="运行它"></a>运行它</h4><p>将工作负载提交到集群只需运行Python应用程序（例如<code>spark-submit</code>，不需要）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd my-project-dir/</span><br><span class="line">$ python3 project.py</span><br></pre></td></tr></table></figure>
<p>在运行时，可以看到运行多个<em>python3</em>进程的从属节点在作业上运行：<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552902767067-19085e70-7bf9-4058-84d8-cc174302f2af.png#align=left&amp;display=inline&amp;height=304&amp;name=1_l-aJ47luaPs9wpj_FTlMSQ.png&amp;originHeight=408&amp;originWidth=1000&amp;size=413381&amp;status=done&amp;width=746" alt="1_l-aJ47luaPs9wpj_FTlMSQ.png"></p>
<p><a name="93533b7d"></a></p>
<h3 id="＃2：集装箱式应用（客户端模式，无火花提交）"><a href="#＃2：集装箱式应用（客户端模式，无火花提交）" class="headerlink" title="＃2：集装箱式应用（客户端模式，无火花提交）"></a>＃2：集装箱式应用（客户端模式，无火花提交）</h3><p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552902810496-c3ddc7d0-c9e2-4e34-823f-54fbc0e9966f.png#align=left&amp;display=inline&amp;height=512&amp;name=1_zcpriTNz1FM6kWUpQNr_RA.png&amp;originHeight=686&amp;originWidth=1000&amp;size=282197&amp;status=done&amp;width=746" alt="1_zcpriTNz1FM6kWUpQNr_RA.png"><br>这是前一个场景的扩展，由于可移植性等原因，最好将Python应用程序作为Docker容器运行，作为CI / CD管道的一部分。<br>除了针对上一个方案推荐的配置外，还需要以下内容：<br><strong>构建容器以包含所有依赖项：</strong>从包含Python 3和/或Java 8 OpenJDK的映像开始，然后pip-install PySpark，PyArrow以及应用程序所需的所有其他库。<br><strong>配置Spark驱动程序主机和端口，在容器中打开它们：</strong>这是执行程序到达容器内驱动程序所必需的。可以通过编程方式设置驱动程序的Spark属性（<code>spark.conf.set(“property”, “value”)</code>）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.host : host_ip_address (e.g. 192.168.1.10)</span><br><span class="line">spark.driver.port : static_port (e.g. 51400)</span><br><span class="line">spark.driver.bindAddress : container_internal_ip (e.g. 10.192.6.81)</span><br><span class="line">spark.driver.blockManagerPort : static_port (e.g. 51500)</span><br></pre></td></tr></table></figure>
<p>在Docker中，端口可以使用-p选项从命令行公开到外部：<code>-p 51400:51400 -p 51500:51500</code>。其他文章建议只发布此端口范围：<code>-p 5000–5010:5000–5010</code><br><a name="d5f01497-1"></a></p>
<h4 id="运行它-1"><a href="#运行它-1" class="headerlink" title="运行它"></a>运行它</h4><p>与前一个场景一样，运行容器将启动Python驱动程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 51400：51400 -p 51500：51500 &lt;docker_image_url&gt;</span><br></pre></td></tr></table></figure>
<p><a name="2efdb298"></a></p>
<h3 id="＃3：通过spark-submit提供Python应用程序（客户端模式）"><a href="#＃3：通过spark-submit提供Python应用程序（客户端模式）" class="headerlink" title="＃3：通过spark-submit提供Python应用程序（客户端模式）"></a>＃3：通过spark-submit提供Python应用程序（客户端模式）</h3><p>此方案实际上与方案＃1相同，仅为了清楚起见包含在此处。唯一的区别是Python应用程序是使用该<code>spark-submit</code>进程启动的。除了日志文件之外，还会将群集事件发送到<em>stdout</em>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd my-project-dir/</span><br><span class="line">$ ls -l </span><br><span class="line">rwxrwxr-x. 3 centos centos  70 Feb 25 02:11 data</span><br><span class="line">-rw-rw-r--. 1 centos centos 220 Feb 25 01:09 project.py</span><br><span class="line">$ spark-submit project.py</span><br></pre></td></tr></table></figure>
<p><strong>笔记：</strong></p>
<ul>
<li>根据我的经验，<code>spark-submit</code>只要从项目根目录（<code>my-project-dir/</code>）调用它就没有必要在调用时传递依赖的子目录/文件。</li>
<li>由于示例应用程序已指定主URL，因此无需将其传递给<code>spark-submit</code>。否则，更完整的命令将是：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master spark://sparkcas1:7077 --deploy-mode client project.py</span><br></pre></td></tr></table></figure>
<ul>
<li>从Spark 2.3开始，无法将集群模式下的Python应用程序提交给独立的Spark集群。这样做会产生错误：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master spark://sparkcas1:7077 --deploy-mode cluster project.py</span><br><span class="line">Error: Cluster deploy mode is currently not supported for python applications on standalone clusters.</span><br></pre></td></tr></table></figure>
<p><a name="938988a1"></a></p>
<h4 id="Takeaways-Python-on-Spark独立集群："><a href="#Takeaways-Python-on-Spark独立集群：" class="headerlink" title="Takeaways- Python on Spark独立集群："></a><strong>Takeaways- Python on Spark独立集群：</strong></h4><ul>
<li>虽然独立群集在生产中不受欢迎（可能是因为商业支持的分发包括群集管理器），但只要不需要多租户和动态资源分配，它们的占用空间就会更小并且做得很好。</li>
<li>对于Python应用程序，部署选项仅限于客户端模式。</li>
<li>使用Docker来容纳Python应用程序具有所有预期的优势，非常适合客户端模式部署。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/如何（以及为什么）创建一个好的验证集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/如何（以及为什么）创建一个好的验证集/" itemprop="url">如何（以及为什么）创建一个好的验证集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T12:19:06+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a name="d41d8cd9"></a></p>
<p>#<br>撰稿：2017年11月13日由<strong>Rachel Thomas</strong>撰写<br>一个非常常见的场景：看似令人印象深刻的机器学习模型在生产中实施时是完全失败的。这些影响包括那些现在对机器学习持怀疑态度且不愿意再次尝试的领导者。怎么会发生这种情况？<br>对于开发结果与生产结果之间的这种脱节的最可能的罪魁祸首之一是选择不当的验证集（或者更糟糕的是，根本没有验证集）。根据数据的性质，选择验证集可能是最重要的一步。尽管sklearn提供了一种<a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener"></a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split</a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">方法</a>，但该方法采用了数据的随机子集，这对于许多现实问题来说是一个糟糕的选择。<br><strong>培训</strong>，<strong>验证</strong>和<strong>测试</strong>集的定义可能相当细微，有时不一致地使用这些术语。在深度学习社区中，“测试时间推断”通常用于指对生产中的数据进行评估，这不是测试集的技术定义。如上所述，sklearn有一种train_test_split方法，但没有train_validation_test_split。Kaggle只提供培训和测试集，但要做得好，您需要将他们的训练集分成您自己的验证和训练集。而且，事实证明Kaggle的测试集实际上被细分为两组。很多初学者可能会感到困惑，这并不奇怪！我将在下面解决这些微妙之处。<br><a name="e7a3668b"></a></p>
<h2 id="首先，什么是“验证集”？"><a href="#首先，什么是“验证集”？" class="headerlink" title="首先，什么是“验证集”？"></a><strong>首先，什么是“验证集”？</strong></h2><p>在创建机器学习模型时，最终目标是使其准确处理新数据，而不仅仅是用于构建数据的数据。考虑下面一组数据的3种不同模型的例子：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770075-60b15664-d2d8-4359-9920-d9d56d8a789b.png#align=left&amp;display=inline&amp;height=281&amp;name=image.png&amp;originHeight=294&amp;originWidth=781&amp;size=83649&amp;status=done&amp;width=746" alt="image.png"><br>资料来源：Andrew Ng的机器学习课程<br>对于最右侧的模型，图示数据点的误差最小（蓝色曲线几乎完美地穿过红点），但它不是最佳选择。这是为什么？如果你要收集一些新的数据点，它们很可能不在右边图表中的那条曲线上，而是更接近中间图形中的曲线。<br>根本的想法是：</p>
<ul>
<li>训练集用于训练给定的模型<br></li>
<li>验证集用于在模型之间进行选择（例如，随机森林或神经网络是否更适合您的问题？您想要一个有40棵树或50棵树的随机森林吗？）<br></li>
<li>测试集告诉你你是怎么做的。如果您已经尝试了很多不同的模型，那么您可能只是偶然得到一个在验证集上表现良好的模型，并且拥有测试集有助于确保不是这种情况。<br></li>
</ul>
<p>验证和测试集的一个关键属性是它们必须代表<strong><strong>您将来会看到</strong></strong>的<strong><strong>新数据</strong></strong>。这可能听起来像一个不可能的命令！根据定义，您还没有看到这些数据。但是你仍然知道一些事情。<br><a name="7086b6ec"></a></p>
<h2 id="什么时候随机子集不够好？"><a href="#什么时候随机子集不够好？" class="headerlink" title="什么时候随机子集不够好？"></a><strong>什么时候随机子集不够好？</strong></h2><p>看一些例子是有益的。虽然这些例子中有很多来自Kaggle比赛，但它们代表了您在工作场所会遇到的问题。<br><a name="cc464807"></a></p>
<h3 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a><strong>时间序列</strong></h3><p>如果您的数据是时间序列，那么选择数据的随机子集将非常简单（您可以在您尝试预测的日期之前和之后查看数据）并且不代表大多数业务用例（您在哪里）正在使用历史数据来构建将来使用的模型。如果您的数据包含日期并且您要构建将来使用的模型，则需要选择具有最新日期的连续部分作为验证集（例如，可用数据的最后两周或上个月） 。<br>假设您要将下面的时间序列数据拆分为训练和验证集：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770129-8cad81d5-fbe9-4945-baca-d69af4bf8853.png#align=left&amp;display=inline&amp;height=492&amp;name=image.png&amp;originHeight=492&amp;originWidth=617&amp;size=31169&amp;status=done&amp;width=617" alt="image.png"><br><em>时间序列数据</em><br>随机子集是一个糟糕的选择（太容易填补空白，并不表示您在生产中需要什么）：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770176-99c4e0dd-3800-454b-bf9d-b9b73bfe4f44.png#align=left&amp;display=inline&amp;height=492&amp;name=image.png&amp;originHeight=492&amp;originWidth=617&amp;size=27107&amp;status=done&amp;width=617" alt="image.png"><br><em>训练集的选择不佳</em><br>使用较早的数据作为训练集（以及验证集的后续数据）：<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770243-648aea97-36ed-4e23-a69e-ec2dcbf32598.png#align=left&amp;display=inline&amp;height=492&amp;name=image.png&amp;originHeight=492&amp;originWidth=617&amp;size=26915&amp;status=done&amp;width=617" alt="image.png"><br><em>为您的训练集提供更好的选择</em><br>Kaggle目前正在竞争<a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" target="_blank" rel="noopener"></a><a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" target="_blank" rel="noopener">预测一系列厄瓜多尔杂货店的销售情况</a>。Kaggle的“培训数据”从2013年1月1日至2017年8月15日运行，测试数据跨越2017年8月16日至2017年8月31日。一个好方法是使用2017年8月1日至8月15日作为验证集，以及所有早期数据作为你的训练集。<br><a name="574cf8ad"></a></p>
<h3 id="新人，新船，新…"><a href="#新人，新船，新…" class="headerlink" title="新人，新船，新…"></a><strong>新人，新船，新…</strong></h3><p>您还需要考虑在生产中进行预测的数据可能与您训练模型所需的数据<strong>有何不同</strong>。<br>在Kaggle <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection" target="_blank" rel="noopener"></a><a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection" target="_blank" rel="noopener">分心驾驶员竞赛中</a>，独立数据是汽车驾驶员的照片，因变量是一个类别，如发短信，吃饭或安全向前看。如果您是从这些数据构建模型的保险公司，请注意您最感兴趣的是模型在您之前没有见过的驱动程序上的表现（因为您可能只为一小部分人提供培训数据）。对于Kaggle比赛也是如此：测试数据由未在训练集中使用的人组成。<br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770581-4e7e5ff9-8b12-415e-8197-bc32f52b5957.png#align=left&amp;display=inline&amp;height=354&amp;name=image.png&amp;originHeight=354&amp;originWidth=469&amp;size=208442&amp;status=done&amp;width=469" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552882770937-9c3f6222-3e65-46e1-a25d-3a7c7241b3bf.png#align=left&amp;display=inline&amp;height=359&amp;name=image.png&amp;originHeight=359&amp;originWidth=467&amp;size=208605&amp;status=done&amp;width=467" alt="image.png"><br><em>同一个人驾驶时在电话上交谈的两个图像。</em><br>如果您将上述图像之一放入训练集中，并将其中一个放在验证集中，那么您的模型似乎表现得比新人更好。另一个观点是，如果你使用所有人来训练你的模型，你的模型可能过分适应那些特定人的特殊性，而不仅仅是学习状态（发短信，吃饭等）。<br>在<a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring" target="_blank" rel="noopener"></a><a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring" target="_blank" rel="noopener">Kaggle渔业竞赛中也</a>有类似的动态，以确定渔船捕获的鱼类种类，以减少非法捕捞濒危种群。测试集由未出现在训练数据中的船组成。这意味着您希望验证集包含不在训练集中的船只。<br>有时可能不清楚您的测试数据会有何不同。例如，对于使用卫星图像的问题，您需要收集有关训练集是否仅包含某些地理位置的更多信息，或者是否来自地理位置分散的数据。<br><a name="af629453"></a></p>
<h2 id="交叉验证的危险"><a href="#交叉验证的危险" class="headerlink" title="交叉验证的危险"></a><strong>交叉验证的危险</strong></h2><p>sklearn没有train_validation_test拆分的原因是假设您经常使用<strong><strong>交叉验证</strong></strong>，其中训练集的不同子集用作验证集。例如，对于3倍交叉验证，数据被分为3组：A，B和C.首先训练模型A和B组合作为训练集，并在验证集C上进行评估。 ，将模型训练为A和C组合作为训练集，并在验证集B上进行评估。依此类推，最终将3倍的模型性能平均化。<br>但是，交叉验证的问题在于，由于上述各节中描述的所有原因，它很少适用于现实世界的问题。交叉验证仅适用于您可以随机调整数据以选择验证集的相同情况。<br><a name="5c6fff0d"></a></p>
<h2 id="Kaggle的“训练集”-你的训练-验证集"><a href="#Kaggle的“训练集”-你的训练-验证集" class="headerlink" title="Kaggle的“训练集”=你的训练+验证集"></a><strong>Kaggle的“训练集”=你的训练+验证集</strong></h2><p>Kaggle比赛的一个好处是它们会迫使你更严格地考虑验证集（为了做得好）。对于那些刚接触Kaggle的人来说，它是一个举办机器学习比赛的平台。Kaggle通常会将数据分为两组，您可以下载：</p>
<ol>
<li>一个<strong><strong>训练集</strong></strong>，其中包括<strong>独立的变量</strong>，以及对<strong>因变量</strong>（你正在尝试预测）。对于试图预测销售的厄瓜多尔杂货店的例子，自变量包括商店ID，商品ID和日期; 因变量是销售数量。对于尝试确定驾驶员是否在车轮后面进行危险行为的示例，自变量可以是驾驶员的图片，并且因变量是类别（例如发短信，吃饭或安全地向前看）。<br></li>
<li>一个<strong><strong>测试集</strong></strong>，它只有自变量。您将对测试集进行预测，您可以将其提交给Kaggle，并获得您的评分。<br></li>
</ol>
<p>这是开始机器学习所需的基本思想，但要做得好，理解起来要复杂一些。您将需要创建自己的培训和验证集（通过拆分Kaggle“培训”数据）。您将使用较小的训练集（Kaggle训练数据的子集）来构建模型，并且在提交给Kaggle之前，您可以在验证集（也是Kaggle的训练数据的子集）上对其进行评估。<br>最重要的原因是Kaggle将测试数据分为两组：公共和私人排行榜。您在公共排行榜上看到的分数仅适用于您预测的一部分（您不知道哪个子集！）。您的预测在私人排行榜上的表现将不会在比赛结束前公布。这一点很重要的原因是你最终可能会过度适应公共排行榜，直到最后你在私人排行榜上做得不好时才会意识到这一点。使用良好的验证集可以防止这种情况。您可以通过查看您的模型是否具有与Kaggle测试集相比较的相似分数来检查您的验证集是否有用。<br>创建自己的验证集很重要的另一个原因是Kaggle限制您每天提交两次，并且您可能希望尝试更多。第三，确切地看到你在验证集上出错的地方是有益的，而且Kaggle没有告诉你测试集的正确答案，甚至没有告诉你哪些数据点你的错误，只是你的整体得分。<br>理解这些区别不仅对Kaggle有用。在任何预测性机器学习项目中，您希望模型能够在新数据上表现良好。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/如何使用良好的软件工程实践设置PySpark环境以进行开发/" itemprop="url">如何使用良好的软件工程实践设置PySpark环境以进行开发</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T11:16:24+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本文中，我们将讨论如何设置我们的开发环境以创建高质量的python代码以及如何自动执行一些繁琐的任务来加速部署。<br>我们将介绍以下步骤：</p>
<ul>
<li>使用<a href="https://pipenv.readthedocs.io/en/latest/" target="_blank" rel="noopener"><strong>pipenv</strong></a>在隔离的虚拟环境中设置我们的依赖<a href="https://pipenv.readthedocs.io/en/latest/" target="_blank" rel="noopener"><strong>项</strong></a></li>
<li>如何为多个作业设置项目结构</li>
<li>如何运行pyspark工作</li>
<li>如何使用<a href="https://opensource.com/article/18/8/what-how-makefile" target="_blank" rel="noopener"><strong>Makefile</strong></a><strong> </strong>自动执行开发步骤</li>
<li>如何使用<a href="http://flake8.pycqa.org/en/latest/" target="_blank" rel="noopener"><strong>flake8</strong></a>测试代码的质量<a href="http://flake8.pycqa.org/en/latest/" target="_blank" rel="noopener"></a></li>
<li>如何使用<a href="https://pypi.org/project/pytest-spark/" target="_blank" rel="noopener"><strong>pytest-spark</strong></a>为PySpark应用程序运行单元测试<a href="https://pypi.org/project/pytest-spark/" target="_blank" rel="noopener"></a></li>
<li>运行测试覆盖率，看看我们是否使用<a href="https://pypi.org/project/pytest-cov/" target="_blank" rel="noopener"><strong>pytest-cov</strong></a>创建了足够的单元测试<a href="https://pypi.org/project/pytest-cov/" target="_blank" rel="noopener"></a><br><a name="54e5af6c"></a><h3 id="第1步：设置虚拟环境"><a href="#第1步：设置虚拟环境" class="headerlink" title="第1步：设置虚拟环境"></a>第1步：设置虚拟环境</h3>虚拟环境有助于我们将特定应用程序的依赖关系与系统的整体依赖关系隔离开来。这很好，因为我们不会涉及现有库的依赖性问题，并且在单独的系统（例如docker容器或服务器）上安装或卸载它们更容易。对于此任务，我们将使用<strong>pipenv。</strong><br>要在mac os系统上安装它，例如运行：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install pipenv</span><br></pre></td></tr></table></figure>
<p>要为应用程序声明我们的依赖项（库），我们需要在项目的路径路径中创建一个<strong>Pipfile</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[source]]</span><br><span class="line">url = &apos;https://pypi.python.org/simple&apos;</span><br><span class="line">verify_ssl = true</span><br><span class="line">name = &apos;pypi&apos;</span><br><span class="line">[requires]</span><br><span class="line">python_version = &quot;3.6&quot;</span><br><span class="line">[packages]</span><br><span class="line">flake8 = &quot;*&quot;</span><br><span class="line">pytest-spark = &quot;&gt;=0.4.4&quot;</span><br><span class="line">pyspark = &quot;&gt;=2.4.0&quot;</span><br><span class="line">pytest-cov = &quot;*&quot;</span><br></pre></td></tr></table></figure>
<p>这里有三个组件。在<strong>[[source]]</strong>标签中，我们声明了下载所有软件包的<strong>url</strong>，在<strong>[requires]中</strong>我们定义了python版本，最后在<strong>[packages]</strong>中声明了我们需要的依赖项。我们可以将依赖项绑定到某个版本，或者使用<strong>“*”</strong>符号来获取最新版本。<br>要创建虚拟环境并激活它，我们需要在终端中运行两个命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipenv --three install</span><br><span class="line">pipenv shell</span><br></pre></td></tr></table></figure>
<p>一旦完成这一步，你应该看到你在一个新的venv中，让项目的名字出现在命令行的终端中（默认情况下，env采用项目的名称）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(pyspark-project-template) host:project$</span><br></pre></td></tr></table></figure>
<p>现在，您可以使用两个命令进出。<br>停用env并返回标准环境：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure>
<p>再次激活虚拟环境（您需要位于项目的根目录中）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source `pipenv --venv`/bin/activate</span><br></pre></td></tr></table></figure></p>
<p><a name="bb546939"></a></p>
<h3 id="第2步：项目结构"><a href="#第2步：项目结构" class="headerlink" title="第2步：项目结构"></a>第2步：项目结构</h3><p>该项目可以具有以下结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">pyspark-project-template</span><br><span class="line">    src/</span><br><span class="line">        jobs/   </span><br><span class="line">            pi/</span><br><span class="line">                __init__.py</span><br><span class="line">                resources/</span><br><span class="line">                    args.json</span><br><span class="line">            word_count/</span><br><span class="line">                __init__.py</span><br><span class="line">                resources/</span><br><span class="line">                    args.json</span><br><span class="line">                    word_count.csv</span><br><span class="line">        main.py</span><br><span class="line">    test/</span><br><span class="line">        jobs/</span><br><span class="line">            pi/</span><br><span class="line">                test_pi.py</span><br><span class="line">            word_count/</span><br><span class="line">                test_word_count.py</span><br></pre></td></tr></table></figure>
<p>排除一些<strong>init</strong>.py文件以简化操作，但您可以在本教程末尾的github上找到完整项目的链接。我们基本上有源代码和测试。每个作业都分成一个文件夹，每个作业都有一个资源文件夹，我们在其中添加该作业所需的额外文件和配置。<br>在本教程中，我使用了两个经典示例 -  <strong>pi</strong>，生成最多小数的pi数和<strong>字数</strong>，以计算csv文件中的单词数。<br><a name="1b6fecdc"></a></p>
<h3 id="第3步：使用spark-submit运行作业"><a href="#第3步：使用spark-submit运行作业" class="headerlink" title="第3步：使用spark-submit运行作业"></a>第3步：使用spark-submit运行作业</h3><p>我们先来看看<strong>main.py</strong>文件的样子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser(description=&apos;My pyspark job arguments&apos;)</span><br><span class="line">    parser.add_argument(&apos;--job&apos;, type=str, required=True, dest=&apos;job_name&apos;,</span><br><span class="line">                        help=&apos;The name of the spark job you want to run&apos;)</span><br><span class="line">    parser.add_argument(&apos;--res-path&apos;, type=str, required=True, dest=&apos;res_path&apos;,</span><br><span class="line">                        help=&apos;Path to the jobs resurces&apos;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(args.job_name)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    job_module = importlib.import_module(&apos;jobs.%s&apos; % args.job_name)</span><br><span class="line">    res = job_module.run(spark, get_config(args.res_path, args.job_name))</span><br><span class="line"></span><br><span class="line">    print(&apos;[JOB &#123;job&#125; RESULT]: &#123;result&#125;&apos;.format(job=args.job_name, result=res))</span><br></pre></td></tr></table></figure>
<p>当我们运行我们的工作时，我们需要两个命令行参数：  <strong>- job</strong>，是我们想要运行的作业的名称（在例外pi或word_count中）和  <strong>- res-path</strong>，是作业的相对路径。我们需要第二个参数，因为spark需要知道我们资源的完整路径。在生产环境中，我们将代码部署在集群上，我们将资源转移到HDFS或S3，我们将使用该路径。<br>在进一步解释代码之前，我们需要提一下，我们必须压缩<strong>作业</strong>文件夹并将其传递给<strong>spark-submit</strong>语句<strong>。</strong>假设我们在项目的根目录中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd src/ </span><br><span class="line">zip -r ../jobs.zip jobs/</span><br></pre></td></tr></table></figure>
<p>这将使代码在我们的应用程序中作为模块提供。基本上在<strong>第16行的main.py中，</strong>我们以编程方式导入<strong>作业</strong>模块。<br>我们的作业<strong>pi</strong>和<strong>word_count</strong>都有一个<strong>run</strong>函数，所以我们只需要运行这个函数来启动这个作业（<strong>main.py中的第17行）。</strong>我们还在那里传递了工作的配置。<br>让我们看一下<strong>word_count</strong>作业，进一步了解这个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line">def get_keyval(row):</span><br><span class="line">    words = filter(lambda r: r is not None, row)</span><br><span class="line">    return [[w.strip().lower(), 1] for w in words]</span><br><span class="line"></span><br><span class="line">def run(spark, config):</span><br><span class="line">    df = spark.read.csv(config[&apos;relative_path&apos;] + config[&apos;words_file_path&apos;])</span><br><span class="line">    mapped_rdd = df.rdd.flatMap(lambda row: get_keyval(row))</span><br><span class="line">    counts_rdd = mapped_rdd.reduceByKey(add)</span><br><span class="line">    return counts_rdd.collect()</span><br></pre></td></tr></table></figure>
<p>此代码在<strong>word_count</strong>文件夹的<strong><strong>init</strong>.py</strong>文件中定义。我们在这里可以看到，我们使用两个配置参数来读取资源文件夹中的csv文件：相对路径和csv文件的位置。其余的代码只计算单词，所以我们不会在这里详细介绍。值得一提的是，每个作业在resources文件夹中都有一个<strong>args.json</strong>文件。这里我们实际定义了传递给作业的配置。这是<strong>word_count</strong>作业的配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;words_file_path&quot;: &quot;/word_count/resources/word_count.csv&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以我们现在有了所有细节来运行我们的<strong>spark-submit</strong>命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --py-files jobs.zip src/main.py --job word_count --res-path /your/path/pyspark-project-template/src/jobs</span><br></pre></td></tr></table></figure>
<p>要运行另一个作业<strong>pi，</strong>我们只需要更改<strong>- job</strong>标志的参数  。<br><a name="b336e1a0"></a></p>
<h3 id="第4步：编写单元测试，并使用覆盖率运行它们"><a href="#第4步：编写单元测试，并使用覆盖率运行它们" class="headerlink" title="第4步：编写单元测试，并使用覆盖率运行它们"></a>第4步：编写单元测试，并使用覆盖率运行它们</h3><p>要为pyspark应用程序编写测试，我们使用<strong>pytest-spark</strong>，一个非常易于使用的模块。<br>该<strong>WORD_COUNT</strong>工作单元测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from src.jobs.word_count import get_keyval, run</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test_get_keyval():</span><br><span class="line">    words=[&apos;this&apos;, &apos;are&apos;, &apos;words&apos;, &apos;words&apos;]</span><br><span class="line">    expected_results=[[&apos;this&apos;, 1], [&apos;are&apos;, 1], [&apos;words&apos;, 1], [&apos;words&apos;, 1]]</span><br><span class="line"></span><br><span class="line">    assert expected_results == get_keyval(words)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test_word_count_run(spark_session):</span><br><span class="line">    expected_results = [(&apos;one&apos;, 1), (&apos;two&apos;, 1), (&apos;three&apos;, 2), (&apos;four&apos;, 2), (&apos;test&apos;, 1)]</span><br><span class="line">    conf = &#123;</span><br><span class="line">        &apos;relative_path&apos;: &apos;/your/path/pyspark-project-template/src/jobs&apos;,</span><br><span class="line">        &apos;words_file_path&apos;: &apos;/word_count/resources/word_count.csv&apos;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    assert expected_results == run(spark_session, conf)</span><br></pre></td></tr></table></figure>
<p>我们需要从<strong>src</strong>模块导入我们想要测试的函数。这里更有趣的部分是我们如何进行<strong>test_word_count_run。</strong>我们可以看到没有初始化的spark会话，我们只是在测试中将其作为参数接收。这要归功于<strong>pytest-spark</strong>模块，因此我们可以专注于编写测试，而不是编写样板代码。<br>接下来让我们讨论一下代码覆盖率。我们怎么知道我们是否编写了足够的单元测试？很简单，我们运行测试覆盖工具，告诉我们尚未测试的代码。对于python，我们可以使用<strong>pytest-cov</strong>模块。要使用代码覆盖率运行所有测试，我们必须运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytest --cov=src test/jobs/</span><br></pre></td></tr></table></figure>
<p>where  <strong>- cov</strong> flag告诉pytest在哪里检查覆盖范围。<br>测试覆盖率结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---------- coverage: platform darwin, python 3.7.2-final-0 -----------</span><br><span class="line">Name                              Stmts   Miss  Cover</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">src/__init__.py                       0      0   100%</span><br><span class="line">src/jobs/__init__.py                  0      0   100%</span><br><span class="line">src/jobs/pi/__init__.py              11      0   100%</span><br><span class="line">src/jobs/word_count/__init__.py       9      0   100%</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">TOTAL                                20      0   100%</span><br></pre></td></tr></table></figure>
<p>我们的测试覆盖率是100％，但是等一下，缺少一个文件！为什么<strong>main.py</strong>没有在那里列出？<br>如果我们认为我们有不需要测试的python代码，我们可以将它从报告中排除。为此，我们需要在项目的根目录中创建一个  <strong>.coveragerc</strong>文件。对于此示例，它看起来像这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[run]</span><br><span class="line">omit = src/main.py</span><br></pre></td></tr></table></figure>
<p><a name="3760f981"></a></p>
<h3 id="第5步：运行静态代码分析"><a href="#第5步：运行静态代码分析" class="headerlink" title="第5步：运行静态代码分析"></a>第5步：运行静态代码分析</h3><p>很好，我们有一些代码，我们可以运行它，我们有良好的覆盖率的单元测试。我们做对了吗？还没！我们还需要确保按照python最佳实践编写易于阅读的代码。为此，我们必须使用名为<strong>flake8</strong>的python模块检查我们的代码<strong>。</strong><br>要运行它：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flake8 ./src</span><br></pre></td></tr></table></figure>
<p>它将分析<strong>src</strong>文件夹。如果我们有干净的代码，我们就不应该收到任何警告。但不，我们有一些问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flake8 ./src</span><br><span class="line">./src/jobs/pi/__init__.py:13:1: E302 expected 2 blank lines, found 1</span><br><span class="line">./src/jobs/pi/__init__.py:15:73: E231 missing whitespace after &apos;,&apos;</span><br><span class="line">./src/jobs/pi/__init__.py:15:80: E501 line too long (113 &gt; 79 characters)</span><br></pre></td></tr></table></figure>
<p>我们来看看代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from random import random</span><br><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUMBER_OF_STEPS_FACTOR = 100000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f(_):</span><br><span class="line">    x = random() * 2 - 1</span><br><span class="line">    y = random() * 2 - 1</span><br><span class="line">    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0</span><br><span class="line"></span><br><span class="line">def run(spark, config):</span><br><span class="line">    number_of_steps = config[&apos;partitions&apos;] * NUMBER_OF_STEPS_FACTOR</span><br><span class="line">    count = spark.sparkContext.parallelize(range(1, number_of_steps + 1),config[&apos;partitions&apos;]).map(f).reduce(add)</span><br><span class="line">    return 4.0 * count / number_of_steps</span><br></pre></td></tr></table></figure>
<p>我们可以看到在第<strong>13</strong>行我们有一个<strong>E302</strong>警告<strong>。</strong>这意味着我们需要在两种方法之间增加一条线。然后是第<strong>15</strong>行的<strong>E231</strong>和<strong>E501</strong>。这一行的第一个警告告诉我们，我们需要在和之间留出一个额外的空间，第二个警告通知我们线路太长，而且很难读（我们可以’甚至在要点中完整地看到它！）。<code>**range(1, number_of_steps +1),**</code><strong> </strong><code>**config[**</code><strong> </strong><br>解决所有警告后，代码看起来更容易阅读：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from random import random</span><br><span class="line">from operator import add</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUMBER_OF_STEPS_FACTOR = 100000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f(_):</span><br><span class="line">    x = random() * 2 - 1</span><br><span class="line">    y = random() * 2 - 1</span><br><span class="line">    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def run(spark, config):</span><br><span class="line">    number_of_steps = config[&apos;partitions&apos;] * NUMBER_OF_STEPS_FACTOR</span><br><span class="line">    count = spark.sparkContext\</span><br><span class="line">        .parallelize(range(1, number_of_steps + 1),</span><br><span class="line">                     config[&apos;partitions&apos;]).map(f).reduce(add)</span><br><span class="line">    return 4.0 * count / number_of_steps</span><br></pre></td></tr></table></figure>
<p><a name="a448168b"></a></p>
<h3 id="第6步：将所有内容与Makefile放在一起"><a href="#第6步：将所有内容与Makefile放在一起" class="headerlink" title="第6步：将所有内容与Makefile放在一起"></a>第6步：将所有内容与Makefile放在一起</h3><p>因为我们在终端中运行了一堆命令，所以在最后一步中我们将研究如何简化和自动执行此任务。<br>我们可以在项目的根目录中创建一个<strong>Makefile</strong>，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.DEFAULT_GOAL := run</span><br><span class="line">init:</span><br><span class="line"> pipenv --three install</span><br><span class="line"> pipenv shell</span><br><span class="line">analyze:</span><br><span class="line"> flake8 ./src</span><br><span class="line">run_tests:</span><br><span class="line"> pytest --cov=src test/jobs/</span><br><span class="line">run:</span><br><span class="line"> find . -name &apos;__pycache__&apos; | xargs rm -rf</span><br><span class="line"> rm -f jobs.zip</span><br><span class="line"> cd src/ &amp;&amp; zip -r ../jobs.zip jobs/</span><br><span class="line"> spark-submit --py-files jobs.zip src/main.py --job $(JOB_NAME) --res-path $(CONF_PATH)</span><br></pre></td></tr></table></figure></p>
<p>如果我们想要使用coverage运行测试，我们只需输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make run_tests</span><br></pre></td></tr></table></figure>
<p>如果我们想要运行<strong>pi</strong>工作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make run JOB_NAME=pi CONF_PATH=/your/path/pyspark-project-template/src/jobs</span><br></pre></td></tr></table></figure>
<p>这就是所有人！希望这个对你有帮助。<br>一如既往，代码存储在<a href="https://github.com/BogdanCojocar/medium-articles/tree/master/pyspark-project-template" target="_blank" rel="noopener">github上</a>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/使用Spark Structured Streaming，XGBoost和Scala进行实时预测/" itemprop="url">使用Spark Structured Streaming，XGBoost和Scala进行实时预测</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T11:06:19+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本文中，我们将讨论构建完整的机器学习管道。第一部分将侧重于在标准批处理模式下训练二元分类器，在第二部分中我们将进行一些实时预测。我们将使用来自<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">泰坦尼克号的</a>数据<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">：机器学习灾难</a>中的众多Kaggle比赛之一。在开始之前，请知道您应该熟悉<a href="https://www.tutorialspoint.com/scala/index.htm" target="_blank" rel="noopener">Scala</a>，<a href="https://spark.apache.org/" target="_blank" rel="noopener">Apache Spark</a>和<a href="http://xgboost.readthedocs.io/en/latest/get_started/" target="_blank" rel="noopener">Xgboost</a>。所有源代码也将在<a href="https://github.com/BogdanCojocar/medium-articles/tree/master/titanic_spark" target="_blank" rel="noopener">Github上提供</a>。很酷，现在让我们开始吧！<br><a name="d8b16075"></a></p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>我们将使用Spark中的ML管道训练XGBoost分类器。分类器将保存为输出，并将在Spark Structured Streaming实时应用程序中用于预测新的测试数据。<br><strong>第1步：启动spark会话</strong><br>我们正在创建一个将在本地运行的spark应用程序，并将使用与使用的核心一样多的线程<code>local[*]</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark  = SparkSession.builder()</span><br><span class="line">  .appName(&quot;Spark XGBOOST Titanic Training&quot;)</span><br><span class="line">  .master(&quot;local[*]&quot;)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>
<p><strong>第2步：定义架构</strong><br>接下来，我们定义从csv读取的数据的模式。这通常比让火花推断模式更好，因为它消耗的资源更少，我们可以完全控制字段。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"PassengerId"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Survival"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Pclass"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Name"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Sex"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Age"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"SibSp"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Parch"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Ticket"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Fare"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Cabin"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Embarked"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br></pre></td></tr></table></figure>
<p><strong>第3步：读取数据</strong><br>我们把csv读成a <code>DataFrame</code>，确保我们提到我们有一个标题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val df_raw = spark</span><br><span class="line">  .read</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .schema(schema)</span><br><span class="line">  .csv(filePath)</span><br></pre></td></tr></table></figure>
<p><strong>第4步：删除空值</strong><br>所有空值都替换为0.这不是理想的，但是对于本教程的目的，它是可以的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df = df_raw.na.fill(0)</span><br></pre></td></tr></table></figure>
<p><strong>第5步：将名义值转换为数字</strong><br>在浏览此步骤的代码之前，让我们简要介绍一些Spark ML概念。他们介绍了ML管道的概念，它是一组构建在其上的高级API <code>DataFrames</code>，可以更轻松地将多个算法组合到一个流程中。管道的主要元素是<code>Transformer</code>和<code>Estimator</code>。第一个可以表示可以将a <code>DataFrame</code>转换为另一个<code>DataFrame</code>的算法，而后者是可以适合a <code>DataFrame</code>来生成a 的算法<code>Transformer</code> 。<br>为了将名义值转换为数字值，我们需要<code>Transformer</code>为每列定义一个：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sexIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">"Sex"</span>)</span><br><span class="line">  .setOutputCol(<span class="string">"SexIndex"</span>)</span><br><span class="line">  .setHandleInvalid(<span class="string">"keep"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> cabinIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">"Cabin"</span>)</span><br><span class="line">  .setOutputCol(<span class="string">"CabinIndex"</span>)</span><br><span class="line">  .setHandleInvalid(<span class="string">"keep"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> embarkedIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">"Embarked"</span>)</span><br><span class="line">  .setOutputCol(<span class="string">"EmbarkedIndex"</span>)</span><br><span class="line">  .setHandleInvalid(<span class="string">"keep"</span>)</span><br></pre></td></tr></table></figure>
<p>我们正在使用它<code>StringIndexer</code>来转换价值观。对于每个<code>Transformer</code>我们定义的输入列和输出列将包含修改后的值。<br><strong>步骤6：将列组合成特征向量</strong><br>我们将使用另一个<code>Transformer</code>将XGBoost分类中使用的列组合<code>Estimator</code>成一个向量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> vectorAssembler = <span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">  .setInputCols(<span class="type">Array</span>(<span class="string">"Pclass"</span>, <span class="string">"SexIndex"</span>, <span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"Fare"</span>, <span class="string">"CabinIndex"</span>, <span class="string">"EmbarkedIndex"</span>))</span><br><span class="line">  .setOutputCol(<span class="string">"features"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>第7步：添加XGBoost估算器</strong><br>定义<code>Estimator</code>它将产生模型。可以在地图中定义估计器的设置。我们还可以设置功能和标签列：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val xgbEstimator = new XGBoostEstimator(Map[String, Any](&quot;num_rounds&quot; -&gt; 100))</span><br><span class="line">  .setFeaturesCol(&quot;features&quot;)</span><br><span class="line">  .setLabelCol(&quot;Survival&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>第8步：构建管道和分类器</strong><br>在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作的顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val pipeline = new Pipeline().setStages(Array(sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgbEstimator))</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552878673454-f586d9ed-0fba-4140-b77d-605cc73a652d.png#align=left&amp;display=inline&amp;height=105&amp;name=1_5GXouKuoLYmdTCgu1onrzw.png&amp;originHeight=141&amp;originWidth=1000&amp;size=44837&amp;status=done&amp;width=746" alt="1_5GXouKuoLYmdTCgu1onrzw.png"><br>输入<code>DataFrame</code>将被多次转换，最终将生成使用我们的数据训练的模型。我们将保存输出，以便在第二个实时应用程序中使用它。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cvModel = pipeline.fit(df)</span><br><span class="line">cvModel.write.overwrite.save(modelPath)</span><br></pre></td></tr></table></figure>
<p><a name="fbee26a1"></a></p>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p>我们将使用Spark Structured Streaming来基本传输文件中的数据。在现实世界的应用程序中，我们从专用的分布式队列（例如Apache Kafka或AWS Kinesis）读取数据，但是对于此演示，我们将只使用一个简单的文件。<br>简要描述<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources" target="_blank" rel="noopener">Spark Structured Streaming</a>是一个构建在Spark SQL之上的流处理引擎。它使用相同的概念，<code>DataFrames</code>数据存储在一个无限制的表中，当数据流入时，该表随着新行的增长而增长。<br><strong>第1步：创建输入读取流</strong><br>我们再次创建一个spark会话并定义数据的架构。请注意，测试csv不包含标签<code>Survival</code> 。最后我们可以创建输入流<code>DataFrame,</code> <code>df</code>。输入路径必须是我们存储csv文件的目录。它可以包含一个或多个具有相同模式的文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">  .appName(<span class="string">"Spark Structured Streaming XGBOOST"</span>)</span><br><span class="line">  .master(<span class="string">"local[*]"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">  <span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"PassengerId"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Pclass"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Name"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Sex"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Age"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"SibSp"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Parch"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Ticket"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Fare"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Cabin"</span>, <span class="type">StringType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"Embarked"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> df = spark</span><br><span class="line">    .readStream</span><br><span class="line">    .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">    .schema(schema)</span><br><span class="line">    .csv(fileDir)</span><br></pre></td></tr></table></figure>
<p><strong>第2步：加载XGBoost模型</strong><br>在对象中，<code>XGBoostModel</code>我们加载预先训练的模型，该模型将应用于我们在流中读取的每一批新行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">XGBoostModel</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> modelPath = <span class="string">"your_path"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> model = <span class="type">PipelineModel</span>.read.load(modelPath)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>(df: <span class="type">DataFrame</span>) = &#123;</span><br><span class="line">    <span class="comment">// replace nan values with 0</span></span><br><span class="line">    <span class="keyword">val</span> df_clean = df.na.fill(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// run the model on new data</span></span><br><span class="line">    <span class="keyword">val</span> result = model.transform(df_clean)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// display the results</span></span><br><span class="line">    result.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>第3步：定义自定义ML接收器</strong><br>为了能够将我们的分类器应用于新数据，我们需要创建一个新的接收器（流和输出之间的接口，在我们的例子中是XGBoost模型）。为此，我们需要一个自定义接收器（<code>MLSink</code>），一个抽象接收器提供器（<code>MLSinkProvider</code>）和provider（）的实现<code>XGBoostMLSinkProvider</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MLSinkProvider</span> <span class="keyword">extends</span> <span class="title">StreamSinkProvider</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(df: <span class="type">DataFrame</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createSink</span></span>(</span><br><span class="line">                  sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">                  parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">                  partitionColumns: <span class="type">Seq</span>[<span class="type">String</span>],</span><br><span class="line">                  outputMode: <span class="type">OutputMode</span>): <span class="type">MLSink</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MLSink</span>(process)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MLSink</span>(<span class="params">process: <span class="type">DataFrame</span> =&gt; <span class="type">Unit</span></span>) <span class="keyword">extends</span> <span class="title">Sink</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatch</span></span>(batchId: <span class="type">Long</span>, data: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    process(data)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XGBoostMLSinkProvider</span> <span class="keyword">extends</span> <span class="title">MLSinkProvider</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(df: <span class="type">DataFrame</span>) &#123;</span><br><span class="line">    <span class="type">XGBoostModel</span>.transform(df)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>第4步：在我们的自定义接收器中写入数据</strong><br>最后一步是定义一个将数据写入自定义接收器的查询。还必须定义检查点位置，以便应用程序“记住”在发生故障时在流中读取的最新行。如果我们运行程序，每个新批次的数据将显示在控制台上，同时包含预测的标签。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">"titanic.XGBoostMLSinkProvider"</span>)</span><br><span class="line">  .queryName(<span class="string">"XGBoostQuery"</span>)</span><br><span class="line">  .option(<span class="string">"checkpointLocation"</span>, checkpoint_location)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/18/yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/yuque/PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试/" itemprop="url">PySpark ML和XGBoost完全集成在Kaggle Titanic数据集上进行了测试</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T10:32:11+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本教程中，我们将讨论使用标准机器学习管道集成PySpark和XGBoost。我们将使用来自<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">泰坦尼克号的</a>数据<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">：机器学习灾难</a>中的众多Kaggle比赛之一。在开始之前，请知道您应该熟悉<a href="https://spark.apache.org/" target="_blank" rel="noopener">Apache Spark</a>和<a href="http://xgboost.readthedocs.io/en/latest/get_started/" target="_blank" rel="noopener">Xgboost</a>以及Python。本教程中使用的代码可以在<a href="https://github.com/BogdanCojocar/medium-articles/blob/master/titanic_xgboost/titanic_xgboost.ipynb" target="_blank" rel="noopener">github</a>上的Jupyther笔记本中找到。<br><a name="fa501ee4"></a></p>
<h4 id="第1步：下载或构建XGBoost-jar"><a href="#第1步：下载或构建XGBoost-jar" class="headerlink" title="第1步：下载或构建XGBoost jar"></a>第1步：下载或构建XGBoost jar</h4><p>python代码需要两个scala jar依赖项才能工作。您可以直接从maven下载它们：</p>
<ul>
<li><a href="https://mvnrepository.com/artifact/ml.dmlc/xgboost4j/0.72" target="_blank" rel="noopener">xgboost4j</a></li>
<li><a href="https://mvnrepository.com/artifact/ml.dmlc/xgboost4j-spark/0.72" target="_blank" rel="noopener">xgboost4j火花</a></li>
</ul>
<p>如果您希望自己构建它们，可以从我之前的<a href="https://medium.com/@bogdan.cojocar/how-to-make-xgboost-available-in-the-spark-notebook-de14e425c948" target="_blank" rel="noopener">教程中</a>找到如何进行构建。<br><a name="059757b0"></a></p>
<h4 id="第2步：下载XGBoost-python包装器"><a href="#第2步：下载XGBoost-python包装器" class="headerlink" title="第2步：下载XGBoost python包装器"></a>第2步：下载XGBoost python包装器</h4><p>您可以从<a href="https://github.com/dmlc/xgboost/files/2161553/sparkxgb.zip" target="_blank" rel="noopener">此处</a>下载PySpark XGBoost代码。这是我们要编写的部分和XGBoost scala实现之间的接口。我们将在本教程后面的代码中看到它如何集成。<br><a name="f8c06a61"></a></p>
<h4 id="第3步：启动一个新的Jupyter笔记本"><a href="#第3步：启动一个新的Jupyter笔记本" class="headerlink" title="第3步：启动一个新的Jupyter笔记本"></a>第3步：启动一个新的Jupyter笔记本</h4><p>我们将开始一个新的笔记本，以便能够编写我们的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p><a name="cdde53ac"></a></p>
<h4 id="第4步：将自定义XGBoost-jar添加到Spark应用程序"><a href="#第4步：将自定义XGBoost-jar添加到Spark应用程序" class="headerlink" title="第4步：将自定义XGBoost jar添加到Spark应用程序"></a>第4步：将自定义XGBoost jar添加到Spark应用程序</h4><p>在开始Spark之前，我们需要添加我们之前下载的jar。我们可以使用<code>--jars</code>标志来做到这一点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os </span><br><span class="line">os.environ [&apos;PYSPARK_SUBMIT_ARGS&apos;] =&apos; -  jars xgboost4j-spark-0.72.jar，xgboost4j-0.72.jar pyspark-shell&apos;</span><br></pre></td></tr></table></figure>
<p><a name="13d9ee40"></a></p>
<h4 id="第5步：将PySpark集成到Jupyther笔记本中"><a href="#第5步：将PySpark集成到Jupyther笔记本中" class="headerlink" title="第5步：将PySpark集成到Jupyther笔记本中"></a>第5步：将PySpark集成到Jupyther笔记本中</h4><p>使PySpark可用的最简单方法是使用该<code>[findspark](https://github.com/minrk/findspark)</code>软件包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import findspark </span><br><span class="line">findspark.init（）</span><br></pre></td></tr></table></figure>
<p><a name="dd505c79"></a></p>
<h4 id="第6步：启动spark会话"><a href="#第6步：启动spark会话" class="headerlink" title="第6步：启动spark会话"></a>第6步：启动spark会话</h4><p>我们现在准备开始火花会议。我们正在创建一个将在本地运行的spark应用程序，并将使用与使用的核心一样多的线程<code>local[*]</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(&quot;PySpark XGBOOST Titanic&quot;)\</span><br><span class="line">        .master(&quot;local[*]&quot;)\</span><br><span class="line">        .getOrCreate()</span><br></pre></td></tr></table></figure>
<p><a name="47ff7d29"></a></p>
<h4 id="第7步：添加PySpark-XGBoost包装器代码"><a href="#第7步：添加PySpark-XGBoost包装器代码" class="headerlink" title="第7步：添加PySpark XGBoost包装器代码"></a>第7步：添加PySpark XGBoost包装器代码</h4><p>正如我们现在有了spark会话，我们可以添加先前下载的包装器代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sparkContext.addPyFile(&quot;YOUR_PATH/sparkxgb.zip&quot;)</span><br></pre></td></tr></table></figure>
<p><a name="7f1a0f62"></a></p>
<h4 id="第8步：定义架构"><a href="#第8步：定义架构" class="headerlink" title="第8步：定义架构"></a>第8步：定义架构</h4><p>接下来，我们定义从csv读取的数据的模式。这通常比让火花推断模式更好，因为它消耗的资源更少，我们可以完全控制字段。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">schema = StructType(</span><br><span class="line">  [StructField(&quot;PassengerId&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Survival&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Pclass&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Name&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Sex&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Age&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;SibSp&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Parch&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Ticket&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Fare&quot;, DoubleType()),</span><br><span class="line">    StructField(&quot;Cabin&quot;, StringType()),</span><br><span class="line">    StructField(&quot;Embarked&quot;, StringType())</span><br><span class="line">  ])</span><br></pre></td></tr></table></figure>
<p><a name="44ee9a9a"></a></p>
<h4 id="步骤9：将csv数据读入数据帧"><a href="#步骤9：将csv数据读入数据帧" class="headerlink" title="步骤9：将csv数据读入数据帧"></a>步骤9：将csv数据读入数据帧</h4><p>我们将csv读入a <code>DataFrame</code>，确保我们提到我们有一个标题，我们也<code>null</code>用0 替换值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df_raw = spark\</span><br><span class="line">  .read\</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)\</span><br><span class="line">  .schema(schema)\</span><br><span class="line">  .csv(&quot;YOUR_PATH/train.csv&quot;)</span><br><span class="line">df = df_raw.na.fill(0)</span><br></pre></td></tr></table></figure>
<p><a name="f50eacdc"></a></p>
<h4 id="步骤10：C-将标称值转换为数字"><a href="#步骤10：C-将标称值转换为数字" class="headerlink" title="步骤10：C 将标称值转换为数字"></a>步骤10：C <strong>将标称值转换为数字</strong></h4><p>在浏览此步骤的代码之前，让我们简要介绍一些Spark ML概念。他们介绍了ML管道的概念，它是一组构建在其上的高级API <code>DataFrames</code>，可以更轻松地将多个算法组合到一个流程中。管道的主要元素是<code>Transformer</code>和<code>Estimator</code>。第一个可以表示可以将a <code>DataFrame</code>转换为另一个<code>DataFrame</code>的算法，而后者是可以适合a <code>DataFrame</code>来生成a 的算法<code>Transformer</code> 。<br>为了将名义值转换为数字值，我们需要<code>Transformer</code>为每列定义一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sexIndexer = StringIndexer()\</span><br><span class="line">  .setInputCol(&quot;Sex&quot;)\</span><br><span class="line">  .setOutputCol(&quot;SexIndex&quot;)\</span><br><span class="line">  .setHandleInvalid(&quot;keep&quot;)</span><br><span class="line">    </span><br><span class="line">cabinIndexer = StringIndexer()\</span><br><span class="line">  .setInputCol(&quot;Cabin&quot;)\</span><br><span class="line">  .setOutputCol(&quot;CabinIndex&quot;)\</span><br><span class="line">  .setHandleInvalid(&quot;keep&quot;)</span><br><span class="line">    </span><br><span class="line">embarkedIndexer = StringIndexer()\</span><br><span class="line">  .setInputCol(&quot;Embarked&quot;)\</span><br><span class="line">  .setOutputCol(&quot;EmbarkedIndex&quot;)\</span><br><span class="line">  .setHandleInvalid(&quot;keep&quot;)</span><br></pre></td></tr></table></figure>
<p>我们正在使用它<code>StringIndexer</code>来转换价值观。对于每个<code>Transformer</code>我们定义的输入列和输出列将包含修改后的值。<br><a name="fd3cbfb0"></a></p>
<h4 id="步骤11：将列组合成特征向量"><a href="#步骤11：将列组合成特征向量" class="headerlink" title="步骤11：将列组合成特征向量"></a>步骤11：<strong>将列组合成特征向量</strong></h4><p>我们将使用另一个<code>Transformer</code>将XGBoost分类中使用的列组合<code>Estimator</code>成一个向量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vectorAssembler = VectorAssembler()\</span><br><span class="line">  .setInputCols([&quot;Pclass&quot;, &quot;SexIndex&quot;, &quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;, &quot;CabinIndex&quot;, &quot;EmbarkedIndex&quot;])\</span><br><span class="line">  .setOutputCol(&quot;features&quot;)</span><br></pre></td></tr></table></figure>
<p><a name="220de1c1"></a></p>
<h4 id="第12步：定义XGBoostEstimator"><a href="#第12步：定义XGBoostEstimator" class="headerlink" title="第12步：定义XGBoostEstimator"></a>第12步：定义XGBoostEstimator</h4><p>在这一步中，我们定义了<code>Estimator</code>将产生模型的东西。这里使用的大多数参数都是默认的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">xgboost = XGBoostEstimator(</span><br><span class="line">    featuresCol=&quot;features&quot;, </span><br><span class="line">    labelCol=&quot;Survival&quot;, </span><br><span class="line">    predictionCol=&quot;prediction&quot;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们只定义<code>feature, label</code>（必须匹配来自的列<code>DataFrame</code>）和<code>prediction</code>包含分类器输出的新列。<br><a name="07d0e7ba"></a></p>
<h4 id="步骤13：建立管道和分类器"><a href="#步骤13：建立管道和分类器" class="headerlink" title="步骤13：建立管道和分类器"></a>步骤13：<strong>建立管道和分类器</strong></h4><p>在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作的顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost])</span><br></pre></td></tr></table></figure>
<p>输入<code>DataFrame</code>将被多次转换，最终将生成使用我们的数据训练的模型。<br><a name="4e3e3575"></a></p>
<h4 id="步骤14：训练模型并预测新的测试数据"><a href="#步骤14：训练模型并预测新的测试数据" class="headerlink" title="步骤14：训练模型并预测新的测试数据"></a>步骤14：训练模型并预测新的测试数据</h4><p>我们首先将数据分成火车和测试，然后我们将模型与火车数据拟合，最后我们看到我们为每位乘客获得的预测：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)</span><br><span class="line">model = pipeline.fit(trainDF)</span><br><span class="line">model.transform(testDF).select(col(&quot;PassengerId&quot;), col(&quot;prediction&quot;)).show()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/15/yuque/将ML投入生产：在Python中使用Apache Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/15/yuque/将ML投入生产：在Python中使用Apache Kafka/" itemprop="url">将ML投入生产：在Python中使用Apache Kafka</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-15T18:03:41+08:00">
                2019-03-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我们将说明如何使用一系列工具（即<a href="https://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>，<a href="https://mlflow.org/" target="_blank" rel="noopener">MLFlow</a>和<a href="https://aws.amazon.com/sagemaker/" target="_blank" rel="noopener">Sagemaker</a>）来帮助生产ML。为此，我们将设置一个简单的场景，我们希望它类似于一些真实用例，然后描述一个潜在的解决方案。可以在<a href="https://github.com/jrzaurin/ml_pipelines" target="_blank" rel="noopener">此处</a>找到包含所有代码的伴随仓库。<br><a name="c931653c"></a></p>
<h3 id="场景"><a href="#场景" class="headerlink" title="场景"></a><strong>场景</strong></h3><p>公司使用一系列服务收集数据，这些服务在用户/客户与公司的网站或应用程序交互时生成事件。当这些交互发生时，算法需要<strong>实时</strong>运行<strong>，</strong>并且需要根据算法的输出（或预测）采取一些立即行动。最重要的是，经过_ñ<em>相互作用（或意见）的算法需要重新训练<strong>不停止</strong>的</em>预测_ <em>服务，</em>因为用户将保持互动。<br>对于这里的练习，我们使用了<a href="https://archive.ics.uci.edu/ml/datasets/adult" target="_blank" rel="noopener">成人</a>数据集，其目标是根据年龄，原籍国等来预测个人是否获得高于/低于50k的收入。为了使该数据集适应前面描述的情景，可以假设通过在线问卷/表格收集该年龄，本国等，我们需要预测用户是否实时获得高/低收入。如果收入高，那么我们会立即给他们打电话/给他们发电子邮件。然后，在_N个_新观察之后，我们重新训练算法，同时我们继续预测新用户。<br><strong>解决方案</strong><br>图1是潜在解决方案的图示。为了实现这个解决方案，我们使用了<a href="https://github.com/dpkp/kafka-python" target="_blank" rel="noopener">Kafka-Python</a>（可以在<a href="https://github.com/dpkp/kafka-python" target="_blank" rel="noopener">这里</a>找到一个很好的教程），以及<a href="https://lightgbm.readthedocs.io/en/latest/" target="_blank" rel="noopener">LightGBM</a>和<a href="https://github.com/hyperopt/hyperopt" target="_blank" rel="noopener">Hyperopt</a>或<a href="https://github.com/HunterMcGushion/hyperparameter_hunter" target="_blank" rel="noopener">HyperparameterHunter</a>。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552644337722-6e53ed5f-2e63-41eb-bddd-1d1968e90f10.png#align=left&amp;display=inline&amp;height=443&amp;name=1___golwnXpxecbplKvGqlPA.png&amp;originHeight=594&amp;originWidth=1000&amp;size=180599&amp;status=done&amp;width=746" alt="1___golwnXpxecbplKvGqlPA.png"><br>图1. <strong>实时预测ML管道</strong>。以下提供完整描述<br>我们将在本练习中使用的唯一Python“局外人”是<a href="https://kafka.apache.org/" target="_blank" rel="noopener">Apache-Kafka</a>（我们将使用python API Kafka-Python，但仍然需要在您的系统中安装Kafka）。如果你在Mac上，只需使用Homebrew：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install kafka</span><br></pre></td></tr></table></figure>
<p>这也将安装zookeeper依赖项。<br>如前所述，我们使用了Adult数据集。这是因为我们的目的是说明潜在的ML管道并提供有用的代码，同时保持相对简单。但请注意，此处描述的管道原则上与数据无关。当然，预处理将根据数据的性质而改变，但如果不相同，管道组件将保持相似。<br><strong>初始化实验</strong><br>您可以在我们的<a href="https://github.com/jrzaurin/ml_pipelines" target="_blank" rel="noopener">仓库中</a>找到用于此帖子的代码（以及更多）。在那里，有一个名为的脚本<code>initialize.py</code> 。该脚本将下载数据集，设置目录结构，预处理数据，在训练数据集上训练初始模型并优化该模型的超参数。在现实世界中，这将对应于通常的实验阶段和离线训练初始算法的过程。<br>在这篇文章中，我们希望主要关注管道和相应的组件而不是ML。尽管如此，让我们简单地提一下我们在本练习中使用的ML工具。<br>鉴于我们正在使用的数据集，数据预处理非常简单。我们编写了一个名为的自定义类<code>[FeatureTools](https://github.com/jrzaurin/ml_pipelines/blob/master/utils/feature_tools.py)</code>，可以<code>utils</code>在repo 中的模块中找到。这个类有<code>.fit</code>和  <code>.transform</code>方法将标准化/缩放数字特征，编码分类特征并生成我们称之为“交叉列”，这是两个（或更多）分类特征之间的笛卡尔积的结果。<br>处理完数据后，我们使用LightGBM将模型与Hyperopt或HyperparameterHunter相匹配，以执行超参数优化。可以在<code>train</code>模块中找到与此任务相关的代码，其中可以找到两个脚本<code>[train_hyperop](https://github.com/jrzaurin/ml_pipelines/blob/master/train/train_hyperopt.py).py</code>和<code>[train_hyperparameterhunter](https://github.com/jrzaurin/ml_pipelines/blob/master/train/train_hyperparameterhunter.py).py</code> 。<br>我们可能会在python（<a href="https://scikit-optimize.github.io/" target="_blank" rel="noopener">Skopt</a>，Hyperopt和HyperparameterHunder）中编写一个单独的帖子来比较超参数优化包，但是现在，请知道：如果你想要速度，那么使用Hyperopt。如果您不关心速度并且想要详细跟踪优化例程，请使用HyperparameterHunter。用<a href="https://www.linkedin.com/in/huntermcgushion/" target="_blank" rel="noopener">Hunter</a> McGushion 的话来说，包装的创造者：</p>
<blockquote>
<p><em>“长期以来，超参数优化一直是一个耗时的过程，只是指向了进一步优化的方向，然后你基本上不得不重新开始。”</em></p>
</blockquote>
<p>HyperparameterHunter就是为了解决这个问题，它做得非常好。目前，该软件包是建立在Skopt之上的，这就是为什么它比Hyperopt慢得多。但是，我知道有人努力将Hyperopt作为HyperparameterHunter的另一个后端包含在内。当发生这种情况时，不会有任何争议，HyperparameterHunter应该是您的首选工具。<br>尽管如此，如果有人感兴趣，我在回购中包含了一个<a href="https://github.com/jrzaurin/ml_pipelines/blob/master/notebooks/skopt_vs_hyperopt.ipynb" target="_blank" rel="noopener">笔记本</a>，比较了Skopt和Hyperopt的表现。<br>让我们现在转到管道流程本身。<br><strong>App Messages Producer</strong><br>这意味着生产管道的哪个部分可能看起来相对简单。因此，我们直接使用Adult数据集生成消息（JSON对象）。<br>在现实世界中，人们将拥有许多可以生成事件的服务。从那里，有一个选项。这些事件中的信息可能存储在数据库中，然后通过常规查询进行汇总。从那里，Kafka服务将消息发布到管道中。或者，这些事件中的所有信息可以直接发布到不同的主题中，“聚合服务”可以将所有信息存储在单个消息中，然后将其发布到管道中（当然，也可以组合使用他们俩）。<br>例如，可能允许用户通过Facebook或Google注册，收集他们的姓名和电子邮件地址。然后他们可能会被要求填写调查问卷，我们会继续收集他们进展的事件。在此过程中的某个时刻，所有这些事件将在单个消息中聚合，然后通过Kafka生产者发布。这篇文章中的管道将从聚合了所有相关信息的点开始。我们这里的消息是Adult数据集中的单个观察。下面我们将包含消息内容的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">’&#123;“age”:<span class="number">25</span>,”workclass”:”Private”,”fnlwgt”:<span class="number">226802</span>,”education”:”<span class="number">11</span>th”,”marital_status”:”Never-married”,”occupation”:”Machine-op-inspct”,”relationship”:”Own-child”,”race”:”Black”,”gender”:”Male”,”capital_gain”:<span class="number">0</span>,”capital_loss”:<span class="number">0</span>,”hours_per_week”:<span class="number">40</span>,”native_country”:”United-States”,”income_bracket”:”&lt;=<span class="number">50</span>K.”&#125;’</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/jrzaurin/ml_pipelines/blob/master/sample_app.py" target="_blank" rel="noopener">App / Service</a>的核心（图1中最灰色，最左边的框）是下面的代码段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">df_test = pd.read_csv(PATH/<span class="string">'adult.test'</span>)</span><br><span class="line">df_test[<span class="string">'json'</span>] = df_test.apply(<span class="keyword">lambda</span> x: x.to_json(), axis=<span class="number">1</span>)</span><br><span class="line">messages = df_test.json.tolist()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_producing</span><span class="params">()</span>:</span></span><br><span class="line">	producer = KafkaProducer(bootstrap_servers=KAFKA_HOST)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">		message_id = str(uuid.uuid4())</span><br><span class="line">		message = &#123;<span class="string">'request_id'</span>: message_id, <span class="string">'data'</span>: json.loads(messages[i])&#125;</span><br><span class="line"></span><br><span class="line">		producer.send(<span class="string">'app_messages'</span>, json.dumps(message).encode(<span class="string">'utf-8'</span>))</span><br><span class="line">		producer.flush()</span><br><span class="line"></span><br><span class="line">		print(<span class="string">"\033[1;31;40m -- PRODUCER: Sent message with id &#123;&#125;"</span>.format(message_id))</span><br><span class="line">		sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_consuming</span><span class="params">()</span>:</span></span><br><span class="line">	consumer = KafkaConsumer(<span class="string">'app_messages'</span>, bootstrap_servers=KAFKA_HOST)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">for</span> msg <span class="keyword">in</span> consumer:</span><br><span class="line">		message = json.loads(msg.value)</span><br><span class="line">		<span class="keyword">if</span> <span class="string">'prediction'</span> <span class="keyword">in</span> message:</span><br><span class="line">			request_id = message[<span class="string">'request_id'</span>]</span><br><span class="line">			print(<span class="string">"\033[1;32;40m ** CONSUMER: Received prediction &#123;&#125; for request id &#123;&#125;"</span>.format(message[<span class="string">'prediction'</span>], request_id))</span><br></pre></td></tr></table></figure>
<p>请注意，我们使用测试数据集来生成消息。这是因为我们设计了一个尽可能与真实世界相似的场景（在一定限度内）。考虑到这一点，我们使用训练数据集来构建初始<code>model</code>和<code>dataprocessor</code>对象。然后，我们使用测试数据集生成消息，目的是模拟随时间接收新信息的过程。<br>关于上面的代码片段，简单地说，生产者会将消息发布到管道（<code>start_producing()</code>）中并使用带有最终预测（<code>start_consuming()</code>）的消息。与我们在此描述的管道不同的方式不包括流程的开始（事件收集和聚合），我们也跳过最后，即如何处理最终预测。尽管如此，我们还是简要讨论了一些用例，这些用例可能会在帖子结尾处有用，这将说明最后阶段。<br>实际上，除了忽略过程的开始和结束之外，我们认为这条管道在现实世界中可以使用的管道相当好。因此，我们希望我们的仓库中包含的代码对您的某些项目有用。<br><strong>预测者和训练师</strong><br>该实现的主要目标是实时运行算法并在<strong>不</strong>停止预测服务的<strong>情况下</strong>每_N次_观察重新训练它。为此，我们实现了两个组件，<strong>Predictor</strong>（在repo中）和<strong>Trainer</strong>（）。<code>[predictor](https://github.com/jrzaurin/ml_pipelines/blob/master/predictor.py).py`</code><a href="https://github.com/jrzaurin/ml_pipelines/blob/master/trainer.py" target="_blank" rel="noopener">trainer</a>.py<code>&lt;br /&gt;现在让我们逐一描述图1中显示的数字，使用代码片段作为我们的指南。请注意，下面的过程假设有一个运行</code>initialize.py<code>脚本，因此初始文件</code>model.p<code>和</code>dataprocessor.p`文件存在于相应的目录中。另外，请强调下面的代码包含Predictor和Trainer的核心。有关完整代码，请参阅<a href="https://github.com/jrzaurin/ml_pipelines" target="_blank" rel="noopener">回购</a>。<br><strong>预报器</strong><br>Predictor代码的核心如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(model_id, messages_count, batch_id)</span>:</span></span><br><span class="line">	<span class="keyword">for</span> msg <span class="keyword">in</span> consumer:</span><br><span class="line">		message = json.loads(msg.value)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> is_retraining_message(msg):</span><br><span class="line">			model_fname = <span class="string">'model_&#123;&#125;_.p'</span>.format(model_id)</span><br><span class="line">			model = reload_model(MODELS_PATH/model_fname)</span><br><span class="line">			print(<span class="string">"NEW MODEL RELOADED &#123;&#125;"</span>.format(model_id))</span><br><span class="line"></span><br><span class="line">		<span class="keyword">elif</span> is_application_message(msg):</span><br><span class="line">			request_id = message[<span class="string">'request_id'</span>]</span><br><span class="line">			pred = predict(message[<span class="string">'data'</span>], column_order)</span><br><span class="line">			publish_prediction(pred, request_id)</span><br><span class="line"></span><br><span class="line">			append_message(message[<span class="string">'data'</span>], MESSAGES_PATH, batch_id)</span><br><span class="line">			messages_count += <span class="number">1</span></span><br><span class="line">			<span class="keyword">if</span> messages_count % RETRAIN_EVERY == <span class="number">0</span>:</span><br><span class="line">				model_id = (model_id + <span class="number">1</span>) % (EXTRA_MODELS_TO_KEEP + <span class="number">1</span>)</span><br><span class="line">				send_retrain_message(model_id, batch_id)</span><br><span class="line">				batch_id += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><strong>（1a）</strong><code>predictor.py</code>片段中的第12行。预测器将从应用程序/服务接收消息，它将进行数据处理并在接收消息时实时运行模型。所有这些都是使用函数中的现有对象<code>dataprocessor</code>和<code>model</code>对象发生的<code>predict</code>。<br><strong>（1b）</strong><code>predictor.py</code>片段中的第13行。一旦我们运行预测，Predictor将发布<code>publish_prediction()</code>最终将由App / Service接收的result（）。<br><strong>（2）</strong><code>predictor.py</code>片段中的第17-20行。每条<code>RETRAIN_EVERY</code>消息，Predictor都会发布一条“ <em>重新训练</em> ”消息（<code>send_retrain_message()</code>），由培训师阅读。<br><strong>训练者</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">()</span>:</span></span><br><span class="line">	consumer = KafkaConsumer(RETRAIN_TOPIC, bootstrap_servers=KAFKA_HOST)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> msg <span class="keyword">in</span> consumer:</span><br><span class="line">		message = json.loads(msg.value)</span><br><span class="line">		<span class="keyword">if</span> <span class="string">'retrain'</span> <span class="keyword">in</span> message <span class="keyword">and</span> message[<span class="string">'retrain'</span>]:</span><br><span class="line">			model_id = message[<span class="string">'model_id'</span>]</span><br><span class="line">			batch_id = message[<span class="string">'batch_id'</span>]</span><br><span class="line">			message_fname = <span class="string">'messages_&#123;&#125;_.txt'</span>.format(batch_id)</span><br><span class="line">			messages = MESSAGES_PATH/message_fname</span><br><span class="line"></span><br><span class="line">			train(model_id, messages)</span><br><span class="line">			publish_traininig_completed(model_id)</span><br></pre></td></tr></table></figure>
<p><strong>（3）</strong><code>trainer.py</code>片段中的第12行。培训师将阅读该消息并使用新的累积数据集（<code>train()</code>）触发重新训练过程。<br>这是原始数据集加上<code>RETRAIN_EVERY</code>新的观察结果。列车功能将独立于<strong>1a</strong>和<strong>1b中</strong>描述的过程运行<em>“初始化实验”</em><strong> </strong>部分中描述的整个过程。换句话说，训练者将重新训练模型，而预测器在消息到达时保持预测。<br>在这个阶段值得一提的是，在这里我们发现我们的实现与将在现实世界中使用的实现之间存在进一步的差异。在我们的实现中，一旦处理了<code>RETRAIN_EVERY</code>多个观察结果，就可以重新训练算法。这是因为我们使用Adult测试数据集来生成消息，其中包括目标列（“ _income_braket_ ”）。在真实的单词中，基于算法输出所采取的动作的真实结果通常在算法运行之后不容易访问，但是一段时间之后。在那种情况下，另一个过程应该是收集真实的结果，一旦收集的真实结果的数量等于<code>RETRAIN_EVERY</code>算法将被重新训练。<br>例如，假设此管道实现了电子商务的实时推荐系统。我们已经离线训练了一个推荐算法，目标列是我们的用户喜欢我们建议的分类表示：0,1,2和3对于不喜欢或与项目交互的用户，喜欢该项目（例如点击像按钮），将项目添加到他们的篮子，并分别购买该项目。当系统提供建议时，我们仍然不知道用户最终会做什么。<br>因此，随着用户信息在网站（或应用程序）中导航时收集和存储用户信息，第二个过程应该收集我们建议的最终结果。只有当两个进程都收集了<code>RETRAIN_EVERY</code>消息和结果时，才会对算法进行重新训练。<br><strong>（4）</strong><code>trainer.py</code>片段中的第13行。重新训练完成后，将发布带有相应信息的消息（<code>published_training_completed()</code>）。<br><strong>（5）</strong><code>predictor.py</code>片段中的第5-8行。Predictor的消费者订阅了两个主题：<code>[‘app_messages’, ‘retrain_topic’]</code>。一旦它通过“retrain_topic”接收到重新训练完成的信息，它将加载新模型并像往常一样保持过程，而不会在过程中的任何时间停止。<br><a name="647586e9"></a></p>
<h3 id="如何运行管道"><a href="#如何运行管道" class="headerlink" title="如何运行管道"></a>如何运行管道</h3><p>在配套仓库中，我们已经包含了如何运行管道（本地）的说明。其实很简单。</p>
<ol>
<li>启动zookeper和kafka：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ brew services start zookeeper</span><br><span class="line">==&gt; Successfully started `zookeeper` (label: homebrew.mxcl.zookeeper)</span><br><span class="line">$ brew services start kafka</span><br><span class="line">==&gt; Successfully started `kafka` (label: homebrew.mxcl.kafka)</span><br></pre></td></tr></table></figure>
<p>2.运行initialize.py：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python initialize.py</span><br></pre></td></tr></table></figure>
<p>3.在终端＃1中运行预测器（或训练器）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python predictor.py</span><br></pre></td></tr></table></figure>
<p>4.在终端＃2中运行训练器（或预测器）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python trainer.py</span><br></pre></td></tr></table></figure>
<p>5.在终端＃3中运行示例应用程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python samplea_app.py</span><br></pre></td></tr></table></figure>
<p>然后，一旦处理了N条消息，您应该看到如下内容：</p>
<p><strong>右上方终端</strong>：我们重新训练了模型，Hyperopt已经进行了10次评估（在实际练习中，这些应该是几百次）。<strong>左上方终端</strong>：一旦对模型进行了重新训练和优化，我们就会看到预测器如何加载新模型（在新LightGBM版本的恼人警告消息之后）。<strong>底部终端</strong>：服务照常进行。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552645079397-f02448b1-2084-4a1b-b5d5-0b51d8af3dc2.png#align=left&amp;display=inline&amp;height=557&amp;name=1_xIT4U_OODiPtDuiPsCbyXw.png&amp;originHeight=746&amp;originWidth=1000&amp;size=347205&amp;status=done&amp;width=746" alt="1_xIT4U_OODiPtDuiPsCbyXw.png"></p>
<p>一些潜在的用例<br>以下是（很多）其他一些潜在用例。<br>实时调整在线旅程<br>让我们考虑出售一些商品的电子商务。当用户浏览网站时，我们会收集有关其行为信息的活动。我们之前已经培训了一种算法，我们知道在10次交互之后，我们可以很好地了解客户是否最终会购买我们的产品。此外，我们也知道他们可能购买的产品可能会很昂贵。因此，我们希望“在旅途中”定制他们的旅程，以促进他们的购物体验。这里的定制可能意味着什么，从缩短行程到改变页面布局。<br>2. 电子邮件/致电您的客户<br>与之前的用例类似，我们现在假设客户决定停止旅程（无聊，缺乏时间，可能太复杂等）。如果算法预测该客户具有很大的潜力，我们可以立即使用像本文所述的管道，或者使用受控延迟，发送电子邮件或调用它们。<br>下一步<br>记录和监控：在即将发布的帖子中，我们将通过MLFlow在管道记录和监控功能中插入。与HyperparameterHunter一起，该解决方案将自动跟踪模型性能和超参数优化，同时提供可视化监控。<br>流量管理：此处描述的解决方案以及相应的代码已经过设计，因此可以在笔记本电脑中轻松地在本地和手动运行。然而，人们会认为在现实生活中，这将需要大规模运行云，而不是手动（请）。在那个阶段，如果我们可以使用涵盖整个机器学习工作流程的完全托管服务，那将是理想的，因此我们不需要关心维护服务，版本控制等。为此目的，我们将使用Sagemaker，它是构建的正是为了这个目的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/15/yuque/使用MLflow赋予Spark功能/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/15/yuque/使用MLflow赋予Spark功能/" itemprop="url">使用MLflow赋予Spark功能</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-15T16:33:35+08:00">
                2019-03-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇文章旨在介绍我们使用<a href="https://mlflow.org/" target="_blank" rel="noopener"><strong>MLflow</strong></a>的初步经验。<br>我们将通过记录所有探索性迭代，开始使用自己的<a href="https://mlflow.org/docs/latest/tracking.html" target="_blank" rel="noopener">跟踪服务器</a>发现MLflow 。然后，我们将展示使用UDF将Spark与MLflow相关联的经验。<br><a name="50f198f0"></a></p>
<h4 id="上下文"><a href="#上下文" class="headerlink" title="上下文"></a>上下文</h4><p>我们利用机器学习和人工智能的力量，使人们能够控制自己的健康和福祉。机器学习模型因此是我们正在开发的数据产品的核心，这就是为什么MLFLow，一个涵盖ML生命周期所有方面的开源平台引起了我们的注意。<br><a name="MLflow"></a></p>
<h3 id="MLflow"><a href="#MLflow" class="headerlink" title="MLflow"></a>MLflow</h3><p>MLflow的主要目标是在ML之上提供额外的层，允许数据科学家与几乎任何机器学习库（<a href="https://github.com/h2oai" target="_blank" rel="noopener"><em>h2o，</em></a><a href="https://keras.io/" target="_blank" rel="noopener"><em>keras，</em></a><a href="http://mleap-docs.combust.ml/" target="_blank" rel="noopener"><em>mleap，</em></a><a href="https://pytorch.org/" target="_blank" rel="noopener"><em>pytorch，</em></a><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener"><em>sklearn和</em></a><a href="https://www.tensorflow.org/" target="_blank" rel="noopener"><em>tensorflow</em></a>）一起工作，同时，它将他们的工作带到另一个层次。<br>MLflow提供三个组件：</p>
<ul>
<li><a href="https://mlflow.org/docs/latest/tracking.html" target="_blank" rel="noopener"><strong>跟踪</strong></a>  - 记录和查询实验：代码，数据，配置和结果。跟踪建模进度非常有用。</li>
<li><a href="https://mlflow.org/docs/latest/projects.html" target="_blank" rel="noopener"><strong>项目</strong></a>  - 在任何平台（<strong>_即_</strong> <a href="https://aws.amazon.com/sagemaker" target="_blank" rel="noopener"><em>Sagemaker</em></a>）上可重复运行的包装格式。</li>
<li><a href="https://mlflow.org/docs/latest/models.html" target="_blank" rel="noopener"><strong>模型</strong></a>  - 将模型发送到各种部署工具的通用格式。<blockquote>
<p><strong>MLflow</strong>（目前处于alpha版本）是一个管理ML生命周期的开源平台，包括实验，可重复性和部署。<br><a name="181d871e"></a></p>
</blockquote>
<h4 id="设置MLflow"><a href="#设置MLflow" class="headerlink" title="设置MLflow"></a>设置MLflow</h4>为了使用MLflow，我们首先需要设置所有Python环境以使用MLflow，我们将使用<a href="https://github.com/pyenv/pyenv" target="_blank" rel="noopener">PyEnv </a>_（_<a href="https://gist.github.com/chris-zen/9e61db6924bd37fbe414f648614ca4c5" target="_blank" rel="noopener"><em>在Mac中</em></a><strong>_设置_</strong>_ → _<a href="https://gist.github.com/chris-zen/9e61db6924bd37fbe414f648614ca4c5" target="_blank" rel="noopener"><em>Python）</em></a>。这将提供一个虚拟环境，我们可以在其中安装运行它所需的所有库。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pyenv install 3.7.0</span><br><span class="line">pyenv global 3.7.0 # Use Python 3.7</span><br><span class="line">mkvirtualenv mlflow # Create a Virtual Env with Python 3.7</span><br><span class="line">workon mlflow</span><br></pre></td></tr></table></figure>
<p>安装所需的库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install mlflow==0.7.0 \</span><br><span class="line">            Cython==0.29 \ </span><br><span class="line">            numpy==1.14.5 \</span><br><span class="line">            pandas==0.23.4 \</span><br><span class="line">            pyarrow==0.11.0</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意：</strong>我们使用PyArrow将模型作为UDF启动。PyArrow和Numpy版本需要修复，因为最新的版本之间存在一些冲突。</p>
</blockquote>
<p><a name="9a02c96e"></a></p>
<h4 id="启动跟踪UI"><a href="#启动跟踪UI" class="headerlink" title="启动跟踪UI"></a>启动跟踪UI</h4><p>MLflow Tracking允许我们使用<a href="https://mlflow.org/docs/latest/python_api/index.html#python-api" target="_blank" rel="noopener">Python</a>和<a href="https://mlflow.org/docs/latest/rest-api.html#rest-api" target="_blank" rel="noopener">REST</a> API 记录和查询实验。此外，还可以定义我们将存储模型工件的位置（<em>Localhost，</em><a href="https://mlflow.org/docs/latest/tracking.html#amazon-s3" target="_blank" rel="noopener"><em>Amazon S3</em></a>_，_<a href="https://mlflow.org/docs/latest/tracking.html#azure-blob-storage" target="_blank" rel="noopener"><em>Azure Blob存储</em></a>_，_<a href="https://mlflow.org/docs/latest/tracking.html#google-cloud-storage" target="_blank" rel="noopener"><em>Google云存储</em></a>_或_<a href="https://mlflow.org/docs/latest/tracking.html#sftp-server" target="_blank" rel="noopener"><em>SFTP服务器</em></a>）。由于我们使用AWS ，因此我们将尝试将<strong>S3</strong>作为工件存储。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Running a Tracking Server</span><br><span class="line">mlflow server \</span><br><span class="line">    --file-store /tmp/mlflow/fileStore \</span><br><span class="line">    --default-artifact-root s3://&lt;bucket&gt;/mlflow/artifacts/ \</span><br><span class="line">    --host localhost</span><br><span class="line">    --port 5000</span><br></pre></td></tr></table></figure>
<p>MLflow建议使用<strong>持久性文件存储</strong>。这<code>file-store</code>是服务器存储运行和实验元数据的位置。因此，在运行服务器时，请确保这指向持久文件系统位置。在这里，我们只是<code>/tmp</code>用于实验。<br>请记住，如果我们想使用mlflow服务器运行旧实验，它们必须存在于文件存储中。但是，如果没有它们，我们仍然可以在UDF中使用它们，因为只需要模型路径。</p>
<blockquote>
<p><strong><em>注意：</em></strong>请记住跟踪UI，模型客户端必须能够访问工件位置。这意味着，无论跟踪UI是否在EC2实例上，如果我们在本地运行MLflow，我们的机器应该可以直接访问S3来编写工件模型。</p>
</blockquote>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552639247139-4605b9cb-f981-43fc-a969-9a2155d75965.png#align=left&amp;display=inline&amp;height=239&amp;name=1_z6iObWdxXOtcIj_cUk2gag.png&amp;originHeight=642&amp;originWidth=2000&amp;size=114273&amp;status=done&amp;width=746" alt="1_z6iObWdxXOtcIj_cUk2gag.png"><br><a name="7d519d5b"></a></p>
<h4 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h4><p>跟踪服务器运行后，我们可以开始训练我们的模型。<br>作为一个例子，我们将使用<a href="https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.py" target="_blank" rel="noopener"><strong><em>MLflow Sklearn示例中</em></strong></a>提供的wine示例的修改。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MLFLOW_TRACKING_URI=http://localhost:5000 python wine_quality.py \</span><br><span class="line">  --alpha 0.9</span><br><span class="line">  --l1_ration 0.5</span><br><span class="line">  --wine_file ./data/winequality-red.csv</span><br></pre></td></tr></table></figure>
<p>如前所述，MLflow允许记录模型的参数，度量和工件，因此我们可以跟踪这些在不同迭代中如何演变。此功能非常有用，因为我们可以通过检查Tracking Server来重现我们的最佳模型，或者验证哪些代码正在执行所需的迭代，因为<strong>它记录</strong><em>（免费）</em><strong>git哈希提交</strong>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">with mlflow.start_run():</span><br><span class="line"></span><br><span class="line">    ... model ...</span><br><span class="line"></span><br><span class="line">    mlflow.log_param(&quot;source&quot;, wine_path)</span><br><span class="line">    mlflow.log_param(&quot;alpha&quot;, alpha)</span><br><span class="line">    mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)</span><br><span class="line"></span><br><span class="line">    mlflow.log_metric(&quot;rmse&quot;, rmse)</span><br><span class="line">    mlflow.log_metric(&quot;r2&quot;, r2)</span><br><span class="line">    mlflow.log_metric(&quot;mae&quot;, mae)</span><br><span class="line"></span><br><span class="line">    mlflow.set_tag(&apos;domain&apos;, &apos;wine&apos;)</span><br><span class="line">    mlflow.set_tag(&apos;predict&apos;, &apos;quality&apos;)</span><br><span class="line">    mlflow.sklearn.log_model(lr, &quot;model&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552639366867-dba4caa9-6ea2-4ca8-b59a-737bbfa774fc.png#align=left&amp;display=inline&amp;height=365&amp;name=1_3soEGGuFumQXV5LSZNCnrA.png&amp;originHeight=489&amp;originWidth=1000&amp;size=130330&amp;status=done&amp;width=746" alt="1_3soEGGuFumQXV5LSZNCnrA.png"><br><a name="146d11ee"></a></p>
<h4 id="服务模型"><a href="#服务模型" class="headerlink" title="服务模型"></a>服务模型</h4><p> 使用“ &gt; <strong><em>mlflow服务器</em></strong>&gt; _ ”_启动的&gt; <strong>MLflow跟踪服务器</strong>还托管REST API，用于跟踪运行并将数据写入本地文件系统。您可以使用“ &gt; <strong>_MLFLOW_TRACKING_URI_</strong>&gt; _”<em>环境变量指定跟踪服务器URI，MLflow跟踪API会自动与该URI处的跟踪服务器通信，以创建/获取运行信息，记录指标等。<br>**</em>参考：<em>**[</em>文档//运行跟踪服务器<em>](<a href="https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server" target="_blank" rel="noopener">https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server</a>)<br>为了提供模型，我们只需要运行一个<strong>跟踪服务器</strong>（**</em>参见_<strong><em>启动UI</em>）和一个</strong>模型运行ID**。<br><br><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552639490766-7db77f84-79f9-486e-963c-c574d85d438e.png#align=left&amp;display=inline&amp;height=120&amp;name=1_vBVbyT3F_YFjjfJuSDVrUA.png&amp;originHeight=161&amp;originWidth=1000&amp;size=81585&amp;status=done&amp;width=746" alt="1_vBVbyT3F_YFjjfJuSDVrUA.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Serve a sklearn model through 127.0.0.0:5005</span><br><span class="line">MLFLOW_TRACKING_URI=http://0.0.0.0:5000 mlflow sklearn serve \</span><br><span class="line">  --port 5005  \</span><br><span class="line">  --run_id 0f8691808e914d1087cf097a08730f17 \</span><br><span class="line">  --model-path model</span><br></pre></td></tr></table></figure>
<p>要使用<strong>MLflow服务功能</strong>为模型提供<strong>服务，</strong>我们需要访问跟踪UI，因此只需指定即可检索模型信息<code>--run_id</code> 。<br>一旦跟踪服务器为模型提供服务，我们就可以查询新的模型端点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Query Tracking Server Endpoint</span><br><span class="line">curl -X POST \</span><br><span class="line">  http://127.0.0.1:5005/invocations \</span><br><span class="line">  -H &apos;Content-Type: application/json&apos; \</span><br><span class="line">  -d &apos;[</span><br><span class="line">	&#123;</span><br><span class="line">		&quot;fixed acidity&quot;: 3.42, </span><br><span class="line">		&quot;volatile acidity&quot;: 1.66, </span><br><span class="line">		&quot;citric acid&quot;: 0.48, </span><br><span class="line">		&quot;residual sugar&quot;: 4.2, </span><br><span class="line">		&quot;chloridessssss&quot;: 0.229, </span><br><span class="line">		&quot;free sulfur dsioxide&quot;: 19, </span><br><span class="line">		&quot;total sulfur dioxide&quot;: 25, </span><br><span class="line">		&quot;density&quot;: 1.98, </span><br><span class="line">		&quot;pH&quot;: 5.33, </span><br><span class="line">		&quot;sulphates&quot;: 4.39, </span><br><span class="line">		&quot;alcohol&quot;: 10.8</span><br><span class="line">	&#125;</span><br><span class="line">]&apos;</span><br><span class="line"></span><br><span class="line">&gt; &#123;&quot;predictions&quot;: [5.825055635303461]&#125;</span><br></pre></td></tr></table></figure>
<p><a name="e475db1c"></a></p>
<h3 id="从Spark运行模型"><a href="#从Spark运行模型" class="headerlink" title="从Spark运行模型"></a>从Spark运行模型</h3><p>虽然通过训练模型和使用服务功能（<strong><em>ref：</em></strong><a href="https://mlflow.org/docs/latest/models.html#local" target="_blank" rel="noopener"><em>mlflow // docs // models #local）</em></a>实时为模型提供服务非常强大，但使用Spark（批量或流式传输）应用模型更是如此强大，因为它加入了分配力量。<a href="https://mlflow.org/docs/latest/models.html#local" target="_blank" rel="noopener"></a><br>想象一下，进行离线训练，然后以更简单的方式将输出模型应用于所有数据。这就是Spark和MLflow共同完美的地方。<br><a name="92cfd913"></a></p>
<h4 id="安装PySpark-Jupyter-Spark"><a href="#安装PySpark-Jupyter-Spark" class="headerlink" title="安装PySpark + Jupyter + Spark"></a>安装PySpark + Jupyter + Spark</h4><p><strong><em>参考：</em></strong><a href="https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f" target="_blank" rel="noopener"><em>PySpark开始 - Jupyter</em></a><br>展示我们如何将MLflow模型应用于Spark数据帧。我们需要使用PySpark设置Jupyter笔记本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">首先下载最新的稳定Apache Spark（当前版本2.3.2）。</span><br><span class="line"></span><br><span class="line">cd ~/Downloads/</span><br><span class="line">tar -xzf spark-2.3.2-bin-hadoop2.7.tgz</span><br><span class="line">mv ~/Downloads/spark-2.3.2-bin-hadoop2.7 ~/</span><br><span class="line">ln -s ~/spark-2.3.2-bin-hadoop2.7 ~/spark̀</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在我们的virtualEnv中安装PySpark和Jupyter</span><br><span class="line"></span><br><span class="line">pip install pyspark jupyter</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">设置Environmnet变量</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=~/spark</span><br><span class="line">export PATH=$SPARK_HOME/bin:$PATH</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=jupyter</span><br><span class="line">export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook --notebook-dir=$&#123;HOME&#125;/Projects/notebooks&quot;</span><br><span class="line">通过定义notebook-dir，我们将能够将我们的笔记本存储和保存在所需的文件夹中。</span><br></pre></td></tr></table></figure>
<p><a name="f6e12a2e"></a></p>
<h4 id="从PySpark启动Jupyter"><a href="#从PySpark启动Jupyter" class="headerlink" title="从PySpark启动Jupyter"></a>从PySpark启动Jupyter</h4><p>由于我们将Jupyter配置为<strong><em>PySpark驱动程序</em></strong>，现在我们可以使用附加到我们笔记本的PySpark上下文启动Jupyter。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(mlflow) afranzi:~$ pyspark</span><br><span class="line">[I 19:05:01.572 NotebookApp] sparkmagic extension enabled!</span><br><span class="line">[I 19:05:01.573 NotebookApp] Serving notebooks from local directory: /Users/afranzi/Projects/notebooks</span><br><span class="line">[I 19:05:01.573 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 19:05:01.573 NotebookApp] http://localhost:8888/?token=c06252daa6a12cfdd33c1d2e96c8d3b19d90e9f6fc171745</span><br><span class="line">[I 19:05:01.573 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[C 19:05:01.574 NotebookApp]</span><br><span class="line"></span><br><span class="line">    Copy/paste this URL into your browser when you connect for the first time,</span><br><span class="line">    to login with a token:</span><br><span class="line">        http://localhost:8888/?token=c06252daa6a12cfdd33c1d2e96c8d3b19d90e9f6fc171745</span><br></pre></td></tr></table></figure>
<p>如上所示，MLflow提供了将模型工件记录到S3的功能。因此，一旦我们选择了一个模型，我们就可以使用该<code>mlflow.pyfunc</code>模块将其作为UDF导入。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import mlflow.pyfunc</span><br><span class="line"></span><br><span class="line">model_path = &apos;s3://&lt;bucket&gt;/mlflow/artifacts/1/0f8691808e914d1087cf097a08730f17/artifacts/model&apos;</span><br><span class="line">wine_path = &apos;/Users/afranzi/Projects/data/winequality-red.csv&apos;</span><br><span class="line">wine_udf = mlflow.pyfunc.spark_udf(spark, model_path)</span><br><span class="line"></span><br><span class="line">df = spark.read.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;true&quot;).option(&apos;delimiter&apos;, &apos;;&apos;).load(wine_path)</span><br><span class="line">columns = [ &quot;fixed acidity&quot;, &quot;volatile acidity&quot;, &quot;citric acid&quot;,</span><br><span class="line">            &quot;residual sugar&quot;, &quot;chlorides&quot;, &quot;free sulfur dioxide&quot;,</span><br><span class="line">            &quot;total sulfur dioxide&quot;, &quot;density&quot;, &quot;pH&quot;,</span><br><span class="line">            &quot;sulphates&quot;, &quot;alcohol&quot;</span><br><span class="line">          ]</span><br><span class="line">          </span><br><span class="line">df.withColumn(&apos;prediction&apos;, wine_udf(*columns)).show(100, False)</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.nlark.com/yuque/0/2019/png/219582/1552639783096-5f2fa010-2bff-4b1f-a478-ec4bde5b218d.png#align=left&amp;display=inline&amp;height=250&amp;name=1_hqSqC1493Tmu5-OHNcKhRg.png&amp;originHeight=335&amp;originWidth=1000&amp;size=196030&amp;status=done&amp;width=746" alt="1_hqSqC1493Tmu5-OHNcKhRg.png"><br>到目前为止，我们已经展示了如何通过在我们所有的葡萄酒数据集中运行葡萄酒质量预测来将PySpark与MLflow结合使用。但是当您需要使用Scala Spark的Python MLflow模块时会发生什么？<br>我们还设法通过在Scala和Python之间共享Spark Context来测试它。这意味着我们注册的MLflow UDF在Python，然后从斯卡拉用它（<em>叶氏，不是一个很好的解决方案，但至少它的东西</em><strong>🍭</strong>）。<br><a name="9f923831"></a></p>
<h4 id="Scala-Spark-MLflow"><a href="#Scala-Spark-MLflow" class="headerlink" title="Scala Spark + MLflow"></a>Scala Spark + MLflow</h4><p>对于此示例，我们将Toree <a href="https://toree.apache.org/" target="_blank" rel="noopener"><strong>内核</strong></a>添加到现有的Jupyter中。<br>安装Spark + Toree + Jupyter</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install toree</span><br><span class="line">jupyter toree install --spark_home=/Users/afranzi/spark --sys-prefix</span><br><span class="line">jupyter kernelspec list</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Available kernels:</span><br><span class="line">  apache_toree_scala    /Users/afranzi/.virtualenvs/mlflow/share/jupyter/kernels/apache_toree_scala</span><br><span class="line">  python3               /Users/afranzi/.virtualenvs/mlflow/share/jupyter/kernels/python3</span><br></pre></td></tr></table></figure>
<p>正如您在附带的笔记本中看到的，UDF在Spark和PySpark之间共享。我们希望这最后一部分对那些喜欢Scala并且必须将ML模型投入生产的团队有所帮助。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Column, DataFrame&#125;</span><br><span class="line"><span class="keyword">import</span> scala.util.matching.Regex</span><br><span class="line"></span><br><span class="line">val FirstAtRe: Regex = <span class="string">"^_"</span>.r</span><br><span class="line">val AliasRe: Regex = <span class="string">"[\\s_.:@]+"</span>.r</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFieldAlias</span><span class="params">(field_name: String)</span>:</span> String = &#123;</span><br><span class="line">    FirstAtRe.replaceAllIn(AliasRe.replaceAllIn(field_name, <span class="string">"_"</span>), <span class="string">""</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectFieldsNormalized</span><span class="params">(columns: List[String])</span><span class="params">(df: DataFrame)</span>:</span> DataFrame = &#123;</span><br><span class="line">    val fieldsToSelect: List[Column] = columns.map(field =&gt;</span><br><span class="line">        col(field).<span class="keyword">as</span>(getFieldAlias(field))</span><br><span class="line">    )</span><br><span class="line">    df.select(fieldsToSelect: _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeSchema</span><span class="params">(df: DataFrame)</span>:</span> DataFrame = &#123;</span><br><span class="line">    val schema = df.columns.toList</span><br><span class="line">    df.transform(selectFieldsNormalized(schema))</span><br><span class="line">&#125;</span><br><span class="line">FirstAtRe = ^_</span><br><span class="line">AliasRe = [\s_.:@]+</span><br><span class="line">getFieldAlias: (field_name: String)String</span><br><span class="line">selectFieldsNormalized: (columns: List[String])(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame</span><br><span class="line">normalizeSchema: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame</span><br><span class="line">Out[<span class="number">1</span>]:</span><br><span class="line">[\s_.:@]+</span><br><span class="line">In [<span class="number">2</span>]:</span><br><span class="line">val winePath = <span class="string">"~/Research/mlflow-workshop/examples/wine_quality/data/winequality-red.csv"</span></span><br><span class="line">val modelPath = <span class="string">"/tmp/mlflow/artifactStore/0/96cba14c6e4b452e937eb5072467bf79/artifacts/model"</span></span><br><span class="line">winePath = ~/Research/mlflow-workshop/examples/wine_quality/data/winequality-red.csv</span><br><span class="line">modelPath = /tmp/mlflow/artifactStore/<span class="number">0</span>/<span class="number">96</span>cba14c6e4b452e937eb5072467bf79/artifacts/model</span><br><span class="line">Out[<span class="number">2</span>]:</span><br><span class="line">/tmp/mlflow/artifactStore/<span class="number">0</span>/<span class="number">96</span>cba14c6e4b452e937eb5072467bf79/artifacts/model</span><br><span class="line">In [<span class="number">3</span>]:</span><br><span class="line">val df = spark.read</span><br><span class="line">              .format(<span class="string">"csv"</span>)</span><br><span class="line">              .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">              .option(<span class="string">"delimiter"</span>, <span class="string">";"</span>)</span><br><span class="line">              .load(winePath)</span><br><span class="line">              .transform(normalizeSchema)</span><br><span class="line">df = [fixed_acidity: string, volatile_acidity: string ... <span class="number">10</span> more fields]</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">[fixed_acidity: string, volatile_acidity: string ... <span class="number">10</span> more fields]</span><br><span class="line">In [<span class="number">4</span>]:</span><br><span class="line">%%PySpark</span><br><span class="line"><span class="keyword">import</span> mlflow</span><br><span class="line"><span class="keyword">from</span> mlflow <span class="keyword">import</span> pyfunc</span><br><span class="line"></span><br><span class="line">model_path = <span class="string">"/tmp/mlflow/artifactStore/0/96cba14c6e4b452e937eb5072467bf79/artifacts/model"</span></span><br><span class="line">wine_quality_udf = mlflow.pyfunc.spark_udf(spark, model_path)</span><br><span class="line"></span><br><span class="line">spark.udf.register(<span class="string">"wineQuality"</span>, wine_quality_udf)</span><br><span class="line">Out[<span class="number">4</span>]:</span><br><span class="line">&lt;function spark_udf.&lt;locals&gt;.predict at <span class="number">0x1116a98c8</span>&gt;</span><br><span class="line">In [<span class="number">6</span>]:</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"wines"</span>)</span><br><span class="line">In [<span class="number">10</span>]:</span><br><span class="line">%%SQL</span><br><span class="line">SELECT </span><br><span class="line">    quality,</span><br><span class="line">    wineQuality(</span><br><span class="line">        fixed_acidity,</span><br><span class="line">        volatile_acidity,</span><br><span class="line">        citric_acid,</span><br><span class="line">        residual_sugar,</span><br><span class="line">        chlorides,</span><br><span class="line">        free_sulfur_dioxide,</span><br><span class="line">        total_sulfur_dioxide,</span><br><span class="line">        density,</span><br><span class="line">        pH,</span><br><span class="line">        sulphates,</span><br><span class="line">        alcohol</span><br><span class="line">    ) AS prediction</span><br><span class="line">FROM wines</span><br><span class="line">LIMIT <span class="number">10</span></span><br><span class="line">Out[<span class="number">10</span>]:</span><br><span class="line">+-------+------------------+</span><br><span class="line">|quality|        prediction|</span><br><span class="line">+-------+------------------+</span><br><span class="line">|      <span class="number">5</span>| <span class="number">5.576883967129615</span>|</span><br><span class="line">|      <span class="number">5</span>|  <span class="number">5.50664776916154</span>|</span><br><span class="line">|      <span class="number">5</span>| <span class="number">5.525504822954496</span>|</span><br><span class="line">|      <span class="number">6</span>| <span class="number">5.504311247097457</span>|</span><br><span class="line">|      <span class="number">5</span>| <span class="number">5.576883967129615</span>|</span><br><span class="line">|      <span class="number">5</span>|<span class="number">5.5556903912725755</span>|</span><br><span class="line">|      <span class="number">5</span>| <span class="number">5.467882654744997</span>|</span><br><span class="line">|      <span class="number">7</span>| <span class="number">5.710602976324739</span>|</span><br><span class="line">|      <span class="number">7</span>| <span class="number">5.657319539336507</span>|</span><br><span class="line">|      <span class="number">5</span>| <span class="number">5.345098606538708</span>|</span><br><span class="line">+-------+------------------+</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]:</span><br><span class="line">spark.catalog.listFunctions.filter(<span class="string">'name like "%wineQuality%").show(20, false)</span></span><br><span class="line"><span class="string">+-----------+--------+-----------+---------+-----------+</span></span><br><span class="line"><span class="string">|name       |database|description|className|isTemporary|</span></span><br><span class="line"><span class="string">+-----------+--------+-----------+---------+-----------+</span></span><br><span class="line"><span class="string">|wineQuality|null    |null       |null     |true       |</span></span><br><span class="line"><span class="string">+-----------+--------+-----------+---------+-----------+</span></span><br></pre></td></tr></table></figure>
<p><a name="38ce27d8"></a></p>
<h4 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h4><p>尽管MLflow目前处于Alpha（🥁），但它看起来很有希望。只需能够运行多个机器学习框架并从同一端点使用它们，它就能将所有推荐系统提升到新的水平。<br>此外，MLflow 通过在它们之间建立公共层，使&gt; <strong>数据工程师</strong>和数据科学家&gt; <strong>更加接近</strong> 。&gt;<br>在对MLflow进行这项研究之后，我们确信我们将进一步研究它并开始在Spark管道和我们的推荐系统中使用它。<br>让文件存储与数据库同步而不是使用FS会很好。这应该允许多个端点使用相同的文件存储。就像使用相同的<a href="https://aws.amazon.com/glue/" target="_blank" rel="noopener">Glue Metastore</a>有多个<a href="https://prestodb.io/" target="_blank" rel="noopener">Presto</a>和<a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">Athena</a>实例一样。<a href="https://aws.amazon.com/glue/" target="_blank" rel="noopener"></a><br>最后，感谢MLflow背后的所有社区，使其成为可能，让我们的数据更有趣。<br>如果您正在玩MLflow，请<strong>随时联系我们</strong>并提供有关您如何使用它的一些反馈！更重要的是，如果您在生产中使用MLflow。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zhos.me/2019/03/15/yuque/MLflow模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhos">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A+">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/15/yuque/MLflow模型/" itemprop="url">MLflow模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-15T15:28:46+08:00">
                2019-03-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a name="e2cd7971"></a></p>
<h1 id="MLflow模型"><a href="#MLflow模型" class="headerlink" title="MLflow模型"></a>MLflow模型<a href="https://mlflow.org/docs/latest/models.html#mlflow-models" target="_blank" rel="noopener"></a></h1><p>MLflow模型是用于打包机器学习模型的标准格式，可用于各种下游工具 - 例如，通过REST API实时提供服务或在Apache Spark上进行批量推理。该格式定义了一种约定，允许您以不同的“风格”保存模型，这些“风味”可以被不同的下游工具理解。</p>
<p>目录</p>
<ul>
<li><a href="https://mlflow.org/docs/latest/models.html#storage-format" target="_blank" rel="noopener">存储格式</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#model-api" target="_blank" rel="noopener">模型API</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#built-in-model-flavors" target="_blank" rel="noopener">内置型号口味</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#custom-flavors" target="_blank" rel="noopener">自定义口味</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#built-in-deployment-tools" target="_blank" rel="noopener">内置部署工具</a><br><a name="2dd6d735"></a><h2 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a><a href="https://mlflow.org/docs/latest/models.html#id2" target="_blank" rel="noopener">存储格式</a></h2>每个MLflow模型都是一个包含任意文件<code>MLmodel</code> 的目录，以及目录根目录中的一个文件，该文件可以定义可以查看模型的多种_风格_。<br>使MLflow模型功能强大的关键概念：它们是部署工具可用于理解模型的约定，这使得编写可与任何ML库中的模型一起使用的工具成为可能，而无需将每个工具与每个库集成。MLflow定义了所有内置部署工具支持的几种“标准”风格，例如描述如何将模型作为Python函数运行的“Python函数”风格。但是，库也可以定义和使用其他类型。例如，MLflow的<a href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#module-mlflow.sklearn" target="_blank" rel="noopener"><code>mlflow.sklearn</code></a>库允许将模型加载为scikit-learn <code>Pipeline</code>对象，以便在知道scikit-learn的代码中使用，或者作为通用Python函数用于仅需要应用模型的工具（例如，工具）用于将模型部署到Amazon SageMaker）。<br>特定模型支持的所有风格都<code>MLmodel</code>以YAML格式在其文件中定义。例如，<a href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#module-mlflow.sklearn" target="_blank" rel="noopener"><code>mlflow.sklearn</code></a>输出模型如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Directory written by mlflow.sklearn.save_model(model, &quot;my_model&quot;)</span><br><span class="line">my_model/</span><br><span class="line">├── MLmodel</span><br><span class="line">└── model.pkl</span><br></pre></td></tr></table></figure>
<p>它的<code>MLmodel</code>文件描述了两种风格：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">time_created:</span> <span class="number">2018</span><span class="bullet">-05</span><span class="bullet">-25</span><span class="attr">T17:28:53.35</span></span><br><span class="line"></span><br><span class="line"><span class="attr">flavors:</span></span><br><span class="line"><span class="attr">  sklearn:</span></span><br><span class="line"><span class="attr">    sklearn_version:</span> <span class="number">0.19</span><span class="number">.1</span></span><br><span class="line"><span class="attr">    pickled_model:</span> <span class="string">model.pkl</span></span><br><span class="line"><span class="attr">  python_function:</span></span><br><span class="line"><span class="attr">    loader_module:</span> <span class="string">mlflow.sklearn</span></span><br></pre></td></tr></table></figure>
<p>该模型然后可以与任何支持工具中使用_任一_的<code>sklearn</code>或 <code>python_function</code>模型的味道。例如，该命令可以为具有flavor 的模型提供服务：<code>mlflow sklearn`</code>sklearn`</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlflow sklearn serve my_model</span><br></pre></td></tr></table></figure>
<p>此外，命令行工具可以将模型打包并部署到AWS SageMaker，只要它们支持这种风格：<code>mlflow sagemaker`</code>python_function`</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlflow sagemaker deploy -m my_model [other options]</span><br></pre></td></tr></table></figure>
<p><a name="ce13267d"></a></p>
<h3 id="MLmodel格式的字段"><a href="#MLmodel格式的字段" class="headerlink" title="MLmodel格式的字段"></a>MLmodel格式的字段<a href="https://mlflow.org/docs/latest/models.html#fields-in-the-mlmodel-format" target="_blank" rel="noopener"></a></h3><p>除了从<strong>口味</strong>场列出了模型的口味，在MLmodel YAML格式可以包含以下字段：<br><a name="TIME_CREATED"></a></p>
<h4 id="TIME-CREATED"><a href="#TIME-CREATED" class="headerlink" title="TIME_CREATED"></a>TIME_CREATED</h4><p>创建模型的日期和时间，采用UTC ISO 8601格式。<br><a name="run_id"></a></p>
<h4 id="run-id"><a href="#run-id" class="headerlink" title="run_id"></a>run_id</h4><p>如果使用<a href="https://mlflow.org/docs/latest/tracking.html#tracking" target="_blank" rel="noopener">MLflow Tracking</a>保存模型，则创建模型的运行的ID 。<br><a name="9f4e50f6"></a></p>
<h2 id="模型API"><a href="#模型API" class="headerlink" title="模型API"></a><a href="https://mlflow.org/docs/latest/models.html#id3" target="_blank" rel="noopener">模型API</a><a href="https://mlflow.org/docs/latest/models.html#model-api" target="_blank" rel="noopener"></a></h2><p>您可以通过多种方式保存和加载MLflow模型。首先，MLflow包括与几个公共库的集成。例如，scikit-learn模型的<code>[mlflow.sklearn](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#module-mlflow.sklearn)包括</code><a href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.save_model" target="_blank" rel="noopener"><code>save_model</code></a>，<a href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" target="_blank" rel="noopener"><code>log_model</code></a>和<a href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.load_model" target="_blank" rel="noopener"><code>load_model</code></a>函数。其次，您可以使用<a href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.Model" target="_blank" rel="noopener"><code>mlflow.models.Model</code></a>该类来创建和编写模型。这个类有四个关键功能：</p>
<ul>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.Model.add_flavor" target="_blank" rel="noopener"><code>add_flavor</code></a>为模型添加味道。每个flavor都有一个字符串名称和一个键值属性字典，其中值可以是任何可以序列化为YAML的对象。</li>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.Model.save" target="_blank" rel="noopener"><code>save</code></a> 将模型保存到本地目录。</li>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.Model.log" target="_blank" rel="noopener"><code>log</code></a> 使用MLflow Tracking将模型记录为当前运行中的工件。</li>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.Model.load" target="_blank" rel="noopener"><code>load</code></a> 从本地目录或先前运行中的工件加载模型。<br><a name="5e2a9075"></a><h2 id="内置型号口味"><a href="#内置型号口味" class="headerlink" title="内置型号口味"></a><a href="https://mlflow.org/docs/latest/models.html#id4" target="_blank" rel="noopener">内置型号口味</a><a href="https://mlflow.org/docs/latest/models.html#built-in-model-flavors" target="_blank" rel="noopener"></a></h2>MLflow提供了几种可能在您的应用程序中有用的标准风格。具体来说，它的许多部署工具都支持这些风格，因此您可以将这些风格导出自己的模型，以便从所有这些工具中受益。<br><a name="19753d27"></a><h3 id="Python函数（python-function）"><a href="#Python函数（python-function）" class="headerlink" title="Python函数（python_function）"></a>Python函数（<code>python_function</code>）<a href="https://mlflow.org/docs/latest/models.html#python-function-python-function" target="_blank" rel="noopener"></a></h3>该<code>python_function</code>模型的味道定义了一个通用的文件系统格式为Python模型和保存和加载模型，并从该格式提供了工具。该格式是自包含的，因为它包含加载和使用模型所需的所有信息。依赖关系直接存储在模型中或通过Conda环境引用。<br><code>python_function</code>模型的约定是具有<code>predict</code>以下签名的方法或函数：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(data: pandas.DataFrame) -&gt; [pandas.DataFrame | numpy.array]</span><br></pre></td></tr></table></figure>
<p>其他MLflow组件期望<code>python_function</code>模型遵循此约定。<br>该<code>python_function</code>模型格式定义为包含所有所需的数据的目录结构，代码和配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./dst-path/</span><br><span class="line">        ./MLmodel: configuration</span><br><span class="line">        &lt;code&gt;: code packaged with the model (specified in the MLmodel file)</span><br><span class="line">        &lt;data&gt;: data packaged with the model (specified in the MLmodel file)</span><br><span class="line">        &lt;env&gt;: Conda environment definition (specified in the MLmodel file)</span><br></pre></td></tr></table></figure>
<p>一个<code>python_function</code>模式目录必须包含<code>MLmodel</code>在其与“python_function”格式和下列参数的根文件：<br><a name="a437d5e1"></a></p>
<h4 id="loader-module-required"><a href="#loader-module-required" class="headerlink" title="loader_module [required]:"></a>loader_module [required]:</h4><p>可以加载模型的Python模块。期望是<code>mlflow.sklearn</code>可导入的模块标识符（例如）<code>importlib.import_module</code>。导入的模块必须包含具有以下签名的函数：<br>&gt; _load_pyfunc（path：string） - &gt; <pyfunc model></pyfunc></p>
<p>path参数由<code>data</code>参数指定，可以引用文件或目录。<br><a name="00d57722"></a></p>
<h4 id="code-optional"><a href="#code-optional" class="headerlink" title="code [optional]:"></a>code [optional]:</h4><p>包含此模型打包的代码的目录的相对路径。在导入模型加载器之前，此目录中的所有文件和目录都将添加到Python路径中。<br><a name="b1b6f654"></a></p>
<h4 id="data-optional"><a href="#data-optional" class="headerlink" title="data [optional]:"></a>data [optional]:</h4><p>包含模型数据的文件或目录的相对路径。路径传递给模型加载器。<br><a name="6bbe3903"></a></p>
<h4 id="env-可选-："><a href="#env-可选-：" class="headerlink" title="env [可选]："></a>env [可选]：</h4><p>导出的Conda环境的相对路径。如果存在，则在运行模型之前激活此环境。<br><a name="cf3e2664"></a></p>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree example/sklearn_iris/mlruns/run1/outputs/linear-lr</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── MLmodel</span><br><span class="line">├── code</span><br><span class="line">│   ├── sklearn_iris.py</span><br><span class="line">│</span><br><span class="line">├── data</span><br><span class="line">│   └── model.pkl</span><br><span class="line">└── mlflow_env.yml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat example/sklearn_iris/mlruns/run1/outputs/linear-lr/MLmodel</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python_function:</span><br><span class="line">  code: code</span><br><span class="line">  data: data/model.pkl</span><br><span class="line">  loader_module: mlflow.sklearn</span><br><span class="line">  env: mlflow_env.yml</span><br><span class="line">  main: sklearn_iris</span><br></pre></td></tr></table></figure>
<p>有关更多信息，请参阅<a href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#module-mlflow.pyfunc" target="_blank" rel="noopener"><code>mlflow.pyfunc</code></a>。<br><a name="b099026c"></a></p>
<h3 id="H-O（h2o）"><a href="#H-O（h2o）" class="headerlink" title="H  O（h2o）"></a>H  O（<code>h2o</code>）<a href="https://mlflow.org/docs/latest/models.html#h2o-h2o" target="_blank" rel="noopener"></a></h3><p>H2O模型风味可以记录和加载H2O模型。这些模型将通过使用保存<a href="https://mlflow.org/docs/latest/python_api/mlflow.h2o.html#mlflow.h2o.save_model" target="_blank" rel="noopener"><code>mlflow.h2o.save_model</code></a>。使用<a href="https://mlflow.org/docs/latest/python_api/mlflow.h2o.html#mlflow.h2o.log_model" target="_blank" rel="noopener"><code>mlflow.h2o.log_model</code></a>也将启用有效的味道。<code>Python Function</code><br>将H2O模型作为PyFunc模型加载时，<code>h2o.init(...)</code>将调用。因此，正确版本的h2o（-py）必须在环境中。给出的参数<code>h2o.init(...)</code>可以<code>model.h2o/h2o.yaml</code>在键下定制<code>init</code>。有关更多信息，请参阅<a href="https://mlflow.org/docs/latest/python_api/mlflow.h2o.html#module-mlflow.h2o" target="_blank" rel="noopener"><code>mlflow.h2o</code></a>。<br><a name="778f4101"></a></p>
<h3 id="Keras（keras）"><a href="#Keras（keras）" class="headerlink" title="Keras（keras）"></a>Keras（<code>keras</code>）</h3><p>该<code>keras</code>模型味道启用日志记录和装载Keras模型。该模型将通过Keras提供的model_save功能以HDF5文件格式保存。此外，模型可以加载为。有关更多信息，请参阅。<code>Python Function</code><a href="https://mlflow.org/docs/latest/python_api/mlflow.keras.html#module-mlflow.keras" target="_blank" rel="noopener"><code>mlflow.keras</code></a><br><a name="bc0099ae"></a></p>
<h3 id="MLeap（mleap）"><a href="#MLeap（mleap）" class="headerlink" title="MLeap（mleap）"></a>MLeap（<code>mleap</code>）</h3><p>该<code>mleap</code>模型风味支持使用MLeap持久性机制保存模型。该<code>mlflow/java</code>软件包中提供了一个用于加载具有MLeap风格格式的MLflow模型的配套模块。有关更多信息，请参阅<a href="https://mlflow.org/docs/latest/python_api/mlflow.mleap.html#module-mlflow.mleap" target="_blank" rel="noopener"><code>mlflow.mleap</code></a>。<br><a name="cf77fb8a"></a></p>
<h3 id="PyTorch（pytorch）"><a href="#PyTorch（pytorch）" class="headerlink" title="PyTorch（pytorch）"></a>PyTorch（<code>pytorch</code>）</h3><p>该<code>pytorch</code>模型味道启用日志记录和装载PyTorch模型。模型使用torch.save（模型）方法以.pth格式完全存储。给定包含已保存模型的目录，您可以将模型记录到MLflow via 。然后可以加载保存的模型以进行推理。有关更多信息，请参阅。<code>log_saved_model`</code>mlflow.pyfunc.load_pyfunc()<code>[</code>mlflow.pytorch`](<a href="https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#module-mlflow.pytorch" target="_blank" rel="noopener">https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#module-mlflow.pytorch</a>)<br><a name="0fdc419b"></a></p>
<h3 id="Scikit-learn（sklearn）"><a href="#Scikit-learn（sklearn）" class="headerlink" title="Scikit-learn（sklearn）"></a>Scikit-learn（<code>sklearn</code>）</h3><p>该<code>sklearn</code>模型的味道提供了一个简单易用的界面来处理scikit学习模式，没有外部的依赖关系。它使用Python的pickle模块保存和加载模型，并生成有效的 <code>python_function</code>flavor模型。有关更多信息，请参阅<a href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#module-mlflow.sklearn" target="_blank" rel="noopener"><code>mlflow.sklearn</code></a>。<br><a name="cddd0a6a"></a></p>
<h3 id="Spark-MLlib（spark）"><a href="#Spark-MLlib（spark）" class="headerlink" title="Spark MLlib（spark）"></a>Spark MLlib（<code>spark</code>）</h3><p>该<code>spark</code>模型的味道能使出口星火MLlib模型作为MLflow模型。导出的模型使用Spark MLLib的本机序列化进行保存，然后可以作为MLlib模型加载回来或作为<code>python_function</code>模型进行部署。当部署为a时<code>python_function</code>，模型会创建自己的SparkContext，并在评分之前将pandas DataFrame输入转换为Spark DataFrame。虽然这不是最有效的解决方案，尤其是对于实时评分，但它使您能够轻松地将任何MLlib PipelineModel（只要PipelineModel没有外部JAR依赖性）部署到MLflow支持的任何端点。有关更多信息，请参阅<a href="https://mlflow.org/docs/latest/python_api/mlflow.spark.html#module-mlflow.spark" target="_blank" rel="noopener"><code>mlflow.spark</code></a>。<br><a name="2e423970"></a></p>
<h3 id="TensorFlow（tensorflow）"><a href="#TensorFlow（tensorflow）" class="headerlink" title="TensorFlow（tensorflow）"></a>TensorFlow（<code>tensorflow</code>）</h3><p>该<code>tensorflow</code>模型的味道使记录TensorFlow 并加载它们早在模型上大熊猫DataFrames推断。给定包含已保存模型的目录，您可以将模型记录到MLflow ，然后使用加载保存的模型进行推理。有关更多信息，请参阅。<code>Saved Models`</code>Python Function<code>log_saved_model</code>mlflow.pyfunc.load_pyfunc<code>[</code>mlflow.tensorflow`](<a href="https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#module-mlflow.tensorflow" target="_blank" rel="noopener">https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#module-mlflow.tensorflow</a>)<br><a name="e070d9ab"></a></p>
<h2 id="自定义口味"><a href="#自定义口味" class="headerlink" title="自定义口味"></a><a href="https://mlflow.org/docs/latest/models.html#id5" target="_blank" rel="noopener">自定义口味</a><a href="https://mlflow.org/docs/latest/models.html#custom-flavors" target="_blank" rel="noopener"></a></h2><p>您可以在MLmodel文件中添加一种风格，可以通过直接编写或使用<a href="https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.Model" target="_blank" rel="noopener"><code>mlflow.models.Model</code></a>类构建它。为您的风味选择一个任意的字符串名称。MLflow工具忽略他们不理解的MLmodel文件中的风格。<br><a name="197ccaae"></a></p>
<h2 id="内置部署工具"><a href="#内置部署工具" class="headerlink" title="内置部署工具"></a><a href="https://mlflow.org/docs/latest/models.html#id6" target="_blank" rel="noopener">内置部署工具</a></h2><p>MLflow提供了在本地计算机和多个生产环境中部署模型的工具。并非所有部署方法都适用于所有型号的风格。Python函数格式和所有兼容格式支持部署。</p>
<ul>
<li><a href="https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-as-a-local-rest-api-endpoint" target="_blank" rel="noopener">将<code>python_function</code>模型部署为本地REST API端点</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml" target="_blank" rel="noopener"><code>python_function</code>在Microsoft Azure ML上部署模型</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker" target="_blank" rel="noopener"><code>python_function</code>在Amazon SageMaker上部署模型</a></li>
<li><a href="https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf" target="_blank" rel="noopener">将<code>python_function</code>模型导出为Apache Spark UDF</a><br><a name="14f802bf"></a><h3 id="将python-function模型部署为本地REST-API端点"><a href="#将python-function模型部署为本地REST-API端点" class="headerlink" title="将python_function模型部署为本地REST API端点"></a><a href="https://mlflow.org/docs/latest/models.html#id7" target="_blank" rel="noopener">将<code>python_function</code>模型部署为本地REST API端点</a><a href="https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-as-a-local-rest-api-endpoint" target="_blank" rel="noopener"></a></h3>MLflow可以在本地部署模型作为本地REST API端点，或直接对CSV文件进行评分。在部署到远程模型服务器之前，此功能是测试模型的便捷方式。您可以使用CLI界面在本地部署Python函数flavor到<a href="https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#module-mlflow.pyfunc" target="_blank" rel="noopener"><code>mlflow.pyfunc</code></a>模块。本地REST API服务器接受以下数据格式作为输入：<blockquote>
<ul>
<li>JSON序列化的pandas DataFrames的<code>split</code>方向。例如， 。使用 请求标头值指定此格式。从MLflow 0.9.0开始，如果是（即没有格式规范），这将是默认格式。<code>data = pandas_df.to_json(orient=&#39;split&#39;)`</code>Content-Type<code>application/json; format=pandas-split</code>Content-Type<code></code>application/json`</li>
</ul>
</blockquote>
</li>
<li>JSON序列化的pandas DataFrames的<code>records</code>方向。<em>我们不建议使用此格式，因为无法保证保留列顺序。</em>目前，使用 或的<code>Content-Type</code>请求标头值 指定此格式。从MLflow 0.9.0开始，将参考 格式。为了向前兼容，我们建议使用格式或指定内容类型。<code>application/json; format=pandas-records`</code>application/json<code>application/json</code>split<code>split</code>application/json; format=pandas-records`</li>
<li>CSV序列化的pandas DataFrames。例如，。使用请求标头值指定此格式。<code>data = pandas_df.to_csv()`</code>Content-Type<code></code>text/csv`</li>
</ul>
<p>有关序列化pandas DataFrames的更多信息，请参阅 <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html" target="_blank" rel="noopener">pandas.DataFrame.to_json</a>。<br><a name="ddf7d2a5"></a></p>
<h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令<a href="https://mlflow.org/docs/latest/models.html#commands" target="_blank" rel="noopener"></a></h4><ul>
<li><code>serve</code> 将模型部署为本地REST API服务器。</li>
<li><code>predict</code> 使用该模型生成本地CSV文件的预测。</li>
</ul>
<p>有关详细信息，请参阅：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlflow  pyfunc  - help </span><br><span class="line">mlflow  pyfunc  serve  - help </span><br><span class="line">mlflow  pyfunc  predict  - help</span><br></pre></td></tr></table></figure>
<p><a name="3aa45b7d"></a></p>
<h3 id="python-function在Microsoft-Azure-ML上部署模型"><a href="#python-function在Microsoft-Azure-ML上部署模型" class="headerlink" title="python_function在Microsoft Azure ML上部署模型"></a><a href="https://mlflow.org/docs/latest/models.html#id8" target="_blank" rel="noopener"><code>python_function</code>在Microsoft Azure ML上部署模型</a><a href="https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml" target="_blank" rel="noopener"></a></h3><p>该<a href="https://mlflow.org/docs/latest/python_api/mlflow.azureml.html#module-mlflow.azureml" target="_blank" rel="noopener"><code>mlflow.azureml</code></a>模块可以将<code>python_function</code>模型打包到Azure ML容器映像中。这些映像可以部署到Azure Kubernetes服务（AKS）和Azure容器实例（ACI）平台，以实现实时服务。生成的Azure ML ContainerImage包含一个Web服务器，它接受以下数据格式作为输入：</p>
<blockquote>
<ul>
<li>JSON序列化的pandas DataFrames的<code>split</code>方向。例如，。使用请求标头值指定此格式。<code>data = pandas_df.to_json(orient=&#39;split&#39;)`</code>Content-Type<code></code>application/json`</li>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.azureml.html#mlflow.azureml.build_image" target="_blank" rel="noopener"><code>build_image</code></a>向现有Azure ML工作区注册MLflow模型，并构建Azure ML容器映像以部署到AKS和ACI。在<a href="https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py" target="_blank" rel="noopener">Azure的ML SDK</a>要求才能使用此功能。<em>Azure ML SDK需要Python 3.它不能与早期版本的Python一起安装。</em></li>
</ul>
</blockquote>
<p><a name="b88cb25c"></a></p>
<h4 id="使用Python-API的示例工作流程"><a href="#使用Python-API的示例工作流程" class="headerlink" title="使用Python API的示例工作流程"></a>使用Python API的示例工作流程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlflow.azureml</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> azureml.core <span class="keyword">import</span> Workspace</span><br><span class="line"><span class="keyword">from</span> azureml.core.webservice <span class="keyword">import</span> AciWebservice, Webservice</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create or load an existing Azure ML workspace. You can also load an existing workspace using</span></span><br><span class="line"><span class="comment"># Workspace.get(name="&lt;workspace_name&gt;")</span></span><br><span class="line">workspace_name = <span class="string">"&lt;Name of your Azure ML workspace&gt;"</span></span><br><span class="line">subscription_id = <span class="string">"&lt;Your Azure subscription ID&gt;"</span></span><br><span class="line">resource_group = <span class="string">"&lt;Name of the Azure resource group in which to create Azure ML resources&gt;"</span></span><br><span class="line">location = <span class="string">"&lt;Name of the Azure location (region) in which to create Azure ML resources&gt;"</span></span><br><span class="line">azure_workspace = Workspace.create(name=workspace_name,</span><br><span class="line">                                   subscription_id=subscription_id,</span><br><span class="line">                                   resource_group=resource_group,</span><br><span class="line">                                   location=location,</span><br><span class="line">                                   create_resource_group=<span class="literal">True</span>,</span><br><span class="line">                                   exist_okay=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build an Azure ML container image for deployment</span></span><br><span class="line">azure_image, azure_model = mlflow.azureml.build_image(model_path=<span class="string">"&lt;path-to-model&gt;"</span>,</span><br><span class="line">                                                      workspace=azure_workspace,</span><br><span class="line">                                                      description=<span class="string">"Wine regression model 1"</span>,</span><br><span class="line">                                                      synchronous=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># If your image build failed, you can access build logs at the following URI:</span></span><br><span class="line">print(<span class="string">"Access the following URI for build logs: &#123;&#125;"</span>.format(azure_image.image_build_log_uri))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Deploy the container image to ACI</span></span><br><span class="line">webservice_deployment_config = AciWebservice.deploy_configuration()</span><br><span class="line">webservice = Webservice.deploy_from_image(</span><br><span class="line">                    image=azure_image, workspace=azure_workspace, name=<span class="string">"&lt;deployment-name&gt;"</span>)</span><br><span class="line">webservice.wait_for_deployment()</span><br><span class="line"></span><br><span class="line"><span class="comment"># After the image deployment completes, requests can be posted via HTTP to the new ACI</span></span><br><span class="line"><span class="comment"># webservice's scoring URI. The following example posts a sample input from the wine dataset</span></span><br><span class="line"><span class="comment"># used in the MLflow ElasticNet example:</span></span><br><span class="line"><span class="comment"># https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine</span></span><br><span class="line">print(<span class="string">"Scoring URI is: %s"</span>, webservice.scoring_uri)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientation</span></span><br><span class="line">sample_input = &#123;</span><br><span class="line">    <span class="string">"columns"</span>: [</span><br><span class="line">        <span class="string">"alcohol"</span>,</span><br><span class="line">        <span class="string">"chlorides"</span>,</span><br><span class="line">        <span class="string">"citric acid"</span>,</span><br><span class="line">        <span class="string">"density"</span>,</span><br><span class="line">        <span class="string">"fixed acidity"</span>,</span><br><span class="line">        <span class="string">"free sulfur dioxide"</span>,</span><br><span class="line">        <span class="string">"pH"</span>,</span><br><span class="line">        <span class="string">"residual sugar"</span>,</span><br><span class="line">        <span class="string">"sulphates"</span>,</span><br><span class="line">        <span class="string">"total sulfur dioxide"</span>,</span><br><span class="line">        <span class="string">"volatile acidity"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"data"</span>: [</span><br><span class="line">        [<span class="number">8.8</span>, <span class="number">0.045</span>, <span class="number">0.36</span>, <span class="number">1.001</span>, <span class="number">7</span>, <span class="number">45</span>, <span class="number">3</span>, <span class="number">20.7</span>, <span class="number">0.45</span>, <span class="number">170</span>, <span class="number">0.27</span>]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(</span><br><span class="line">              url=webservice.scoring_uri, data=json.dumps(sample_input),</span><br><span class="line">              headers=&#123;<span class="string">"Content-type"</span>: <span class="string">"application/json"</span>&#125;)</span><br><span class="line">response_json = json.loads(response.text)</span><br><span class="line">print(response_json)</span><br></pre></td></tr></table></figure>
<p><a name="35118db0"></a></p>
<h4 id="Example-workflow-using-the-MLflow-CLI"><a href="#Example-workflow-using-the-MLflow-CLI" class="headerlink" title="Example workflow using the MLflow CLI"></a>Example workflow using the MLflow CLI</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">mlflow azureml build-image -w &lt;workspace-name&gt; -m &lt;model-path&gt; -d <span class="string">"Wine regression model 1"</span></span><br><span class="line"></span><br><span class="line">az ml service create aci -n &lt;deployment-name&gt; --image-id &lt;image-name&gt;:&lt;image-version&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># After the image deployment completes, requests can be posted via HTTP to the new ACI</span></span><br><span class="line"><span class="comment"># webservice's scoring URI. The following example posts a sample input from the wine dataset</span></span><br><span class="line"><span class="comment"># used in the MLflow ElasticNet example:</span></span><br><span class="line"><span class="comment"># https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine</span></span><br><span class="line"></span><br><span class="line">scoring_uri=$(az ml service show --name &lt;deployment-name&gt; -v | jq -r <span class="string">".scoringUri"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientation</span></span><br><span class="line">sample_input=<span class="string">'</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    "columns": [</span></span><br><span class="line"><span class="string">        "alcohol",</span></span><br><span class="line"><span class="string">        "chlorides",</span></span><br><span class="line"><span class="string">        "citric acid",</span></span><br><span class="line"><span class="string">        "density",</span></span><br><span class="line"><span class="string">        "fixed acidity",</span></span><br><span class="line"><span class="string">        "free sulfur dioxide",</span></span><br><span class="line"><span class="string">        "pH",</span></span><br><span class="line"><span class="string">        "residual sugar",</span></span><br><span class="line"><span class="string">        "sulphates",</span></span><br><span class="line"><span class="string">        "total sulfur dioxide",</span></span><br><span class="line"><span class="string">        "volatile acidity"</span></span><br><span class="line"><span class="string">    ],</span></span><br><span class="line"><span class="string">    "data": [</span></span><br><span class="line"><span class="string">        [8.8, 0.045, 0.36, 1.001, 7, 45, 3, 20.7, 0.45, 170, 0.27]</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">&#125;'</span></span><br><span class="line"></span><br><span class="line">echo $sample_input | curl -s -X POST $scoring_uri\</span><br><span class="line">-H <span class="string">'Cache-Control: no-cache'</span>\</span><br><span class="line">-H <span class="string">'Content-Type: application/json'</span>\</span><br><span class="line">-d @-</span><br></pre></td></tr></table></figure>
<p>For more info, see:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlflow azureml --<span class="built_in">help</span></span><br><span class="line">mlflow azureml build-image --<span class="built_in">help</span></span><br></pre></td></tr></table></figure>
<p><a name="753d81d7"></a></p>
<h3 id="python-function在Amazon-SageMaker上部署模型"><a href="#python-function在Amazon-SageMaker上部署模型" class="headerlink" title="python_function在Amazon SageMaker上部署模型"></a><a href="https://mlflow.org/docs/latest/models.html#id9" target="_blank" rel="noopener"><code>python_function</code>在Amazon SageMaker上部署模型</a><a href="https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker" target="_blank" rel="noopener"></a></h3><p>该<a href="https://mlflow.org/docs/latest/python_api/mlflow.sagemaker.html#module-mlflow.sagemaker" target="_blank" rel="noopener"><code>mlflow.sagemaker</code></a>模块可以<code>python_function</code>在具有SageMaker兼容环境的Docker容器中本地部署模型，并在SageMaker上远程部署模型。要远程部署到SageMaker，您需要设置环境和用户帐户。要将自定义模型导出到SageMaker，您需要在Amazon ECR上提供与MLflow兼容的Docker镜像。MLflow提供默认的Docker镜像定义; 但是，由您来构建映像并将其上载到ECR。MLflow包括<code>build_and_push_container</code>执行此步骤的效用函数。构建和上传后，您可以将MLflow容器用于所有MLflow模型。使用该<a href="https://mlflow.org/docs/latest/python_api/mlflow.sagemaker.html#module-mlflow.sagemaker" target="_blank" rel="noopener"><code>mlflow.sagemaker</code></a> 模块部署的模型Web服务器接受以下数据格式作为输入，具体取决于部署风格：</p>
<blockquote>
<ul>
<li><code>python_function</code>：对于此部署风格，端点接受与pyfunc服务器相同的格式。<a href="https://mlflow.org/docs/latest/models.html#pyfunc-deployment" target="_blank" rel="noopener">pyfunc部署文档</a>中描述了这些格式 。</li>
<li><code>mleap</code>：对于此部署风格，端点仅接受<code>split</code>方向上的 JSON序列化pandas DataFrame 。例如， 。使用 请求标头值指定此格式。<code>data = pandas_df.to_json(orient=&#39;split&#39;)`</code>Content-Type<code></code>application/json`</li>
</ul>
</blockquote>
<p><a name="ddf7d2a5-1"></a></p>
<h4 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h4><ul>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.sagemaker.html#mlflow.sagemaker.run_local" target="_blank" rel="noopener"><code>run-local</code></a>在Docker容器中本地部署模型。图像和环境应与模型远程运行的方式相同，因此在部署之前测试模型非常有用。</li>
<li>该<code>build-and-push-container</code>CLI命令构建一个MLfLow多克尔图像并上传到ECR。调用者必须设置正确的权限。图像是本地构建的，并且需要Docker存在于执行此步骤的计算机上。</li>
<li><a href="https://mlflow.org/docs/latest/python_api/mlflow.sagemaker.html#mlflow.sagemaker.deploy" target="_blank" rel="noopener"><code>deploy</code></a>在Amazon SageMaker上部署模型。MLflow将Python Function模型上传到S3并启动为该模型提供服务的Amazon SageMaker端点。</li>
</ul>
<p>使用MLflow CLI的示例工作流程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlflow sagemaker build-and-push-container  - build the container (only needs to be called once)</span><br><span class="line">mlflow sagemaker run-local -m &lt;path-to-model&gt;  -    </span><br><span class="line">        remotely</span><br></pre></td></tr></table></figure>
<p>For more info, see:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mlflow sagemaker --help</span><br><span class="line">mlflow sagemaker build-and-push-container --help</span><br><span class="line">mlflow sagemaker run-local --help</span><br><span class="line">mlflow sagemaker deploy --help</span><br></pre></td></tr></table></figure>
<p><a name="f943a389"></a></p>
<h3 id="将python-function模型导出为Apache-Spark-UDF"><a href="#将python-function模型导出为Apache-Spark-UDF" class="headerlink" title="将python_function模型导出为Apache Spark UDF"></a><a href="https://mlflow.org/docs/latest/models.html#id10" target="_blank" rel="noopener">将<code>python_function</code>模型导出为Apache Spark UDF</a><a href="https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf" target="_blank" rel="noopener"></a></h3><p>您可以将<code>python_function</code>模型输出为Apache Spark UDF，可以将其上载到Spark群集并用于对模型进行评分。<br>例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pyfunc_udf = mlflow.pyfunc.spark_udf(&lt;path-to-model&gt;)</span><br><span class="line">df = spark_df.withColumn(&quot;prediction&quot;, pyfunc_udf(&lt;features&gt;))</span><br></pre></td></tr></table></figure>
<p>生成的UDF基于Spark的Pandas UDF，目前仅限于为每个观察生成单个值或相同类型的值数组。默认情况下，我们将第一个数字列作为double返回。您可以通过提供<code>result_type</code> 参数来控制返回的结果。支持以下值：</p>
<blockquote>
<ul>
<li><code>&#39;int&#39;</code>或<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.IntegerType" target="_blank" rel="noopener">IntegerType</a>：<code>int32</code>返回可以适合结果的最左边的整数， 如果没有，则引发异常。</li>
<li><code>&#39;long&#39;</code>或<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.LongType" target="_blank" rel="noopener">LongType</a>：<code>int64</code> 返回可以适合结果的最左边的长整数，如果没有则引发异常。</li>
<li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.ArrayType" target="_blank" rel="noopener">数组类型</a>（<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.IntegerType" target="_blank" rel="noopener">IntegerType</a> | <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.LongType" target="_blank" rel="noopener">LongType</a>）：返回一个可以放入所需尺寸的所有整数列。</li>
<li><code>&#39;float&#39;</code>或<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.FloatType" target="_blank" rel="noopener">FloatType</a>：<code>float32</code>如果没有数字列，则返回最左边的数字结果转换 为或引发异常。</li>
<li><code>&#39;double&#39;</code>或<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType" target="_blank" rel="noopener">DoubleType</a>：<code>double</code>如果没有数字列，则返回最左边的数字结果转换 为或引发异常。</li>
<li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.ArrayType" target="_blank" rel="noopener">ArrayType</a>（<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.FloatType" target="_blank" rel="noopener">FloatType</a> | <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType" target="_blank" rel="noopener">DoubleType</a>）：返回<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType" target="_blank" rel="noopener">强制</a>转换为请求的所有数字列。类型。如果存在数字列，则会引发异常。</li>
<li><code>&#39;string&#39;</code>或<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StringType" target="_blank" rel="noopener">StringType</a>：Result是最左边的列转换为字符串。</li>
<li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.ArrayType" target="_blank" rel="noopener">ArrayType</a>（<a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StringType" target="_blank" rel="noopener">StringType</a>）：返回转换为字符串的所有列。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.types import ArrayType, FloatType</span><br><span class="line">pyfunc_udf = mlflow.pyfunc.spark_udf(&lt;path-to-model&gt;, result_type=ArrayType(FloatType()))</span><br><span class="line"># The prediction column will contain all the numeric columns returned by the model as floats</span><br><span class="line">df = spark_df.withColumn(&quot;prediction&quot;, pyfunc_udf(&lt;features&gt;))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhos</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
